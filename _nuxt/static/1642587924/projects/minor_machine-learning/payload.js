__NUXT_JSONP__("/projects/minor_machine-learning", (function(a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z,A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R,S,T,U,V,W,X,Y,Z,_,$,aa,ab,ac,ad,ae,af,ag,ah,ai,aj,ak,al,am,an,ao,ap){K.slug=L;K.description="A project where I apply machine learning to the aspect of cyber. Here I am using techniques such as naive bayes for label detection in a dataset";K.title=M;K.duration="full time 3 weeks";K.image="\u002Fimages\u002Fprojects\u002Fminor\u002Fmachine_learning\u002Fminor_cyber_machine_learning.png";K.tech=[{name:M},{name:"naive bayes"}];K.category=[{name:"minor"},{name:"cyber"},{name:"body of knowledge"}];K.author={name:"Rik Peeters",image:"\u002Fimages\u002Fme.jpg",social:{twitter:"rikp777"}};K.github="https:\u002F\u002Fgithub.com\u002Frikp777\u002Fstat_project";K.createdAt="2021-11-17T00:00:00.000Z";K.toc=[{id:N,depth:t,text:O},{id:P,depth:t,text:Q},{id:R,depth:u,text:S},{id:T,depth:u,text:U},{id:V,depth:t,text:W},{id:X,depth:u,text:Y},{id:Z,depth:u,text:_},{id:$,depth:u,text:aa},{id:ab,depth:t,text:ac},{id:ad,depth:t,text:ae}];K.body={type:"root",children:[{type:b,tag:G,props:{id:"machine-learning"},children:[{type:b,tag:k,props:{href:"#machine-learning",ariaHidden:l,tabIndex:m},children:[{type:b,tag:d,props:{className:[n,o]},children:[]}]},{type:a,value:"Machine Learning"}]},{type:a,value:c},{type:b,tag:v,props:{id:N},children:[{type:b,tag:k,props:{href:"#introduction",ariaHidden:l,tabIndex:m},children:[{type:b,tag:d,props:{className:[n,o]},children:[]}]},{type:a,value:O}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"I would like to learn how to apply \"Machine Learning\" to data and thereby perform a useful analysis on this data.\nI would like to learn this as a new challenge to learn more about the techniques behind Machine Learning.\nWhat is it, what is it used for or when to use it.\nI am going to link to Cyber by performing a data analysis on a dataset related to the subject of cyber.\nIn doing so, I am using the \"Eron\" dataset which is specifically used for forensics.\nIn order to apply all of this, I need to learn about the different types of algorithms out there.\nFor this I need Python and knowledge of the types of algorithms out there and when to apply them.\nBy executing and following course on Udacity and the associated assignments,\nI am going to make this learning goal measurable for myself.\nFurthermore, I also want to eventually be able to perform an analysis on the Enron dataset."}]},{type:a,value:c},{type:b,tag:v,props:{id:P},children:[{type:b,tag:k,props:{href:"#evaluate",ariaHidden:l,tabIndex:m},children:[{type:b,tag:d,props:{className:[n,o]},children:[]}]},{type:a,value:Q}]},{type:a,value:c},{type:b,tag:w,props:{id:R},children:[{type:b,tag:k,props:{href:"#version-1",ariaHidden:l,tabIndex:m},children:[{type:b,tag:d,props:{className:[n,o]},children:[]}]},{type:a,value:S}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Currently I am working on Udacity: "},{type:b,tag:k,props:{href:"https:\u002F\u002Fwww.udacity.com\u002Fcourse\u002Fintro-to-machine-learning--ud120",rel:[H,I,J],target:A},children:[{type:a,value:"\"Intro to Machine Learning\""}]},{type:a,value:".\nThis course I have now 50% completed.\nDuring this course I learned about Naive Bayes, SVM, Decision Trees and Regression.\nThe material was easy to understand but at the moment I do not know how I can apply it within my own project\u002Fresearch.\nThe step from theory to practice"}]},{type:a,value:c},{type:b,tag:w,props:{id:T},children:[{type:b,tag:k,props:{href:"#version-2",ariaHidden:l,tabIndex:m},children:[{type:b,tag:d,props:{className:[n,o]},children:[]}]},{type:a,value:U}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Here I have been working on building an algorithm that detects gibberish.\nThis was quite difficult because I had no prior knowledge.\nI contacted lecturer Peter Lambooij and discussed how I could best approach this.\nHe suggested that I delve further into Naive Bayes.\nI had a small knowledge of Naive Bayes from the Udacity course but to build the algorithm I needed more knowledge so I decided to delve further into this subject.\nAfter finding out how the algorithm works and its applications I started to build a prototype.\nAfter the prototype, I built a final version whose elaboration is described below.\nDescribing the elaboration was difficult because the prepairing of the data was quite complicated."}]},{type:a,value:c},{type:b,tag:G,props:{id:"assignments"},children:[{type:b,tag:k,props:{href:"#assignments",ariaHidden:l,tabIndex:m},children:[{type:b,tag:d,props:{className:[n,o]},children:[]}]},{type:a,value:"Assignments"}]},{type:a,value:c},{type:b,tag:v,props:{id:V},children:[{type:b,tag:k,props:{href:"#enron-dataset",ariaHidden:l,tabIndex:m},children:[{type:b,tag:d,props:{className:[n,o]},children:[]}]},{type:a,value:W}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"The Enron Email Corpus is a massive dataset, containing approximately 500,000 messages from executives of the Enron Corporation.\nEnron was a large American company that was investigated by the Federal Energy Regulatory Commission (FERC) in 2001 after its rather spectacular bankruptcy and dissolution.\nIt contains records of about 150 users, mostly management of Enron, organized in folders.\nThe corpus contains a total of about 0.5M messages.\nThis data was originally made public, and put on the web, by the Federal Energy Regulatory Commission during its investigation.\nIn its raw form, the Enron corpus is a massive collection of folders containing 2.2 Gigabytes of messages in MBOX format, all individually preserved"}]},{type:a,value:c},{type:b,tag:w,props:{id:X},children:[{type:b,tag:k,props:{href:"#biggest-corporate-fraud",ariaHidden:l,tabIndex:m},children:[{type:b,tag:d,props:{className:[n,o]},children:[]}]},{type:a,value:Y}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"But first we need to know something about the biggest corporate fraud in American history!\nThe Enron fraud is a big, messy, and totally fascinating story of corporate malfeasance of almost every type imaginable.\nAlthough it started out as an energy company, it was involved in so many complex issues that people didn't understand exactly what the company was doing.\nEnron was fantastic! Very attractive to investors and the fastest growing company that was making money at a rate no one had ever seen.\nThey were the darlings of Wall Street, a company that could never lose.\nIt had good political connections, was making money at an incredible rate, and seemed to be the leading innovative company and highly profitable investment."}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"In the course I will be taking,\nwe are going to use Python to analyze the dataset,\nand discover patterns and clues through data exploration,\nas well as build a regression model that could predict the bonus of a person at Enron based on the salaries he receives."}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"recourse:"}]},{type:a,value:c},{type:b,tag:p,props:{},children:[{type:a,value:c},{type:b,tag:h,props:{},children:[{type:b,tag:k,props:{href:"https:\u002F\u002Fwww.colorado.edu\u002Fics\u002Fsites\u002Fdefault\u002Ffiles\u002Fattached-files\u002F01-11_0.pdf",rel:[H,I,J],target:A},children:[{type:a,value:"The EnronSent Corpus"}]}]},{type:a,value:c},{type:b,tag:h,props:{},children:[{type:b,tag:k,props:{href:"https:\u002F\u002Ftowardsdatascience.com\u002Fderiving-patterns-of-fraud-from-the-enron-dataset-64cbceb65c36",rel:[H,I,J],target:A},children:[{type:a,value:"Deriving Patterns of Fraud from the Enron Dataset"}]}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:w,props:{id:Z},children:[{type:b,tag:k,props:{href:"#analysis-on-enron-mail-datasets",ariaHidden:l,tabIndex:m},children:[{type:b,tag:d,props:{className:[n,o]},children:[]}]},{type:a,value:_}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Analysis on enron mail datasets:"}]},{type:a,value:"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"},{type:b,tag:af,props:{},children:[{type:b,tag:ag,props:{},children:[{type:b,tag:j,props:{},children:[{type:b,tag:x,props:{},children:[{type:a,value:"Question"}]},{type:b,tag:x,props:{},children:[{type:a,value:"answer"}]}]}]},{type:b,tag:ah,props:{},children:[{type:b,tag:j,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"How many data points (people) are in the data set?"}]},{type:b,tag:e,props:{},children:[{type:a,value:"146"}]}]},{type:b,tag:j,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"For each person, how many functions are available?"}]},{type:b,tag:e,props:{},children:[{type:a,value:ai}]}]},{type:b,tag:j,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"How many POIs are there in the E+F dataset?"}]},{type:b,tag:e,props:{},children:[{type:a,value:"18"}]}]},{type:b,tag:j,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"How many POIs are there in total?"}]},{type:b,tag:e,props:{},children:[{type:a,value:"35"}]}]},{type:b,tag:j,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"What is the total value of James Prentice's shares?"}]},{type:b,tag:e,props:{},children:[{type:a,value:"1095040"}]}]},{type:b,tag:j,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"How many emails do we have from Wesley Colwell to persons of interest?"}]},{type:b,tag:e,props:{},children:[{type:a,value:"240"}]}]},{type:b,tag:j,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"What is the value of stock options exercised by Jeffrey K Skilling?"}]},{type:b,tag:e,props:{},children:[{type:a,value:"19250000"}]}]},{type:b,tag:j,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"Who made the most money?"}]},{type:b,tag:e,props:{},children:[{type:a,value:"YEAP SOON"}]}]},{type:b,tag:j,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"How many people in this dataset have a quantified salary?"}]},{type:b,tag:e,props:{},children:[{type:a,value:"95"}]}]},{type:b,tag:j,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"How many people in this dataset have an email address?"}]},{type:b,tag:e,props:{},children:[{type:a,value:"111"}]}]},{type:b,tag:j,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"How many people in the E+F dataset (as it currently exists) have \"NaN\" for their total payments?"}]},{type:b,tag:e,props:{},children:[{type:a,value:ai}]}]},{type:b,tag:j,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"What percentage of people in the data set as a whole do?"}]},{type:b,tag:e,props:{},children:[{type:a,value:"14.38%"}]}]},{type:b,tag:j,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"How many POIs in the E+F dataset have \"NaN\" for their total payments?"}]},{type:b,tag:e,props:{},children:[{type:a,value:"0"}]}]},{type:b,tag:j,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"What percentage of POIs as a whole is this?"}]},{type:b,tag:e,props:{},children:[{type:a,value:"0%"}]}]}]}]},{type:a,value:c},{type:b,tag:w,props:{id:$},children:[{type:b,tag:k,props:{href:"#learning-objectives",ariaHidden:l,tabIndex:m},children:[{type:b,tag:d,props:{className:[n,o]},children:[]}]},{type:a,value:aa}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"The assignment above here elaborate the following learning objectives from body of knowledge:"}]},{type:a,value:c},{type:b,tag:p,props:{},children:[{type:a,value:c},{type:b,tag:h,props:{},children:[{type:a,value:"Study forensic methodologies and practices (see resources below). List these standards and how and where they are applied in your portfolio or in a blog or article."}]},{type:a,value:c},{type:b,tag:h,props:{},children:[{type:a,value:"Work out forensic challenges that you can find online or get from teachers."}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:v,props:{id:ab},children:[{type:b,tag:k,props:{href:"#gibberish-detection---naive-bayes",ariaHidden:l,tabIndex:m},children:[{type:b,tag:d,props:{className:[n,o]},children:[]}]},{type:a,value:ac}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"For the project where I had to analyze and conclude a hypothesis, I created a Machine Learning algortime by applying Naive Bayes.\nHereby I prepared different dataset and merged them."}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"The datasets used for this purpose consist of:"}]},{type:a,value:"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"},{type:b,tag:af,props:{},children:[{type:b,tag:ag,props:{},children:[{type:b,tag:j,props:{},children:[{type:b,tag:x,props:{},children:[{type:a,value:"Dataset"}]},{type:b,tag:x,props:{},children:[{type:a,value:"Description"}]},{type:b,tag:x,props:{},children:[{type:a,value:aj}]}]}]},{type:b,tag:ah,props:{},children:[{type:b,tag:j,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"Amazon Reviews"}]},{type:b,tag:e,props:{},children:[{type:a,value:"Top review left under a product by customers"}]},{type:b,tag:e,props:{},children:[{type:a,value:"Positive (1)"}]}]},{type:b,tag:j,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"Youtube Cyptic comments"}]},{type:b,tag:e,props:{},children:[{type:a,value:"Spam comments left by users under videos"}]},{type:b,tag:e,props:{},children:[{type:a,value:ak}]}]},{type:b,tag:j,props:{},children:[{type:b,tag:e,props:{},children:[{type:a,value:"Gibberish"}]},{type:b,tag:e,props:{},children:[{type:a,value:"A dataset with random characters"}]},{type:b,tag:e,props:{},children:[{type:a,value:ak}]}]}]}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"I labeled this dataset with code and made it usable for analysis.\nTo apply Naive Bayes, I used scikit-learn.\nNaive Bayes is a method for calculating the probability of something happening.\nIn the project, I use this because I want to calculate the probability that text is positive or negative in nature.\nThe probability is calculated by analyzing the dataset and attaching weights to it.\nFor example, how often does a specific word occur in a text.\nLet's use the words \"dog,\" \"toy,\" \"brown,\" and \"sunny\" as examples.\nSuppose the analysis shows that the word dog occurs as many as 100 times and the other words also occur around that range.\nThen we add a dataset with the characteristics of gibberish language.\nWeights are also attached to this.\nNow when we continue to make a prediction of the word \"sun\" it will always be positive.\nBut if we are going to predict the word \"adskladhjl\" it will be negative."}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"So to get this done we need to start training the algorithm below is an example of this."}]},{type:a,value:c},{type:b,tag:"div",props:{className:["nuxt-content-highlight"]},children:[{type:b,tag:"pre",props:{className:["language-py","line-numbers"]},children:[{type:b,tag:"code",props:{},children:[{type:b,tag:d,props:{className:[g,B]},children:[{type:a,value:"# Count word usage "}]},{type:a,value:"\nvectorizer "},{type:b,tag:d,props:{className:[g,r]},children:[{type:a,value:s}]},{type:a,value:" CountVectorizer"},{type:b,tag:d,props:{className:[g,i]},children:[{type:a,value:y}]},{type:a,value:"stop_words"},{type:b,tag:d,props:{className:[g,r]},children:[{type:a,value:s}]},{type:b,tag:d,props:{className:[g,"string"]},children:[{type:a,value:"'english'"}]},{type:b,tag:d,props:{className:[g,i]},children:[{type:a,value:z}]},{type:a,value:"\nall_features "},{type:b,tag:d,props:{className:[g,r]},children:[{type:a,value:s}]},{type:a,value:" vectorizer"},{type:b,tag:d,props:{className:[g,i]},children:[{type:a,value:C}]},{type:a,value:"fit_transform"},{type:b,tag:d,props:{className:[g,i]},children:[{type:a,value:y}]},{type:a,value:"df_merged"},{type:b,tag:d,props:{className:[g,i]},children:[{type:a,value:C}]},{type:a,value:"Response"},{type:b,tag:d,props:{className:[g,i]},children:[{type:a,value:z}]},{type:a,value:"\n\n"},{type:b,tag:d,props:{className:[g,B]},children:[{type:a,value:"# Split list into random train and test subsets"}]},{type:a,value:"\nx_train"},{type:b,tag:d,props:{className:[g,i]},children:[{type:a,value:q}]},{type:a,value:" x_test"},{type:b,tag:d,props:{className:[g,i]},children:[{type:a,value:q}]},{type:a,value:al},{type:b,tag:d,props:{className:[g,i]},children:[{type:a,value:q}]},{type:a,value:" y_test"},{type:b,tag:d,props:{className:[g,i]},children:[{type:a,value:q}]},{type:a,value:" "},{type:b,tag:d,props:{className:[g,r]},children:[{type:a,value:s}]},{type:a,value:" train_test_split"},{type:b,tag:d,props:{className:[g,i]},children:[{type:a,value:y}]},{type:a,value:"all_features"},{type:b,tag:d,props:{className:[g,i]},children:[{type:a,value:q}]},{type:a,value:" df_merged"},{type:b,tag:d,props:{className:[g,i]},children:[{type:a,value:C}]},{type:a,value:aj},{type:b,tag:d,props:{className:[g,i]},children:[{type:a,value:q}]},{type:a,value:" test_size"},{type:b,tag:d,props:{className:[g,r]},children:[{type:a,value:s}]},{type:b,tag:d,props:{className:[g,"number"]},children:[{type:a,value:"0.35"}]},{type:b,tag:d,props:{className:[g,i]},children:[{type:a,value:z}]},{type:a,value:" \n\nclassifier "},{type:b,tag:d,props:{className:[g,r]},children:[{type:a,value:s}]},{type:a,value:" MultinomialNB"},{type:b,tag:d,props:{className:[g,i]},children:[{type:a,value:y}]},{type:b,tag:d,props:{className:[g,i]},children:[{type:a,value:z}]},{type:a,value:am},{type:b,tag:d,props:{className:[g,B]},children:[{type:a,value:"# Create Model (naive bayes) multinomial"}]},{type:a,value:"\nclassifier"},{type:b,tag:d,props:{className:[g,i]},children:[{type:a,value:C}]},{type:a,value:"fit"},{type:b,tag:d,props:{className:[g,i]},children:[{type:a,value:y}]},{type:a,value:"x_train"},{type:b,tag:d,props:{className:[g,i]},children:[{type:a,value:q}]},{type:a,value:al},{type:b,tag:d,props:{className:[g,i]},children:[{type:a,value:z}]},{type:a,value:am},{type:b,tag:d,props:{className:[g,B]},children:[{type:a,value:"# Train Model"}]},{type:a,value:c}]}]}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"In my code, I use the multinomial Naive Bayes classifier. "},{type:b,tag:"br",props:{},children:[]},{type:a,value:"\nThis is suitable for classification with discrete features (e.g., word counts for text classification)"}]},{type:a,value:c},{type:b,tag:"nuxt-link",props:{target:A,rel:["some",a],to:an},children:[{type:a,value:"\n  "},{type:b,tag:D,props:{width:E,src:an,alt:F},children:[]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"It is very important for an algorithm that the accuracy is high. From the image above you can see that my algorithm can make a prediction for future data with a certainty of around 98%."}]},{type:a,value:c},{type:b,tag:G,props:{id:"workshop-tilburg"},children:[{type:b,tag:k,props:{href:"#workshop-tilburg",ariaHidden:l,tabIndex:m},children:[{type:b,tag:d,props:{className:[n,o]},children:[]}]},{type:a,value:"Workshop Tilburg"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"During this workshop it was explained how machine learning could be applied to detect cyber attacks. Here we were given a presentation with the basic principles and techniques. We were then allowed to apply these within the supplied dataset. Because I already did two projects within this field it was not so difficult and it was easy because they used scikit learn."}]},{type:a,value:c},{type:b,tag:v,props:{id:ad},children:[{type:b,tag:k,props:{href:"#confusion-matrix",ariaHidden:l,tabIndex:m},children:[{type:b,tag:d,props:{className:[n,o]},children:[]}]},{type:a,value:ae}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"When you are using machine learning algorithms it is useful to always test your dataset. This can be done with a confusion matrix when done you can see where your algorithm goes wrong and where it predicted wrong values. It gives you a general performance level of a classification model."}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Below I am going to briefly explain how to read a confusion table:"}]},{type:a,value:c},{type:b,tag:D,props:{width:E,src:"\u002Fimages\u002Fprojects\u002Fminor\u002Fmachine_learning\u002Fconfusion_matrix.png",alt:F},children:[]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"as you can see the total predictions that this model did was 165. Within those 165 predictions it gave a total of 100 times yes when it was actually yes and it predicted no 50 times when it was actually no. But we can also see the errors we can for example see that it  predicted no 5 times when it was textually yes and 10 times yes when it was actually no."}]},{type:a,value:c},{type:b,tag:p,props:{},children:[{type:a,value:c},{type:b,tag:h,props:{},children:[{type:a,value:"50 times No when it was actually no = true negatives"}]},{type:a,value:c},{type:b,tag:h,props:{},children:[{type:a,value:"100 times yes when it was actually yes = true positives"}]},{type:a,value:c},{type:b,tag:h,props:{},children:[{type:a,value:"5 times no when it was actually yes = true positives = type two error"}]},{type:a,value:c},{type:b,tag:h,props:{},children:[{type:a,value:"10 times yes when it was actually no = false positives = type one error"}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:D,props:{width:E,src:"\u002Fimages\u002Fprojects\u002Fminor\u002Fmachine_learning\u002Fconfusion_matrix_sum.png",alt:F},children:[]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"This is a list of rates that are often computed from a confusion matrix:"}]},{type:a,value:c},{type:b,tag:p,props:{},children:[{type:a,value:c},{type:b,tag:h,props:{},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Accuracy: Overall, how often is the classifier corrrect?"}]},{type:a,value:c},{type:b,tag:p,props:{},children:[{type:a,value:c},{type:b,tag:h,props:{},children:[{type:a,value:"(TP+TN)\u002Ftotal = (100+50)\u002F165 = 0,91"}]},{type:a,value:c}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:h,props:{},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Misclassification Rate (Error rate): Overall, how often is it wrong?"}]},{type:a,value:c},{type:b,tag:p,props:{},children:[{type:a,value:c},{type:b,tag:h,props:{},children:[{type:a,value:"(FP+FN)\u002Ftotal = (10+5)\u002F165 = 0.09"}]},{type:a,value:c}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:h,props:{},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"True Positive Rate: When it's actually yes, how often does it predict yes?"}]},{type:a,value:c},{type:b,tag:p,props:{},children:[{type:a,value:c},{type:b,tag:h,props:{},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"TP\u002Factual yes = 100\u002F105 = 0.95"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Also known as \"Sensitivity\" or \"Recall\""}]},{type:a,value:c}]},{type:a,value:c}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:h,props:{},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"False Positive Rate: When it's actually no, how often does it predict yes?"}]},{type:a,value:c},{type:b,tag:p,props:{},children:[{type:a,value:c},{type:b,tag:h,props:{},children:[{type:a,value:"FP\u002Factual no = 10\u002F60 = 0.17"}]},{type:a,value:c}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:h,props:{},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"True Negative Rate: When it's actually no, how often does it predict no?"}]},{type:a,value:c},{type:b,tag:p,props:{},children:[{type:a,value:c},{type:b,tag:h,props:{},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"TN\u002Factual no = 50\u002F60 = 0.83"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Equivalent to 1 minus False Positive Rate"}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Also known as \"Specificity\""}]},{type:a,value:c}]},{type:a,value:c}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:h,props:{},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Precision: When it predicts yes, how often is it correct?"}]},{type:a,value:c},{type:b,tag:p,props:{},children:[{type:a,value:c},{type:b,tag:h,props:{},children:[{type:a,value:"TP\u002Fpredicted yes = 100\u002F110 = 0.91"}]},{type:a,value:c}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:h,props:{},children:[{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Prevalence: How often does the yes condition actually occur in our sample?"}]},{type:a,value:c},{type:b,tag:p,props:{},children:[{type:a,value:c},{type:b,tag:h,props:{},children:[{type:a,value:"actual yes\u002Ftotal = 105\u002F165 =0.64"}]},{type:a,value:c}]},{type:a,value:c}]},{type:a,value:c}]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Below is a confusion matrix I made at the workshop in tilburg:"}]},{type:a,value:c},{type:b,tag:D,props:{width:E,src:"\u002Fimages\u002Fprojects\u002Fminor\u002Fmachine_learning\u002Fmachine_learning_confusion_matrix.png",alt:F},children:[]},{type:a,value:c},{type:b,tag:f,props:{},children:[{type:a,value:"Here we can see that the table is bigger but the same principle remains."}]}]};K.dir="\u002Fprojects\u002Fminor";K.path="\u002Fprojects\u002Fminor\u002Fmachine-learning";K.extension=".md";K.updatedAt="2022-01-19T10:23:35.896Z";K.bodyPlainText="\n# Machine Learning\n## Introduction\nI would like to learn how to apply \"Machine Learning\" to data and thereby perform a useful analysis on this data. \nI would like to learn this as a new challenge to learn more about the techniques behind Machine Learning. \nWhat is it, what is it used for or when to use it. \nI am going to link to Cyber by performing a data analysis on a dataset related to the subject of cyber. \nIn doing so, I am using the \"Eron\" dataset which is specifically used for forensics. \nIn order to apply all of this, I need to learn about the different types of algorithms out there. \nFor this I need Python and knowledge of the types of algorithms out there and when to apply them. \nBy executing and following course on Udacity and the associated assignments, \nI am going to make this learning goal measurable for myself. \nFurthermore, I also want to eventually be able to perform an analysis on the Enron dataset.\n\n## Evaluate\n### Version (1)\nCurrently I am working on Udacity: [\"Intro to Machine Learning\"](https:\u002F\u002Fwww.udacity.com\u002Fcourse\u002Fintro-to-machine-learning--ud120). \nThis course I have now 50% completed. \nDuring this course I learned about Naive Bayes, SVM, Decision Trees and Regression. \nThe material was easy to understand but at the moment I do not know how I can apply it within my own project\u002Fresearch. \nThe step from theory to practice\n\n### Version (2)\nHere I have been working on building an algorithm that detects gibberish. \nThis was quite difficult because I had no prior knowledge.\nI contacted lecturer Peter Lambooij and discussed how I could best approach this. \nHe suggested that I delve further into Naive Bayes. \nI had a small knowledge of Naive Bayes from the Udacity course but to build the algorithm I needed more knowledge so I decided to delve further into this subject. \nAfter finding out how the algorithm works and its applications I started to build a prototype. \nAfter the prototype, I built a final version whose elaboration is described below. \nDescribing the elaboration was difficult because the prepairing of the data was quite complicated.\n\n# Assignments\n## Enron Dataset\nThe Enron Email Corpus is a massive dataset, containing approximately 500,000 messages from executives of the Enron Corporation. \nEnron was a large American company that was investigated by the Federal Energy Regulatory Commission (FERC) in 2001 after its rather spectacular bankruptcy and dissolution. \nIt contains records of about 150 users, mostly management of Enron, organized in folders. \nThe corpus contains a total of about 0.5M messages. \nThis data was originally made public, and put on the web, by the Federal Energy Regulatory Commission during its investigation. \nIn its raw form, the Enron corpus is a massive collection of folders containing 2.2 Gigabytes of messages in MBOX format, all individually preserved\n\n### Biggest corporate fraud\nBut first we need to know something about the biggest corporate fraud in American history!\nThe Enron fraud is a big, messy, and totally fascinating story of corporate malfeasance of almost every type imaginable. \nAlthough it started out as an energy company, it was involved in so many complex issues that people didn't understand exactly what the company was doing. \nEnron was fantastic! Very attractive to investors and the fastest growing company that was making money at a rate no one had ever seen. \nThey were the darlings of Wall Street, a company that could never lose. \nIt had good political connections, was making money at an incredible rate, and seemed to be the leading innovative company and highly profitable investment.\n\nIn the course I will be taking, \nwe are going to use Python to analyze the dataset, \nand discover patterns and clues through data exploration, \nas well as build a regression model that could predict the bonus of a person at Enron based on the salaries he receives.\n\nrecourse:\n- [The EnronSent Corpus](https:\u002F\u002Fwww.colorado.edu\u002Fics\u002Fsites\u002Fdefault\u002Ffiles\u002Fattached-files\u002F01-11_0.pdf)\n- [Deriving Patterns of Fraud from the Enron Dataset](https:\u002F\u002Ftowardsdatascience.com\u002Fderiving-patterns-of-fraud-from-the-enron-dataset-64cbceb65c36)\n\n### Analysis on enron mail datasets \nAnalysis on enron mail datasets:\n| Question | answer |\n| ----------- | ----------- |\n| How many data points (people) are in the data set? | 146 |\n| For each person, how many functions are available? | 21 |\n| How many POIs are there in the E+F dataset? | 18 |\n| How many POIs are there in total? | 35 |\n| What is the total value of James Prentice's shares?\t| 1095040 |\n| How many emails do we have from Wesley Colwell to persons of interest? | 240 |\n| What is the value of stock options exercised by Jeffrey K Skilling?\t| 19250000 |\n| Who made the most money? | YEAP SOON |\n| How many people in this dataset have a quantified salary? | 95 |\n| How many people in this dataset have an email address? | 111 |\n| How many people in the E+F dataset (as it currently exists) have \"NaN\" for their total payments? | 21 |\n| What percentage of people in the data set as a whole do? | 14.38% |\n| How many POIs in the E+F dataset have \"NaN\" for their total payments? | 0 |\n| What percentage of POIs as a whole is this? | 0% |\n\n### learning objectives\nThe assignment above here elaborate the following learning objectives from body of knowledge:\n- Study forensic methodologies and practices (see resources below). List these standards and how and where they are applied in your portfolio or in a blog or article.\n- Work out forensic challenges that you can find online or get from teachers.\n\n\n## Gibberish detection - naive bayes \nFor the project where I had to analyze and conclude a hypothesis, I created a Machine Learning algortime by applying Naive Bayes. \nHereby I prepared different dataset and merged them. \n\nThe datasets used for this purpose consist of:\n| Dataset | Description | Label |\n| ----------- | ----------- | ----------- |\n| Amazon Reviews | Top review left under a product by customers | Positive (1) |\n| Youtube Cyptic comments | Spam comments left by users under videos | Negative (0) |\n| Gibberish | A dataset with random characters | Negative (0) |\n\nI labeled this dataset with code and made it usable for analysis. \nTo apply Naive Bayes, I used scikit-learn. \nNaive Bayes is a method for calculating the probability of something happening. \nIn the project, I use this because I want to calculate the probability that text is positive or negative in nature. \nThe probability is calculated by analyzing the dataset and attaching weights to it. \nFor example, how often does a specific word occur in a text. \nLet's use the words \"dog,\" \"toy,\" \"brown,\" and \"sunny\" as examples. \nSuppose the analysis shows that the word dog occurs as many as 100 times and the other words also occur around that range. \nThen we add a dataset with the characteristics of gibberish language. \nWeights are also attached to this. \nNow when we continue to make a prediction of the word \"sun\" it will always be positive. \nBut if we are going to predict the word \"adskladhjl\" it will be negative. \n\nSo to get this done we need to start training the algorithm below is an example of this. \n```py\n# Count word usage \nvectorizer = CountVectorizer(stop_words='english')\nall_features = vectorizer.fit_transform(df_merged.Response)\n\n# Split list into random train and test subsets\nx_train, x_test, y_train, y_test, = train_test_split(all_features, df_merged.Label, test_size=0.35) \n\nclassifier = MultinomialNB()  # Create Model (naive bayes) multinomial\nclassifier.fit(x_train, y_train)  # Train Model\n```\n\n\nIn my code, I use the multinomial Naive Bayes classifier. \\\nThis is suitable for classification with discrete features (e.g., word counts for text classification)\n\n\u003Ca href=\"\u002Fimages\u002Fprojects\u002Fminor\u002Fmachine_learning\u002Fminor_cyber_machine_learning_scores.png\" target=\"_blank\" rel=\"some text\"\u003E\n  \u003Cimg width=\"80%\" src=\"\u002Fimages\u002Fprojects\u002Fminor\u002Fmachine_learning\u002Fminor_cyber_machine_learning_scores.png\" alt=\"Machine learning main image\"\u002F\u003E\n\u003C\u002Fa\u003E\n\nIt is very important for an algorithm that the accuracy is high. From the image above you can see that my algorithm can make a prediction for future data with a certainty of around 98%. \n\n# Workshop Tilburg\n\nDuring this workshop it was explained how machine learning could be applied to detect cyber attacks. Here we were given a presentation with the basic principles and techniques. We were then allowed to apply these within the supplied dataset. Because I already did two projects within this field it was not so difficult and it was easy because they used scikit learn. \n\n## Confusion Matrix\n\nWhen you are using machine learning algorithms it is useful to always test your dataset. This can be done with a confusion matrix when done you can see where your algorithm goes wrong and where it predicted wrong values. It gives you a general performance level of a classification model. \n\nBelow I am going to briefly explain how to read a confusion table: \n\n\u003Cimg width=\"80%\" src=\"\u002Fimages\u002Fprojects\u002Fminor\u002Fmachine_learning\u002Fconfusion_matrix.png\" alt=\"Machine learning main image\"\u002F\u003E\n\nas you can see the total predictions that this model did was 165. Within those 165 predictions it gave a total of 100 times yes when it was actually yes and it predicted no 50 times when it was actually no. But we can also see the errors we can for example see that it  predicted no 5 times when it was textually yes and 10 times yes when it was actually no. \n\n- 50 times No when it was actually no = true negatives \n- 100 times yes when it was actually yes = true positives \n- 5 times no when it was actually yes = true positives = type two error \n- 10 times yes when it was actually no = false positives = type one error\n\n\u003Cimg width=\"80%\" src=\"\u002Fimages\u002Fprojects\u002Fminor\u002Fmachine_learning\u002Fconfusion_matrix_sum.png\" alt=\"Machine learning main image\"\u002F\u003E\n\nThis is a list of rates that are often computed from a confusion matrix:\n\n- Accuracy: Overall, how often is the classifier corrrect? \n\n  - (TP+TN)\u002Ftotal = (100+50)\u002F165 = 0,91\n\n- Misclassification Rate (Error rate): Overall, how often is it wrong?\n\n  - (FP+FN)\u002Ftotal = (10+5)\u002F165 = 0.09\n\n- True Positive Rate: When it's actually yes, how often does it predict yes?\n\n  - TP\u002Factual yes = 100\u002F105 = 0.95\n\n    Also known as \"Sensitivity\" or \"Recall\"\n\n- False Positive Rate: When it's actually no, how often does it predict yes?\n\n  - FP\u002Factual no = 10\u002F60 = 0.17\n\n- True Negative Rate: When it's actually no, how often does it predict no?\n\n  - TN\u002Factual no = 50\u002F60 = 0.83\n\n    Equivalent to 1 minus False Positive Rate\n\n    Also known as \"Specificity\"\n\n- Precision: When it predicts yes, how often is it correct?\n\n  - TP\u002Fpredicted yes = 100\u002F110 = 0.91\n\n- Prevalence: How often does the yes condition actually occur in our sample?\n\n  - actual yes\u002Ftotal = 105\u002F165 =0.64\n\nBelow is a confusion matrix I made at the workshop in tilburg: \n\n\u003Cimg width=\"80%\" src=\"\u002Fimages\u002Fprojects\u002Fminor\u002Fmachine_learning\u002Fmachine_learning_confusion_matrix.png\" alt=\"Machine learning main image\"\u002F\u003E\n\nHere we can see that the table is bigger but the same principle remains. \n";K.readingTime="10 min read";K.twitterShareUrl="https:\u002F\u002Ftwitter.com\u002Fintent\u002Ftweet";return {data:[{project:K,_img:ao}],fetch:{"data-v-312258fc:0":{toastOptions:{duration:2000,theme:"bubble"},currentlyActiveToc:"",observer:ap,observerOptions:{root:void 0,threshold:0},project:K,_img:ao}},mutations:[["setItem",{item:ap,id:L,resource:"comments"}]]}}("text","element","\n","span","td","p","token","li","punctuation","tr","a","true",-1,"icon","icon-link","ul",",","operator","=",2,3,"h2","h3","th","(",")","_blank","comment",".","img","80%","Machine learning main image","h1","nofollow","noopener","noreferrer",{},"minor_machine-learning","machine learning","introduction","Introduction","evaluate","Evaluate","version-1","Version (1)","version-2","Version (2)","enron-dataset","Enron Dataset","biggest-corporate-fraud","Biggest corporate fraud","analysis-on-enron-mail-datasets","Analysis on enron mail datasets","learning-objectives","learning objectives","gibberish-detection---naive-bayes","Gibberish detection - naive bayes","confusion-matrix","Confusion Matrix","table","thead","tbody","21","Label","Negative (0)"," y_train","  ","\u002Fimages\u002Fprojects\u002Fminor\u002Fmachine_learning\u002Fminor_cyber_machine_learning_scores.png",{},null)));